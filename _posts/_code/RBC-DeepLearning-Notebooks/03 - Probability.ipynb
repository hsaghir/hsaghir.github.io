{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Probability and Statistics Review\n",
    "\n",
    "Based on \"Machine Learning: A Probabilistic Perspective\" by Kevin P. Murphy (Chapter 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# What is probability?\n",
    " \n",
    "* At least two different interpretations:\n",
    "    * **Frequentist**: probabilities are long-run frequencies of events\n",
    "    * **Bayesian**: probabilities are used to quantify our **uncertainty**\n",
    "\n",
    "One advantage of the Bayesian interpretation is that it can be used to model events that do not have long-term frequencies. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# A brief review of probability theory\n",
    "\n",
    "## Discrete random variables\n",
    "\n",
    "$p(A)$ denotes the probability that the event $A$ is true\n",
    "\n",
    "* $0 \\leq p(A) \\leq 1$\n",
    "\n",
    "We write $p(\\bar{A})$ to denote the probability of the event not $A$\n",
    "\n",
    "* $p(\\bar{A}) = 1 - p(A)$\n",
    "\n",
    "We can extend the notion of binary events by defining a **discrete random variable** $X$ which can take on any value from a finite or countably infinite set $\\mathcal{X}$. We denote the probability of the event that $X = x$ by $p(X = x)$ or just $p(x)$ for short.\n",
    "\n",
    "* $0 \\leq p(x) \\leq 1$\n",
    "* $\\sum_{x \\in \\mathcal{X}} p(x) = 1$\n",
    "\n",
    "Let's look at some discrete distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "ax[0].bar([1, 2, 3, 4],[0.25, 0.25, 0.25, 0.25], align='center')\n",
    "ax[0].set_ylim([0, 1])\n",
    "_ = ax[0].set_xticks([1, 2, 3, 4])\n",
    "ax[0].set_title('Uniform distribution')\n",
    "\n",
    "ax[1].bar([1, 2, 3, 4],[0, 1.0, 0, 0], align='center')\n",
    "ax[1].set_ylim([0, 1])\n",
    "_ = ax[1].set_xticks([1, 2, 3, 4])\n",
    "ax[1].set_title('Degenerate distribution')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Fundamental rules\n",
    "\n",
    "### Probability of a union of two events\n",
    "\n",
    "Given two events, $A$ and $B$, we define the probability of $A$ or $B$ as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(A \\lor B) &= p(A) + p(B) - p(A \\land B) \\\\\n",
    "&= p(A) + p(B) & \\text{if $A$ and $B$ are mutually exclusive}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Joint probabilities\n",
    "\n",
    "We define the probability of the joint event $A$ and $B$ as \n",
    "\n",
    "$$\n",
    "p(A,B) = p(A \\land B) = p(A|B)p(B)\n",
    "$$\n",
    "\n",
    "Given a **joint distribution** on two events p(A,B), we define the **marginal distribution** as\n",
    "\n",
    "$$\n",
    "p(A) = \\sum_b p(A,B) = \\sum_b p(A|B)p(B)\n",
    "$$\n",
    "\n",
    "### Conditional probability\n",
    "\n",
    "We define the **conditional probability** of event $A$, given that event $B$ is true, as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(A|B) &= \\frac{p(A,B)}{p(B)} & \\text{if $p(B) > 0$}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "## Bayes' rule\n",
    "\n",
    "Manipulating the basic definition of conditional probability gives us one of the most important formulas in probability theory\n",
    "\n",
    "$$p(X=x|Y=y) = \\frac{p(X=x,Y=y)}{P(Y=y)} = \\frac{p(Y=y|X=x)p(X=x)}{\\sum_{x'}p(Y=y|X=x')p(X=x')}$$\n",
    "\n",
    "## Independence and conditional independence\n",
    "\n",
    "We say $X$ and $Y$ are **unconditionally independent** or **marginally independent**, denoted $X \\perp Y$, if we can represent the joint as the product of the two marginals, i.e.,\n",
    "\n",
    "$$X \\perp Y \\Longleftrightarrow p(X,Y) = p(X)p(Y)$$\n",
    "\n",
    "<img width=400px src=\"images/pxyGrid.svg\">\n",
    "\n",
    "In general, we say a **set** of variables is mutually independent if the joint can be written as a product of marginals.\n",
    "\n",
    "We say $X$ and $Y$ are **conditionally independent** given $Z$ iff the conditional joint can be written as a product of conditional marginals:\n",
    "\n",
    "$$X \\perp Y|Z \\Longleftrightarrow p(X,Y|Z)=p(X|Z)p(Y|Z)$$\n",
    "\n",
    "CI assumptions allow us to build large probabilistic models from small pieces.\n",
    "\n",
    "## Continuous random variables\n",
    "\n",
    "Suppose $X$ is some uncertain continuous quantity. The probability that $X$ lies in any interval $a \\leq X \\leq b$ can be computed as follows. Define the events $A = (X \\leq a), B = (X \\leq b)$ and $W = (a < X \\leq b)$. We have that $B = A \\vee W$, and since $A$ and $W$ are mutually exclusive, the sum rule gives\n",
    "\n",
    "$$p(B) = p(A) + p(W)$$\n",
    "\n",
    "and hence\n",
    "\n",
    "$p(W) = p(B) - p(A)$\n",
    "\n",
    "Define the function $F(q) \\triangleq p(X \\leq q)$. This is called the **cumulative distribution function** or **cdf** of $X$. This is a monotonically non-decreasing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# CDF of Gaussian N(0,1)\n",
    "import scipy.stats as stats\n",
    "f = lambda x : stats.norm.cdf(x, 0, 1)\n",
    "x = np.arange(-3, 3, 0.1)\n",
    "y = f(x)\n",
    "\n",
    "plt.plot(x, y, 'b')\n",
    "plt.title('CDF')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using the above notation, we have\n",
    "$$p(a < X \\leq b) = F(b) - F(a)$$\n",
    "\n",
    "Now define $f(x) = \\frac{d}{dx} F(x)$ (we assume this derivative exists); this is called a **probability density function** or **pdf**. Given a pdf, we can compute the probability of a continuous variable being in a finite interval as follows:\n",
    "\n",
    "$$P(a < X \\leq b) = \\int_a^b f(x) dx$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# PDF of Gaussian N(0,1)\n",
    "# shaded area has 0.05 of the mass\n",
    "# also written mu +/- 2 \\sigma\n",
    "f = lambda x : stats.norm.pdf(x, 0, 1)\n",
    "x = np.arange(-4, 4, 0.1)\n",
    "y = f(x)\n",
    "\n",
    "plt.plot(x, y, 'b')\n",
    "l_x = np.arange(-4, -1.96, 0.01)\n",
    "plt.fill_between(l_x, f(l_x))\n",
    "u_x = np.arange(1.96, 4, 0.01)\n",
    "plt.fill_between(u_x, f(u_x))\n",
    "\n",
    "plt.title('PDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We require $p(x) \\geq 0$, but it is possible for $p(x)>1$ for any given $x$, so long as the density integrates to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Example of p(x) > 1, Uniform distribution on (0, 0.5)\n",
    "f = lambda x: stats.uniform.pdf(x, 0, 0.5)\n",
    "x = np.arange(-0.5, 1, 0.01)\n",
    "y = f(x)\n",
    "\n",
    "plt.plot(x, y, 'b')\n",
    "plt.title('Uniform PDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Mean and variance\n",
    "\n",
    "The most familiar property of a distribution is its **mean**, or **expected value**, denoted by $\\mu$. For discrete rv's, it is defined as $\\mathbb{E}[X] \\triangleq \\sum_{x \\in \\mathcal{X}} x p(x)$, and for continuous rv's, it is defined as $\\mathbb{E}[X] \\triangleq \\int_{\\mathcal{X}} x p(x) dx$.\n",
    "\n",
    "The **variance** is a measure of the \"spread\" of a distribution, denoted by $\\sigma^2$. This is defined as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{var}[X] & \\triangleq \\mathbb{E}\\left[ \\left( X - \\mu\\right)^2 \\right] = \\int \\left( x - \\mu \\right) ^2 p(x) dx \\\\\\\n",
    "&= \\int x^2 p(x)dx + \\mu^2 \\int p(x) dx - 2 \\mu \\int x p(x) dx = \\mathbb{E}[X^2] - \\mu^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "from which we derive the useful result\n",
    "\n",
    "$$\\mathbb{E}[X^2] = \\mu^2 + \\sigma^2$$\n",
    "\n",
    "The **standard deviation** is defined as\n",
    "\n",
    "$$\\text{std}[X] \\triangleq \\sqrt{\\text{var}[X]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Some common discrete distributions\n",
    "\n",
    "## The binomial and Bernoulli distributions\n",
    "\n",
    "Suppose we toss a coin $n$ times. Let $X \\in {0, \\ldots, n}$ be the number of heads. If the probability of heads is $\\theta$, then we say $X$ has a **binomial** distribution, written as $X \\sim \\text{Bin}(n, \\theta)$. The probability mass function (pmf) is given by\n",
    "\n",
    "$$\\text{Bin}(k|n,\\theta) \\triangleq {n\\choose k} \\theta^k(1 - \\theta)^{n-k}$$\n",
    "\n",
    "where\n",
    "$$ {n\\choose k} \\triangleq \\frac{n!}{(n-k)!k!}$$\n",
    "\n",
    "is the number of ways to choose $k$ items from $n$.\n",
    "\n",
    "This distribution has a mean of $n\\theta$ and a variance of $n\\theta(1-\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "x = np.arange(11)\n",
    "\n",
    "f = lambda x : stats.binom.pmf(x, 10, 0.25)\n",
    "\n",
    "ax[0].bar(x, f(x), align='center')\n",
    "#ax[0].set_ylim([0, 1])\n",
    "_ = ax[0].set_xticks(x)\n",
    "ax[0].set_title(r'$\\theta$ = 0.25')\n",
    "\n",
    "f = lambda x : stats.binom.pmf(x, 10, 0.9)\n",
    "    \n",
    "ax[1].bar(x, f(x), align='center')\n",
    "#ax[1].set_ylim([0, 1])\n",
    "_ = ax[1].set_xticks(x)\n",
    "ax[1].set_title(r'$\\theta$ = 0.9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now suppose we toss a coin only once. Let $X \\in {0,1}$ be a binary random variable, with probability of \"success\" or \"heads\" of $\\theta$. We say that $X$ has a **Bernoulli** distribution. This is written as $X \\sim \\text{Ber}(\\theta)$, where the pmf is defined as\n",
    "\n",
    "$$\\text{Ber}(x|\\theta) = \\theta^{\\mathbb{I}(x=1)}(1-\\theta)^{\\mathbb{I}(x=0)}$$\n",
    "\n",
    "In other words,\n",
    "\n",
    "$$ \\text{Ber}(x|\\theta) = \\left\\{ \n",
    "\\begin{array}{rl}\n",
    " \\theta &\\mbox{ if $x=1$} \\\\\n",
    "  1 - \\theta &\\mbox{ if $x=0$}\n",
    "\\end{array}\n",
    "\\right. $$\n",
    "\n",
    "This is obviously just a special case of a Binomial distribution with $n=1$.\n",
    "\n",
    "### The multinomial and multinoulli distribution\n",
    "\n",
    "To model the outcomes of tossing a $K$-sided die, we can use the **multinomial** distribution. This is defined as follows: let $\\mathbf{x}=(x_1, \\ldots, x_K)$ be a random vector, where $x_j$ is the number of times side $j$ of the die occurs. Then $\\mathbf{x}$ has the following pmf:\n",
    "\n",
    "$$\\text{Mu}(\\mathbf{x}|n, \\mathbf{\\theta}) \\triangleq {n \\choose x_1,\\ldots,x_K} \\prod_{j=1}^K \\theta_j^{x_j}$$\n",
    "\n",
    "where $\\theta_j$ is the probability that side $j$ shows up, and\n",
    "\n",
    "$${n \\choose x_1,\\ldots,x_K} \\triangleq \\frac{n!}{x_1!x_2! \\ldots x_K!}$$\n",
    "\n",
    "is the **multinomial coefficient** (the number of ways to divide a set of size $n=\\sum_{k=1}^K x_k$ into subsets with sizes $x_1$ up to $x_K$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now suppose $n=1$. This is like rolling a $K$-sided dice once, so $\\mathbf{x}$ will be a vector of 0s and 1s (a bit vector), in which only one bit can be turned on. Specifically, if the dice shows up as face $k$, then the $k$'th bit will be on. In this case, we can think of $x$ as being a scalar categorical random variable with $K$ states (values), and $\\mathbf{x}$ is its **dummy encoding**, that is, $\\mathbf{x} = \\left[\\mathbb{I}(x=1),\\ldots,\\mathbb{I}(x=K)\\right]$. For example, if $K=3$, we encode the states 1, 2, and 3 as $(1, 0, 0), (0, 1, 0)$ and $(0, 0, 1)$. This is also called **one-hot encoding**. In this case, the pmf becomes\n",
    "\n",
    "$$\\text{Mu}(\\mathbf{x}|1, \\mathbf{\\theta}) = \\prod_{j=1}^K \\theta_j^{\\mathbb{I}(x_j=1)}$$\n",
    "\n",
    "This very common special case is known as a **categorical** or **discrete** distribution (Kevin Murphy's text adopts the term **multinoulli distribution** by analogy with the binomial/Bernoulli distinction). We will use the following notation\n",
    "\n",
    "$$\\text{Cat}(x|\\mathbf{\\theta}) \\triangleq \\text{Mu}(\\mathbf{x}|1, \\mathbf{\\theta})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Some common continuous distributions\n",
    "\n",
    "## Gaussian (normal) distribution\n",
    "\n",
    "The most widely used distribution in statistics and machine learning is the Gaussian or normal distribution. Its pdf is given by\n",
    "\n",
    "$$\\mathcal{N}(x|\\mu, \\sigma^2) \\triangleq \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{1}{2\\sigma^2}(x - \\mu)^2}$$\n",
    "\n",
    "where $\\mu = \\mathbb{E}[X]$ is the mean (and mode), and $\\sigma^2 = \\text{var}[X]$ is the variance. $\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}$ is the normalization constant needed to ensure the density integrates to 1.\n",
    "\n",
    "We write $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ to denote that $p(X=x) = \\mathcal{N}(x|\\mu, \\sigma^2)$. If $X \\sim \\mathcal{N}(0,1)$, we say $X$ follows a **standard normal** distribution. \n",
    "\n",
    "We will sometimes talk about the **precision** of a Gaussian, by which we mean the inverse variance: $\\lambda = 1/\\sigma^2$.\n",
    "\n",
    "The Gaussian distribution is the most widely used distribution in statistics. Why?\n",
    "\n",
    "* It has two parameters that are easy to interpret\n",
    "* The central limit theorem tells us that sums of independent random variables have an approximately Gaussian distribution, making it a good fit for modeling residual errors or \"noise\"\n",
    "* The Gaussian distribution makes the least number of assumptions (i.e. has maximum entropy) which makes it a good default choice in many cases\n",
    "* It has a simple mathematical form, which results in easy to implement, but often highly effective methods\n",
    "\n",
    "## The Student $t$ distribution\n",
    "\n",
    "One problem with the Gaussian distribution is that it is sensitive to outliers, since the log-probability only decays quadratically with distance from the centre. A more robust distribution is the **Student** $t$ **distribution**. Its pdf is as follows\n",
    "\n",
    "$$\\mathcal{T}(x|\\mu, \\sigma^2, \\nu) \\propto \\left[ 1 + \\frac{1}{\\nu} \\left( \\frac{x-\\mu}{\\sigma}\\right)^2\\right]^{-\\left(\\frac{\\nu + 1}{2}\\right)}$$\n",
    "\n",
    "where $\\mu$ is the mean, $\\sigma^2>0$ is the scale parameter, and $\\nu > 0$ is called the **degrees of freedom**.\n",
    "\n",
    "The distribution has the following properties:\n",
    "\n",
    "mean = $\\mu$, mode = $\\mu$, var = $\\frac{\\nu \\sigma^2}{(\\nu - 2)}$\n",
    "\n",
    "The variance is only defined if $\\nu > 2$. The mean is only defined if $\\nu > 1$. It is common to use $\\nu = 4$, which gives good performance in a range of problems. For $\\nu \\gg 5$, the Student distribution rapidly approaches a Gaussian distribution and loses its robustness properties.\n",
    "\n",
    "## The Laplace distribution\n",
    "\n",
    "Another distribution with heavy tails is the **Laplace distribution**, also known as the **double sided exponential** distribution. This has the following pdf:\n",
    "\n",
    "$$\\text{Lap}(x|\\mu,b) \\triangleq \\frac{1}{2b} \\exp \\left( - \\frac{|x - \\mu|}{b}\\right)$$\n",
    "\n",
    "Here $\\mu$ is a location parameter and $b>0$ is a scale parameter. This distribution has the following properties:\n",
    "\n",
    "mean = $\\mu$, mode = $\\mu$, var = $2b^2$\n",
    "\n",
    "Not only does it have heavier tails, it puts more probability density at 0 than the Gaussian. This property is a useful way to encourage sparsity in a model, as we will see later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Show Gaussian, Student, Laplace pdfs and log pdfs\n",
    "fig, ax = plt.subplots(2, 1, sharex=True)\n",
    "\n",
    "g = lambda x : stats.norm.pdf(x, loc=0, scale=1)\n",
    "t = lambda x : stats.t.pdf(x, df=1, loc=0, scale=1)\n",
    "l = lambda x : stats.laplace.pdf(x, loc=0, scale=1/np.sqrt(2))\n",
    "\n",
    "x = np.arange(-4, 4, 0.1)\n",
    "\n",
    "\n",
    "ax[0].plot(x, g(x), 'b-', label='Gaussian')\n",
    "ax[0].plot(x, t(x), 'r.', label='Student')\n",
    "ax[0].plot(x, l(x), 'g--', label='Laplace')\n",
    "\n",
    "ax[0].legend(loc='best')\n",
    "ax[0].set_title('pdfs')\n",
    "\n",
    "ax[1].plot(x, np.log(g(x)), 'b-', label='Gaussian')\n",
    "ax[1].plot(x, np.log(t(x)), 'r.', label='Student')\n",
    "ax[1].plot(x, np.log(l(x)), 'g--', label='Laplace')\n",
    "ax[1].set_title('log pdfs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Demonstrate fitting Gaussian, Student, and Laplace to data\n",
    "# with and without outliers\n",
    "n = 30  # n data points\n",
    "np.random.seed(0)\n",
    "data = np.random.randn(n)\n",
    "\n",
    "outliers = np.array([8, 8.75, 9.5])\n",
    "nn = len(outliers)\n",
    "nbins = 7\n",
    "\n",
    "# fit each of the models to the data (no outliers)\n",
    "model_g = stats.norm.fit(data)\n",
    "model_t = stats.t.fit(data)\n",
    "model_l = stats.laplace.fit(data)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, sharex=True)\n",
    "\n",
    "x = np.arange(-10, 10, 0.1)\n",
    "\n",
    "g = lambda x : stats.norm.pdf(x, loc=model_g[0], scale=model_g[1])\n",
    "t = lambda x : stats.t.pdf(x, df=model_t[0], loc=model_t[1], scale=model_t[2])\n",
    "l = lambda x : stats.laplace.pdf(x, loc=model_l[0], scale=model_l[1])\n",
    "\n",
    "ax[0].hist(data, bins=25, range=(-10, 10),\n",
    "           normed=True, alpha=0.25, facecolor='gray')\n",
    "ax[0].plot(x, g(x), 'b-', label='Gaussian')\n",
    "ax[0].plot(x, t(x), 'r.', label='Student')\n",
    "ax[0].plot(x, l(x), 'g--', label='Laplace')\n",
    "\n",
    "ax[0].legend(loc='best')\n",
    "ax[0].set_title('no outliers')\n",
    "\n",
    "# fit each of the models to the data (with outliers)\n",
    "newdata = np.r_[data, outliers]  # row concatenation\n",
    "model_g = stats.norm.fit(newdata)\n",
    "model_t = stats.t.fit(newdata)\n",
    "model_l = stats.laplace.fit(newdata)\n",
    "\n",
    "\n",
    "g = lambda x : stats.norm.pdf(x, loc=model_g[0], scale=model_g[1])\n",
    "t = lambda x : stats.t.pdf(x, df=model_t[0], loc=model_t[1], scale=model_t[2])\n",
    "l = lambda x : stats.laplace.pdf(x, loc=model_l[0], scale=model_l[1])\n",
    "\n",
    "ax[1].hist(newdata, bins=25, range=(-10, 10),\n",
    "           normed=True, alpha=0.25, facecolor='gray')\n",
    "ax[1].plot(x, g(x), 'b-', label='Gaussian')\n",
    "ax[1].plot(x, t(x), 'r.', label='Student')\n",
    "ax[1].plot(x, l(x), 'g--', label='Laplace')\n",
    "\n",
    "\n",
    "ax[1].set_title('with outliers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Joint probability distributions\n",
    "\n",
    "A **joint probability distribution** has the form $p(x_1,\\ldots,x_D)$ for a set of $D>1$ variables, and models the (stochastic) relationships between the variables. If all the variables are discrete, we can represent the joint distribution as a big multi-dimensional array, with one variable per dimension. However, the number of parameters needed to define such a model is $O(K^D)$, where $K$ is the number of states for each variable.\n",
    "\n",
    "We can define high dimensional joint distributions using fewer parameters by making conditional independence assumptions. In the case of continuous distributions, an alternative approach is to restrict the form of the pdf to certain functional forms, some of which are examined below.\n",
    "\n",
    "## Covariance and correlation\n",
    "\n",
    "The **covariance** between two rv's $X$ and $Y$ measures the degree to which $X$ and $Y$ are (linearly) related. Covariance is defined as\n",
    "\n",
    "$$\\text{cov}[X,Y] \\triangleq \\mathbb{E}\\left[\\left(X - \\mathbb{E}[X]\\right)\\left(Y - \\mathbb{E}[Y]\\right)\\right]=\\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]$$\n",
    "\n",
    "If $\\mathbf{x}$ is a $d$-dimensional random vector, its **covariance matrix** is defined to be the following symmetric, positive semi-definite matrix:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{cov}[\\mathbf{x}] & \\triangleq \\mathbf{E} \\left[\\left(\\mathbf{x} - \\mathbb{E}[\\mathbf{x}]\\right)\\left(\\mathbf{x} - \\mathbb{E}[\\mathbf{x}]\\right)^T\\right]\\\\\n",
    "& = \\left( \\begin{array}{ccc}\n",
    "  \\text{var}[X_1] & \\text{cov}[X_1, X_2] &  \\ldots & \\text{cov}[X_1, X_d] \\\\\n",
    "  \\text{cov}[X_2, X_1] & \\text{var}[X_2] &  \\ldots & \\text{cov}[X_2, X_d] \\\\\n",
    "  \\vdots               & \\vdots          & \\ddots  & \\vdots\\\\\n",
    "  \\text{cov}[X_d, X_1] & \\text{cov}[X_d, X_2] &  \\ldots & \\text{var}[X_d] \n",
    "  \\end{array} \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Covariances can be between $-\\infty$ and $\\infty$. Sometimes it is more convenient to work with a normalized measure, with finite bounds. The (Pearson) **correlation coefficient** between $X$ and $Y$ is defined as\n",
    "\n",
    "$$\\text{corr}[X,Y] \\triangleq \\frac{\\text{cov}[X,Y]}{\\sqrt{\\text{var}[X]\\text{var}[Y]}}$$\n",
    "\n",
    "A **correlation matrix** has the form\n",
    "\n",
    "$$\n",
    "\\mathbf{R} = \\left( \\begin{array}{ccc}\n",
    "  \\text{corr}[X_1, X_1] & \\text{corr}[X_1, X_2] &  \\ldots & \\text{corr}[X_1, X_d] \\\\\n",
    "  \\text{corr}[X_2, X_1] & \\text{corr}[X_2, X_2] &  \\ldots & \\text{corr}[X_2, X_d] \\\\\n",
    "  \\vdots               & \\vdots          & \\ddots  & \\vdots\\\\\n",
    "  \\text{corr}[X_d, X_1] & \\text{corr}[X_d, X_2] &  \\ldots & \\text{corr}[X_d, X_d] \n",
    "  \\end{array} \\right)\n",
    "$$\n",
    "\n",
    "One can show that $-1 \\leq \\text{corr}[X,Y] \\leq 1$. Hence, in a correlation matrix, each entry on the diagonal is 1, and the other entries are between -1 and 1. One can also show that $\\text{corr}[X,Y]=1$ iff $Y=aX + b$ for some parameters $a$ and $b$, i.e. there is a *linear* relationship between $X$ and $Y$. A good way to think of the correlation coefficient is as a degree of linearity.\n",
    "\n",
    "If $X$ and $Y$ are independent, meaning $p(X,Y)=p(X)p(Y)$, then $\\text{cov}[X,Y]=0$, and hence $\\text{corr}[X,Y]=0$ so they are uncorrelated. However, the converse is not true: *uncorrelated does not imply independent*. Some striking examples are shown below.\n",
    "\n",
    "<img src=\"images/Correlation_examples.png\">\n",
    "\n",
    "\n",
    "Source: http://upload.wikimedia.org/wikipedia/commons/0/02/Correlation_examples.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The multivariate Gaussian\n",
    "\n",
    "The **multivariate Gaussian** or **multivariate normal (MVN)** is the most widely used joint probability density function for continuous variables. The pdf of the MVN in $D$ dimensions is defined by the following\n",
    "\n",
    "$$\\mathcal{N}(\\mathbf{x}|\\boldsymbol\\mu,\\mathbf{\\Sigma}) \\triangleq \\frac{1}{(2 \\pi)^{D/2}|\\mathbf{\\Sigma}|^{1/2}} \\exp \\left[ - \\frac{1}{2} \\left(\\mathbf{x} - \\boldsymbol\\mu \\right)^T \\mathbf{\\Sigma}^{-1} \\left(\\mathbf{x} - \\boldsymbol\\mu\\right)\\right]$$\n",
    "\n",
    "where $\\boldsymbol\\mu = \\mathbb{E}[\\mathbf{x}] \\in \\mathbb{R}^D$ is the mean vector, and $\\Sigma = \\text{cov}[\\mathbf{x}]$ is the $D \\times D$ covariance matrix. Sometimes we will work in terms of the **precision matrix** or **concentration matrix** instead. This is just the inverse covariance matrix, $\\Lambda = \\Sigma^{-1}$. The normalization constant $(2 \\pi)^{-D/2}|\\Lambda|^{1/2}$ ensures that the pdf integrates to 1.\n",
    "\n",
    "The figure below plots some MVN densities in 2d for three different kinds of covariance matrices. A full covariance matrix has $D(D+1)/2$ parameters (we divide by 2 since $\\Sigma$ is symmetric). A diagonal covariance matrix has $D$ parameters, and has 0s on the off-diagonal terms. A **spherical** or **isotropic** covariance, $\\Sigma = \\sigma^2 \\mathbf{I}_D$, has one free parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plot a MVN in 2D and 3D\n",
    "import matplotlib.mlab as mlab\n",
    "from scipy.linalg import eig, inv\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "delta = 0.05\n",
    "x = np.arange(-10.0, 10.0, delta)\n",
    "y = np.arange(-10.0, 10.0, delta)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "S = np.asarray([[2.0, 1.8],\n",
    "                [1.8, 2.0]])\n",
    "mu = np.asarray([0, 0])\n",
    "\n",
    "Z = mlab.bivariate_normal(X, Y, sigmax=S[0, 0], sigmay=S[1, 1], \n",
    "                          mux=mu[0], muy=mu[1], sigmaxy=S[0, 1])\n",
    "\n",
    "#fig, ax = plt.subplots(2, 2, figsize=(10, 10),\n",
    "#                       subplot_kw={'aspect': 'equal'})\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 1)\n",
    "\n",
    "CS = ax.contour(X, Y, Z)\n",
    "plt.clabel(CS, inline=1, fontsize=10)\n",
    "\n",
    "ax.set_xlim((-6, 6))\n",
    "ax.set_ylim((-6, 6))\n",
    "ax.set_title('full')\n",
    "\n",
    "# Decorrelate\n",
    "[D, U] = eig(S)\n",
    "\n",
    "S1 = np.dot(np.dot(U.T, S), U)\n",
    "\n",
    "Z = mlab.bivariate_normal(X, Y, sigmax=S1[0, 0], sigmay=S1[1, 1], \n",
    "                          mux=mu[0], muy=mu[0], sigmaxy=S1[0, 1])\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 2)\n",
    "CS = ax.contour(X, Y, Z)\n",
    "plt.clabel(CS, inline=1, fontsize=10)\n",
    "\n",
    "\n",
    "ax.set_xlim((-10, 10))\n",
    "ax.set_ylim((-5, 5))\n",
    "ax.set_title('diagonal')\n",
    "\n",
    "# Whiten\n",
    "A = np.dot(np.sqrt(np.linalg.inv(np.diag(np.real(D)))), U.T)\n",
    "mu2 = np.dot(A, mu)\n",
    "S2 = np.dot(np.dot(A, S), A.T)  # may not be numerically equal to I\n",
    "\n",
    "\n",
    "#np.testing.assert_allclose(S2, np.eye(2))  # check\n",
    "print np.allclose(S2, np.eye(2))\n",
    "\n",
    "# plot centred on original mu, not shifted mu\n",
    "Z = mlab.bivariate_normal(X, Y, sigmax=S2[0, 0], sigmay=S2[1, 1], \n",
    "                          mux=mu[0], muy=mu[0], sigmaxy=S2[0, 1])\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 3)\n",
    "CS = ax.contour(X, Y, Z)\n",
    "plt.clabel(CS, inline=1, fontsize=10)\n",
    "\n",
    "ax.set_xlim((-6, 6))\n",
    "ax.set_ylim((-6, 6))\n",
    "ax.set_title('spherical')\n",
    "\n",
    "# demonstration of how to do a surface plot\n",
    "axx = fig.add_subplot(2, 2, 4, projection='3d')\n",
    "surf = axx.plot_surface(X, Y, Z, rstride=5, cstride=5, cmap=cm.coolwarm,\n",
    "                        linewidth=0, antialiased=False)\n",
    "axx.set_title('spherical')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
