---
layout: article
title: The unreasonable elegance of deep generative models
comments: true
categories: data_science
image:
  teaser: jupyter-main-logo.svg
---

## RL Philosophy
Reinforcement learning is the story of an agent that knows nothing and wants to learn by interacting with its environment. The environment is usually a black box from agent's point of view meaning that it can only provide the agent with an observation for every action the agent takes.

At the start of interaction, the agent might not know what it wants to do since it doesn't know much about its environment. However, if we go the Freud's route and say that an agent's goal in life should be to maximize its pleasure, then the agent will have an objective (to maximize its pleasure in life, whatever that is).

The agent probably doesn't know itself either as it starts interacting with the environment and starts enjoying certain things as it explores. So in some sense it is developing a models of its own hidden desires (reward function) based on what provides it with pleasure. It also learns to assign a value to different states (state value func - V(s) func) and actions (action value func - Q(a) func) based on the hidden desires(reward). Therefore, it learns an objective function (accumulated reward over time) that it needs to maximize to get the most pleasure in life.

Now that the agent knows what it wants, it starts thinking about what series of actions (policy) it should take to maximize what it wants. In doing so, it will need to develop a model of its environment in its head (prediction) to know what happens if it takes an action (model environment) and whether that action and its consequences are good or bad (value function) from maximum total reward perspective. 

## RL from Supervised learning perspective

In both, we want to learn a mapping from inputs or states to outputs or actions:
    + In supervised learning, we can sample from model, calculated error and optimize parameters. What need to be concerned about is over-fitting and model being certain when it hasn't seen enough data points. 
    + In RL, we can sample from our policy function, calculate reward, and optimize parameters (be greedy). What we need to be concerned about is exploitation before enough exploration. 
    + The key concept that connects both paradigms is uncertainty which is quantified using entropy. If we add entropy to our supervised objective, we are regularizing it so that it doesn't over-fit. If we add entropy to our expected reward, we are encouraging it to explore more before exploitation.

In supervised learning, we usually have an iid assumption between samples of the dataset where in RL, the samples are a trajectory where iid assumption doesn't hold. However, if we have a structured prediction supervised learning (e.g. image captioning), then we have dependencies across the sequence of predictions for each sample.

### Problem setup

We want to learn a mapping $$X \to a$$:
    + from the inputs $$X = [x_1, x_2, ..., X_T]$$. (Inputs might be a trajectory, i.e. iid assumption won't hold)
    + to outputs / actions $$a = [a_1, a_2, ... , a_T]$$
    + that maximizes a non-decomposable expected reward (objective) function $$r(a|X)$$
        - In classical supervised learning, the objective is decomposable due to iid assumption of samples to the distance of model output with target.
        - In RL, the instead of target values, we have a bunch of rating/reward functions that may be sparse (not available for all inputs), and also may be delayed till the end. 
    + Model or policy is probability of actions/outputs given inputs $$\pi_\theta (a|x) = \prod_i \pi_\theta (a_i|X_{1,..., i-1}, a_{1,..., i-1})$$. 
    + The policy or model that maximizes the expected reward or objective, $$E[r(a|X)]$$, is the optimal policy/model $$\pi^* (a|x)$$. If actions are discrete, the optimal policy is represented using a categorical variable. We can use the softmax trick to make it continuous with a temprature parameter ($$\tau$$), i.e. $$\pi^*(a|x) = \frac{1}{Z} \exp(\frac{r(a|x)}{\tau})$$.
        - If we use the soft optimal model, the conditional log-likelihood of the model is exactly the KL divergence between the optimal model and the model i.e. $$KL[\frac{\pi^* (a|x)}{\pi_\theta (a|x)}]$$ when the temprature parameter is zero.
        - If we use the soft optimal policy, the expected reward is exactly the KL divergence in the other direction, i.e. between the policy function and the optimal policy i.e. $$KL[\frac{\pi_\theta (a|x)}{\pi^* (a|x)}]$$ again at temperature zero. 

In classical supervised learning, we minimize the conditional log-likelihood objective function, while in RL, we optimize the expected reward function. These two are actually pretty much very close concepts but are not well understood. The key question is what divergence functions can we use to better do this optimization in both domains. 
    + We can use a non-zero temperature and add it to the maximum likelihood objective in terms of expected reward + tempreture * entropy. [Peng & Williams]
    + we can define a conditional log-likelihood at temperatures higher than zero can call that reward augmented maximum likelihood. [Norouzi et al.]
    + We can combine the two directions of the KL to benefit from both mode seeking (RL case because it samples from policy distribution) and mode covering (Supervised learning case because we are sampling from the optimal model distribution) [UREX -> Norouzi et al]
    + We can optimze the entropy-regularized expected reward with partial rewards that are decomposed (bridging the value & policy based RL)

## RL from control perspective:
Reinforcement learning is the study of how to use past data to enhance the future manipulation of a dynamical system. How does this differ from ordinary machine learning? The main view of this survey is of reinforcement learning as optimal control when the dynamics are unknown. Our goal will be to find a set of inputs that drives a dynamical system to maximize some objective beginning with minimal knowledge of how the system responds to inputs.

In the classic optimal control problem, we begin with a dynamical system governed by the difference equation xt+1 = ft(xt, ut , et) where (xt) is the state of the system, (ut) is the control action, and (et) is a random disturbance. (ft) is the rule that maps the current state, control action, and disturbance at time t to a new state. Assume that at every time, we receive some reward R(xt , ut) for our current (xt) and (ut). The goal is to maximize this reward. In terms of mathematical optimization, we aim to maximize the expected reward over N time steps with respect to the control sequence (ut), subject to the dynamics specified by the state transition rule (ft). The expected value is over the disturbance, and assumes that (ut) is to be chosen having only seen the states (x0) through (xt) and previous inputs (u0) through (ut−1). (Rt) is the reward gained at each time step and is determined by the state and control action. Note that (xt) is not really a decision variable in the optimization problem: it is completely determined by the previous state, control action, and disturbance (i.e. trajectory).

rather than optimizing over deterministic sequences of controls (ut), we instead optimize over policies. A control policy (or simply “a policy”) is a function, π, that takes a trajectory from a dynamical system and outputs a new control action. Note that π only gets access to previous states and control actions.

- In short, We can use RL when ALL following conditions are met:
    1. [States] When data comes in the form of **trajectories** (there is dependency in data so iid assumption doesn't hold)
    2. [Actions] Need to make a sequence of **decisions** (that change the trajectory)
    3. [Observations] We can observe **feedbacks** of actions in the form of new states since the actions change the environment.

### Model-based vs. Model-free RL:
The function f governs the dynamics of the system (aka model). Model-free methods aim to solve optimal control problems only by probing the system and improving strategies based on past rewards and states. Many researchers argue for algorithms that can innately learn to control without access to the complex details required to simulate a dynamical system. They argue that it is often easier to find a policy for a task than it is to fit a general purpose model of the system dynamics. Model-free methods are primarily divided into two approaches: 
 - Policy Search: directly searches for policies by using data from previous episodes (trajectories) in order to improve the reward.
 - Approximate Dynamic Programming: uses Bellman’s principle of optimality, i.e. using previously observed data, to approximate a policy that maximizes the expectation of the reward.

What is important to note is that all of the approaches surveyed reduce to some sort of function fitting from noisy observations of the dynamical system.In model-based reinforcement learning, we fit a model of the state transitions to best match observed trajectories. In approximate dynamic programming, we fit a reward function that best characterizes the “cost to go” for experimentally observed states. And in direct policy search, we attempt to find a policy that directly maximizes the optimal control problem using only input-output data.

- Can we estimate the state space (model) using latent variable models (VAEs)?
    + see [LFADS](https://arxiv.org/pdf/1608.06315.pdf)


### formulating the RL problem
To be able to create agents and let them learn, we need to formulate the problem. Let's use the amazing tools of probability theory to formulate the agent's life story. 

Suppose you are an agent, situated in an environment. The environment is in a certain state. The agent can perform certain actions in the environment. These actions sometimes result in a reward. Actions transform the environment and lead to a new state.

- some definitions:
    - State: States are basically data, therefore we need to define the state in problem formulation as sufficient information that can predict an action. 
    - The environment consists of a set of possible states $$S = {s_0, s_1, ... s_n}$$.
    - States transition to one another as time goes by based on the past visited states, and the actions received by environment $$p(s_{i+1}|s_0, s_1, .., s_i, a_0, a_1, .. , a_i)$$. 
    -  Policy: a sequence of actions to go from an initial state to a target state (conditional probability of actions given past states).
    -  Model: Agent's model of the environment (conditional probability of next state given past states)
    -  Reward: objective function assigning signal of success/failure. (an atomic signal per sample)
    -  Return: If the problem is finite horizon, it might be just sum of rewards. If it is infinite horizon, it is usually discounted sum (weighted sum) of future rewards.
    -  Value Function: Expected reward. If the problem is deterministic expectation of reward is the same as return. It assesses how good each state/action is (averaged future reward accumulated from each particular state/action). 
    -  credit assignment problem: which of the preceding actions was responsible for getting the reward and to what extent.
    -  explore-exploit dilemma: Once you have figured out a strategy to collect a certain number of rewards, should you stick with it or experiment with something that could result in even bigger rewards?
    - The other way to view eligibility traces is more mechanistic. From this perspective, an eligibility trace is a temporary record of the occurrence of an event, such as the visiting of a state or the taking of an action. The trace marks the memory parameters associated with the event as eligible for undergoing learning changes. When a TD error occurs, only the eligible states or actions are assigned credit or blame for the error. Thus, eligibility traces help bridge the gap between events and training information. Like TD methods themselves, eligibility traces are a basic mechanism for temporal credit assignment.


We can think of RL as a structured semi-supervised problem whereas in supervised learning one has a label for each training example and in unsupervised learning one has no labels at all, in RL one has sparse and time-delayed labels – the rewards. 


As with all other real problems we make simplifying assumptions to be able to model this phenomenon.

An agent at state s, takes action a, and the environment produces a reward r, and an observation that shifts the state of the agent to s+1. RL methods are employed to address two related problems:
    + prediction: learn value function for a certain policy to predict how far it is from its objective at every state.
        * dynamic programming:
        * Monte Carlo method:
            - calculate the gradient using reinforce gradient
        * evolution:
            - this can estimate reinforce gradient
        * temporal difference (TD) learning: updating a guess on the basis of another guess.
    + control: learn policy function.

If we know state transition function (agent model) and the reward function (environment model), algos are called model-based algos. It that case, dynamic programming based algorithms i.e. 1)value iteration and 2) policy iteration can be used to solve for the value function and the optimal policy respectively. If we don't know models (transition and reward functions), then the optimal value function and/or the optimal policy will have to be learned. For example:
    + Actor-critic methods are adaptive policy iteration algos which approximate value function using TD.
    + Q-learning: a unifying algorithm which allows for simultaneous value function and policy optimization.


There are two scenarios in RL, off-policy vs on-policy:
    + on-policy: the agent follows the policy that it is trying to optimize
    + off-policy: the agent isn't following the policy it is trying to optimize. For example, Q-learning. Therefore we can have an importance sampling ratio which is basically policy ratio.


### Markov Decision Process:
The set of states and actions, together with rules for transitioning from one state to another (environment), make up a Markov decision process (MDP). MDPs have no memory meaning that the decision at each state depends only on the current state and action and not on past states. An agent starts at a certain state s_0, takes action a_0, collects reward r_0 and visits new state s_1 and repeat. The goal of the agent is to maximize the expected cumulative reward.
    + A set of states [s1,s2,…,sM]
    + A set of actions [a1,a2,…,aN]
    + A set of rewards [r1,r2,…,rL]
    + A set of transition probabilities [s11,s12,…,s1M,s21,s22,…,s2M,…,sMM][]

- The environment provides: 
    + A reward function R(s,a) which gives us the reward that comes from taking an action a_t in state s_t at time t. 
    + A transition function S(s,a) which will give us the next state . The actions a_t are generated by the agent by following one or several policies. 

- Find a policy function (i.e. density function of joint P(s,a)) that generates a series of actions to get maximum reward in the future.


- To perform well in the long-term, we need to take into account not only the immediate rewards, but also the future rewards we are going to get. because our environment is stochastic, we can never be sure, if we will get the same rewards the next time we perform the same actions. The more into the future we go, the more it may diverge. For that reason it is common to use discounted future reward (from point t onward). 

$$Return_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + ..$$

- If we set the discount factor γ=0, then our strategy will be short-sighted and we rely only on the immediate rewards. If we want to balance between immediate and future rewards, we should set discount factor to something like γ=0.9. If our environment is deterministic and the same actions always result in same rewards, then we can set discount factor γ=1.



### Algorithms
Let's start by looking at the value function. For the stochastic case, value function is the expectation of sum of rewards

$$V(s,a) = E[r_1, r_2, .. , r_n]$$

We can break down this expectation to immediate reward for current state and other reward terms. This results in a recursive formulation for the value function that is called the Bellman equation. 

Value at state $$s$$ by taking action $$a$$ is the sum over actions of the policy times reward for this specific action plus transition to next state times the value at the next state.

$$V(s,a) = \sum_a \pi(s,a) r(s,a) + T(s,s')V(s',a')$$ 

If states and actions are finite, the Bellman equation will be simply a system of linear equations.

#### policy evaluation (dynamic programming):
We've got a fixed policy, we want to evaluate the values of states (V(s) - value function). we initialize all state values to zeros. then for each state, we follow the fixed policy till we get the reward. Then we can assign a value to that state. We repeat this process until the difference betwee the two consequtive values of a state are smaller than a threshold. 

If I simply discretize the Bellman equation and evaluate the value function iteratively for a specific policy starting from a random value and updating using the Bellman equation, I get policy evaluation algorithm. complexity is O(S^3). 

The next step is to modify the policy based on the learned value function V(s). We go through each state, then evaluate what is the best action I can take based on the value of that state. 

#### Policy iteration:
If I want to learn a good policy, a naive way is to start from a random policy, evaluate it using the policy evaluation method explained above, and greedily update the policy. Continuing this loop until convergence is called policy iteration. Complexity is O(S^3+A S^2) for each iteration. 

#### Value iteration
An insight from policy iteration is that maybe I don't need to evaluate the whole policy in each iteration and just update policy greedily with just a single policy evaluation iteration which means that I am integrating the policy iteration update into the policy evaluation loop. This has the complexity O(N^2) for each iteration. The problem is then how many iteration do I need for convergence? In practice value iteration needs many much more iterations than policy iteration, which is why policy iteration and value iteration may be used in practice. 

#### Monte Carlo Learning
a sample based method that doesn't assume a model of the environment is available. 

The idea is to iteratively update the value function using a notion of the gradient or error of the value function:

$$V(s_i) <- V(s_i) + \alpha (U(s_i) -  V(s_i))$$

Here the U is a notion of the ground truth. It is actually the empirical return calculated via Monte Carlo, which means that instead of waiting till the end to estimate the return, we use empirical returns and take a Monte Carlo average. This requires many evaluations of the empirical return so that we can estimate the expected return with a Monte Carlo estimate. 

#### Temporal Difference (TD) Learning
problem: for a given policy, compute the state value function V(s).

Here the idea is similar to Monte Carlo learning, but the notion of error/gradient is defined using an estimate of the Bellman equation


$$V(s_t) <- V(s_t) + \alpha ([r_{t} + \gamma V(s_{t+1})] -  V(s_i))$$

Here, the term $$[r_{t+1} + \gamma V(s_{t+1})]$$ is coming from the Bellman equation estimate for value. It is not exactly a ground truth value since it depends on the policy/model and reward estimates. If those two are poorly estimated, then TD learning won't work very well. 

#### Deep RL

The idea in deep RL is to use the raw pixels of Atari for example as states/inputs and use a neural net to learn the value function (Q-function) that assigns a numerical value to each state (pixel combinations) of the game. Many deep learning advancements like conv nets, memory, attentions, etc are all used in the neural net to better learn the Q-function.


#### Q-Learning

- Q-learning attempts to solve the credit assignment problem – it propagates rewards back in time, until it reaches the crucial decision point which was the actual cause for the obtained reward.

- In Q-learning we define a function Q(s, a) as the value function representing the maximum discounted future reward when we perform action a in state s, and continue optimally from that point on. it is “the best possible score at the end of the game after performing action a in state s“. It is called Q-function, because it represents the “quality” of a certain action in a given state. At each time point You want to select the action that results in the highest score at the end of game. Once you have the magical Q-function, the answer (policy) becomes really simple – pick the action with the highest Q-value!


- Maximum future reward for this state and action is the immediate reward plus maximum future reward for the next state. Therefore, we can express the Q-value of state s and action a in terms of the Q-value of the next state s' as $$Q(s,a) = r + \gamma max_{a'} Q(s',a')$$. This is called the Bellman equation. The main idea in Q-learning is that we can iteratively approximate the Q-function using the above Bellman equation by randomly choosing actions and exploring different states. 

- The max_{a'} Q(s',a') that we use to update Q(s,a) is only an approximation and in early stages of learning it may be completely wrong. However the approximation gets more and more accurate with every iteration and it has been shown, that if we perform this update enough times, then the Q-function will converge and represent the true Q-value.

- Deep Q-learning: We could represent our Q-function with a neural network, that takes the state (four game screens) and action as input and outputs the corresponding Q-value. Alternatively we could take only game screens as input and output the Q-value for each possible action. Q-values can be any real values, which makes it a regression task, that can be optimized with simple squared error loss.
    + Do a feedforward pass for the current state s to get predicted Q-values for all actions.
    + Do a feedforward pass for the next state s’ and calculate maximum overall network outputs max a’ Q(s’, a’).
    + Set Q-value target for action to r + γmax a’ Q(s’, a’) (use the max calculated in step 2). For all other actions, set the Q-value target to the same as originally returned from step 1, making the error 0 for those outputs.
    + Update the weights using backpropagation.

- A bunch of tricks are used in practice. The most important trick is experience replay. During gameplay all the experiences < s, a, r, s’ > are stored in a replay memory. When training the network, random minibatches from the replay memory are used instead of the most recent transition. This breaks the similarity of subsequent training samples, which otherwise might drive the network into a local minimum. Also experience replay makes the training task more similar to usual supervised learning, which simplifies debugging and testing the algorithm. One could actually collect all those experiences from human gameplay and then train network on these.


- when a Q-table or Q-network is initialized randomly, then its predictions are initially random as well. If we pick an action with the highest Q-value, the action will be random and the agent performs crude “exploration”. As a Q-function converges, it returns more consistent Q-values and the amount of exploration decreases. So one could say, that Q-learning incorporates the exploration as part of the algorithm. But this exploration is “greedy”, it settles with the first effective strategy it finds. A simple and effective fix for the above problem is ε-greedy exploration – with probability ε choose a random action, otherwise go with the “greedy” action with the highest Q-value.

#### Policy Gradient (Reinforce):
Since the reward is not differentiable w.r.t. the discrete actions taken (actions are samples from a multinomial on softmax output of the classifier), we can't backprop throught the actions. The problem is that we need the gradient of the expectation of the reward over all taken actions, to change the parameters of the policy neural net with gradient ascent in order to maximize the expected reward. The question now is how to compute the gradient of the expected reward:

$$\grad_\theta \EE_\tau[R_\tau]$$

In a more general form, if we need the gradient of an expection on a function that may not be differentiable, what can we do?

$$\nabla_\theta \mathbb{E}_p(z;\theta) [f(z)] = \nabla_\theta \int p(z;\theta) f(z) dz$$

This gradient is difficult to compute because the expectation integral is typically unknown and the parameters $$\theta$$ with respect to which we are computing the gradient, are of the distribution $$p(z;\theta)$$. Furthermore, we might want to compute this gradient when the function f is not differentiable. First we take the derivative inside the integral (We can reason about the correctness of this by appealing to the Leibniz integral rule):

$$\nabla_\theta \mathbb{E}_p(z;\theta) [f(z)] = \int \nabla_\theta p(z;\theta)f(z)dz$$

Then We use reinforce or policy gradient trick where we replace the gradient of the probability distribution on actions (i.e. the softmax output of the network where we use a multinomial distribution to sample),  with the gradient of log of that probability times the probability, i.e. 

$$ \nabla_\theta p(z;\theta) = \nabla_\theta \log p(z;\theta) p(z;\theta)$$.

We can then simply write the original expectation in terms of an expectation on the derivative of log probability of the policy network outputs.

$$\nabla_\theta \mathbb{E}_p(z;\theta) [f(z)] = \mathbb{E}_p(z;\theta) [f(z) \nabla_\theta \log p(z;\theta)]$$ 

This expectation is easy to calculate by running the simulation, sampling n actions from the softmax output of the policy neural net, and using a Monte carlo (average) estimator:

$$\frac{1}{n} \sum_n f(z(n)) \nabla_\theta \log p(z(n);\theta)$$

Note that The function f(z) need not be differentiable. Instead, we should be able to evaluate it or observe its value for a given action (z). Additionally, obtaining samples from the distribution p(z;\theta) (i.e. policy network) should be easy, since this is needed for Monte Carlo evaluation of the integral. In other words, given a trajectory sampled according to the policy function, we can use above formula as an unbiased estimator for the gradient of the original expectation. If we sample enough trajectory and use the average instead of the expectation, we get a good and unbiased estimate, but the variance may be high.

This process is implemented in pytorch easily as: 

```python
# forward pass
action_probabilities = self.model(Variable(state))  # softmax output on actions     
selected_actions = action_probabilities.multinomial().data # sample from a categorical
probs = action_probabilities[:, selected_action[0,0]].view(1, -1) # logits for selected actions
log_probs = probs.log() # log-probability
# Reinforce 
loss = ( - log_probs[i]* R)
# backward pass
loss.backward()
```


#### Actor Critic and A3C

The only difference between policy gradient and actor critic is that we now have an additional network 'critic' that evaluates each (state, action) pair and tries to estimate the local reward. 

It is usually implemented as a neural net that is used for both the policy network and value network. Simply, the state goes in as input to the NN, and the NN has two sets of outputs. The first output is a classifier on discrete actions which predicts next action (a multinomial sample from softmax probabilities) and regression output that predicts the value of the current action. This network is run in a simulator of the environment where we start from an initial state (NN input), decide an action and send it to simulator, get the next state and so on till the simulator gives us finish signal. At that point we recieve the reward.

#### implementation

- discounted expected reward: if your environment gives you a local reward for each (state, action) pair,  you need to calculated sum of discounted rewards and maximize that.  starting at the end of episode, discount the sum of rewards of future actions and add to each local reward.
```python
for r in policy.local_rewards[::-1]:
    R = r + args.gamma * R
    rewards.insert(0, R)

# and then discounted expected reward is normalized
rewards = (rewards - rewards.mean()) / (rewards.std() + eps)

```
- Single reward at end of episode (sparse reward): If your environment gives you only a single reward value at the end of the episode,  then just use that value for all (state, action) pairs. 

```python
rewards = [R for _ in range(len(policy.saved_log_probs))]
```

- Reinforce Loss: Reinforce just encourages the chosen actions in an episode/trajectory by the rewards of the episode/trajectory, so we simply sum the log_probabilities of chosen actions and multiply them by the discounted expected reward for those actions

```python
for log_prob, r in zip(policy.saved_log_probs, rewards):
    policy_loss.append(-log_prob * r)
```

- Actor and Critic Losses: This loss is just the sum of reinforce loss and the value (critic) loss. The only difference is that instead of using the pure discounted expected reward in reinforce, we reinforce taken actions by the difference between discounted expected reward and value. That difference is used to adjust the expected value of that action from that state.
```python
for (log_prob, value), r in zip(saved_actions, rewards):
    reward = r - value.item()
    policy_losses.append(-log_prob * reward)
    value_losses.append(F.smooth_l1_loss(value, torch.tensor([r])))

loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()

```

-  Running reward: we usually use a threshold on running reward as a condition for stopping RL simulation.
```python
running_reward = 10
for i_episode in count(1):
    t = run_episode() # forward pass, return how long it took
    running_reward = running_reward * 0.99 + t * 0.01
    finish_episode() # backward pass and gradients
    if running_reward > env.spec.reward_threshold:
                print("Solved! Running reward is now {} and "
                      "the last episode runs to {} time steps!".format(running_reward, t))
```

### optimization

Given a parametrised policy $\pi(a | x; \theta)$, the basic idea is to use gradient ascent to optimise $\theta$ to maximise the expected total reward. At each step of the optimisation, we compute $\grad_\theta \EE_\tau[R_\tau]$, and then update our parameter vector $\theta$ by a small amount, called the learning rate, $\alpha > 0$:

$$ \theta \leftarrow \theta + \alpha \grad_\theta \EE_\tau[R_\tau]. $$

If we gradually decrease the learning rate over time, then this will converge to a local optimum. In practice, even though we may not reach the global optimum, this general idea is effective at solving these kinds of optimisation problems.


