---
layout: article
title: The unreasonable elegance of deep generative models
comments: true
categories: data_science
image:
  teaser: jupyter-main-logo.svg
---

## Background
Reinforcement learning is the story of an agent that knows nothing and wants to learn by interacting with its environment. The environment is usually a black box from agent's point of view that can only provide the agent with an observation for every action the agent takes.

At the start of interaction, the agent might not know what it wants to do since it doesn't know much about its environment. However, if we go the Freud's route and say that an agent's goal in life should be to maximize its pleasure, then the agent will have an objective (to maximize its pleasure in life whatever that is). 5

The agent probably doesn't know itself either as it starts interacting with the environment and starts enjoying certain things as it explores. So in some sense it is developing a models of its own hidden desires (reward function) based on what provides it with pleasure. Therefore, it learns an objective function (accumulated reward over time) that it needs to maximize to get the most pleasure in life.

Now that the agent knows what it wants, it starts thinking about what series of actions (policy) it should take to maximize what it wants. In doing so, it will need to develop a model of its environment in its head (prediction) to know what happens if it takes an action (model environment) and whether that action and its consequences are good or bad (value function) from maximum total reward perspective. 


### We can use RL when ALL following conditions are met:
    1. [States] When data comes in the form of **trajectories** (there is dependency in data so iid assumption doesn't hold)
    2. [Actions] Need to make a sequence of **decisions** (that change the trajectory)
    3. [Observations] We can observe **feedbacks** of actions in the form of new states since the actions change the environment.

## formulating the problem
To be able to create agents and let them learn, we need to formulate the problem. Let's use the amazing tools of probability theory to formulate the agent's life story. 

Suppose you are an agent, situated in an environment. The environment is in a certain state. The agent can perform certain actions in the environment. These actions sometimes result in a reward. Actions transform the environment and lead to a new state.





## How do Supervised learning and RL relate?

In both, we want to learn a mapping from inputs or states to outputs or actions:
    + In supervised learning, we can sample from model, calculated error and optimize parameters. What need to be concerned about is over-fitting and model being certain when it hasn't seen enough data points. 
    + In RL, we can sample from our policy function, calculate reward, and optimize parameters (be greedy). What we need to be concerned about is exploitation before enough exploration. 
    + The key concept that connects both paradigms is uncertainty which is quantified using entropy. If we add entropy to our supervised objective, we are regularizing it so that it doesn't over-fit. If we add entropy to our expected reward, we are encouraging it to explore more before exploitation.

In supervised learning, we usually have an iid assumption between samples of the dataset where in RL, the samples are a trajectory where iid assumption doesn't hold. However, if we have a structured prediction supervised learning (e.g. image captioning), then we have dependencies across the sequence of predictions for each sample.

### Problem setup

We want to learn a mapping $$X \to a$$:
    + from the inputs $$X = [x_1, x_2, ..., X_T]$$. (Inputs might be a trajectory, i.e. iid assumption won't hold)
    + to outputs / actions $$a = [a_1, a_2, ... , a_T]$$
    + that maximizes a non-decomposable expected reward (objective) function $$r(a|X)$$
        - In classical supervised learning, the objective is decomposable due to iid assumption of samples to the distance of model output with target.
        - In RL, the instead of target values, we have a bunch of rating/reward functions that may be sparse (not available for all inputs), and also may be delayed till the end. 
    + Model or policy is probability of actions/outputs given inputs $$\pi_\theta (a|x) = \prod_i \pi_\theta (a_i|X_{1,..., i-1}, a_{1,..., i-1})$$. 
    + The policy or model that maximizes the expected reward or objective, $$E[r(a|X)]$$, is the optimal policy/model $$\pi^* (a|x)$$. If actions are discrete, the optimal policy is represented using a categorical variable. We can use the softmax trick to make it continuous with a temprature parameter ($$\tau$$), i.e. $$\pi^*(a|x) = \frac{1}{Z} \exp(\frac{r(a|x)}{\tau})$$.
        - If we use the soft optimal model, the conditional log-likelihood of the model is exactly the KL divergence between the optimal model and the model i.e. $$KL[\frac{\pi^* (a|x)}{\pi_\theta (a|x)}]$$ when the temprature parameter is zero.
        - If we use the soft optimal policy, the expected reward is exactly the KL divergence in the other direction, i.e. between the policy function and the optimal policy i.e. $$KL[\frac{\pi_\theta (a|x)}{\pi^* (a|x)}]$$ again at temperature zero. 

In classical supervised learning, we minimize the conditional log-likelihood objective function, while in RL, we optimize the expected reward function. These two are actually pretty much very close concepts but are not well understood. The key question is what divergence functions can we use to better do this optimization in both domains. 
    + We can use a non-zero temperature and add it to the maximum likelihood objective in terms of expected reward + tempreture * entropy. [Peng & Williams]
    + we can define a conditional log-likelihood at temperatures higher than zero can call that reward augmented maximum likelihood. [Norouzi et al.]
    + We can combine the two directions of the KL to benefit from both mode seeking (RL case because it samples from policy distribution) and mode covering (Supervised learning case because we are sampling from the optimal model distribution) [UREX -> Norouzi et al]
    + We can optimze the entropy-regularized expected reward with partial rewards that are decomposed (bridging the value & policy based RL)


### environment:
- State: States are basically data, therefore we need to define the state in problem formulation as sufficient information that can predict an action. 
- The environment consists of a set of possible states $$S = {s_0, s_1, ... s_n}$$.
- States transition to one another as time goes by based on the past visited states, and the actions received by environment $$p(s_{i+1}|s_0, s_1, .., s_i, a_0, a_1, .. , a_i)$$. 
-  Policy: a sequence of actions to go from an initial state to a target state (conditional probability of actions given past states).
-  Model: Agent's model of the environment (conditional probability of next state given past states)
-  Reward: objective function assigning signal of success/failure. (an atomic signal per sample)
-  Return: If the problem is finite horizon, it might be just sum of rewards. If it is infinite horizon, it is usually discounted sum (weighted sum) of future rewards.
-  Value Function: Expected reward. If the problem is deterministic expectation of reward is the same as return. It assesses how good each state/action is (averaged future reward accumulated from each particular state/action). 
-  credit assignment problem: which of the preceding actions was responsible for getting the reward and to what extent.
-  explore-exploit dilemma: Once you have figured out a strategy to collect a certain number of rewards, should you stick with it or experiment with something that could result in even bigger rewards?



## modeling

We can think of RL as a structured semi-supervised problem whereas in supervised learning one has a label for each training example and in unsupervised learning one has no labels at all, in RL one has sparse and time-delayed labels – the rewards. 


As with all other real problems we make simplifying assumptions to be able to model this phenomenon.

An agent at state s, takes action a, and the environment produces a reward r, and an observation that shifts the state of the agent to s+1. RL methods are employed to address two related problems:
    + prediction: learn value function for a certain policy to predict how far it is from its objective at every state.
        * dynamic programming:
        * Monte Carlo method:
            - calculate the gradient using reinforce gradient
        * evolution:
            - this can estimate reinforce gradient
        * temporal difference (TD) learning: updating a guess on the basis of another guess.
    + control: learn policy function.

If we know state transition function (agent model) and the reward function (environment model), algos are called model-based algos. It that case, dynamic programming based algorithms i.e. 1)value iteration and 2) policy iteration can be used to solve for the value function and the optimal policy respectively. If we don't know models (transition and reward functions), then the optimal value function and/or the optimal policy will have to be learned. For example:
    + Actor-critic methods are adaptive policy iteration algos which approximate value function using TD.
    + Q-learning: a unifying algorithm which allows for simultaneous value function and policy optimization.








There are two scenarios in RL, off-policy vs on-policy:
    + on-policy: the agent follows the policy that it is trying to optimize
    + off-policy: the agent isn't following the policy it is trying to optimize. For example, Q-learning. Therefore we can have an importance sampling ratio which is basically policy ratio.





- The other way to view eligibility traces is more mechanistic. From this perspective, an eligibility trace is a temporary record of the occurrence of an event, such as the visiting of a state or the taking of an action. The trace marks the memory parameters associated with the event as eligible for undergoing learning changes. When a TD error occurs, only the eligible states or actions are assigned credit or blame for the error. Thus, eligibility traces help bridge the gap between events and training information. Like TD methods themselves, eligibility traces are a basic mechanism for temporal credit assignment.



## Markov Decision Process:
The set of states and actions, together with rules for transitioning from one state to another (environment), make up a Markov decision process (MDP). MDPs have no memory meaning that the decision at each state depends only on the current state and action and not on past states. An agent starts at a certain state s_0, takes action a_0, collects reward r_0 and visits new state s_1 and repeat. The goal of the agent is to maximize the expected cumulative reward.
    + A set of states [s1,s2,…,sM][]
    + A set of actions [a1,a2,…,aN][]
    + A set of rewards [r1,r2,…,rL][]
    + A set of transition probabilities [s11,s12,…,s1M,s21,s22,…,s2M,…,sMM][]

- The environment provides: 
    + A reward function R(s,a) which gives us the reward that comes from taking an action a_t in state s_t at time t. 
    + A transition function S(s,a) which will give us the next state . The actions a_t are generated by the agent by following one or several policies. 

- Find a policy function (i.e. density function of joint P(s,a)) that generates a series of actions to get maximum reward in the future.


- To perform well in the long-term, we need to take into account not only the immediate rewards, but also the future rewards we are going to get. because our environment is stochastic, we can never be sure, if we will get the same rewards the next time we perform the same actions. The more into the future we go, the more it may diverge. For that reason it is common to use discounted future reward (from point t onward). 

$$Return_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + ..$$

- If we set the discount factor γ=0, then our strategy will be short-sighted and we rely only on the immediate rewards. If we want to balance between immediate and future rewards, we should set discount factor to something like γ=0.9. If our environment is deterministic and the same actions always result in same rewards, then we can set discount factor γ=1.



## Algorithms
Let's start by looking at the value function. For the stochastic case, value function is the expectation of sum of rewards

$$V(s,a) = E[r_1, r_2, .. , r_n]$$

We can break down this expectation to immediate reward for current state and other reward terms. This results in a recursive formulation for the value function that is called the Bellman equation. 

Value at state $$s$$ by taking action $$a$$ is the sum over actions of the policy times reward for this specific action plus transition to next state times the value at the next state.

$$V(s,a) = \sum_a \pi(s,a) r(s,a) + T(s,s')V(s',a')$$ 

If states and actions are finite, the Bellman equation will be simply a system of linear equations.

### policy evaluation (dynamic programming):
If I simply discretize the Bellman equation and evaluate the value function iteratively for a specific policy starting from a random value and updating using the Bellman equation, I get policy evaluation algorithm. complexity is O(S^3)

### Policy iteration:
If I want to learn a good policy, a naive way is to start from a random policy, evaluate it using the policy evaluation method explained above, and greedily update the policy. Continuing this loop until convergence is called policy iteration. Complexity is O(S^3+A S^2) for each iteration. 

### Value iteration
An insight from policy iteration is that maybe I don't need to evaluate the whole policy in each iteration and just update policy greedily with just a single policy evaluation iteration which means that I am integrating the policy iteration update into the policy evaluation loop. This has the complexity O(N^2) for each iteration. The problem is then how many iteration do I need for convergence? In practice value iteration needs many much more iterations than policy iteration, which is why policy iteration and value iteration may be used in practice. 

### Monte Carlo Learning
The idea is to iteratively update the value function using a notion of the gradient or error of the value function:

$$V(s_i) <- V(s_i) + \alpha (U(s_i) -  V(s_i))$$

Here the U is a notion of the ground truth. It is actually the empirical return calculated via Monte Carlo, which means that instead of waiting till the end to estimate the return, we use empirical returns and take a Monte Carlo average. This requires many evaluations of the empirical return so that we can estimate the expected return with a Monte Carlo estimate. 

### Temporal Difference (TD) Learning
Here the idea is similar to Monte Carlo learning, but the notion of error/gradient is defined using an estimate of the Bellman equation


$$V(s_t) <- V(s_t) + \alpha ([r_{t} + \gamma V(s_{t+1})] -  V(s_i))$$

Here, the term $$[r_{t+1} + \gamma V(s_{t+1})]$$ is coming from the Bellman equation estimate for value. It is not exactly a ground truth value since it depends on the policy/model and reward estimates. If those two are poorly estimated, then TD learning won't work very well. 

### Deep RL

The idea in deep RL is to use the raw pixels of Atari for example as states/inputs and use a neural net to learn the value function (Q-function) that assigns a numerical value to each state (pixel combinations) of the game. Many deep learning advancements like conv nets, memory, attentions, etc are all used in the neural net to better learn the Q-function.


### Q-Learning

- Q-learning attempts to solve the credit assignment problem – it propagates rewards back in time, until it reaches the crucial decision point which was the actual cause for the obtained reward.

- In Q-learning we define a function Q(s, a) as the value function representing the maximum discounted future reward when we perform action a in state s, and continue optimally from that point on. it is “the best possible score at the end of the game after performing action a in state s“. It is called Q-function, because it represents the “quality” of a certain action in a given state. At each time point You want to select the action that results in the highest score at the end of game. Once you have the magical Q-function, the answer (policy) becomes really simple – pick the action with the highest Q-value!


- Maximum future reward for this state and action is the immediate reward plus maximum future reward for the next state. Therefore, we can express the Q-value of state s and action a in terms of the Q-value of the next state s' as $$Q(s,a) = r + \gamma max_{a'} Q(s',a')$$. This is called the Bellman equation. The main idea in Q-learning is that we can iteratively approximate the Q-function using the above Bellman equation by randomly choosing actions and exploring different states. 

- The max_{a'} Q(s',a') that we use to update Q(s,a) is only an approximation and in early stages of learning it may be completely wrong. However the approximation gets more and more accurate with every iteration and it has been shown, that if we perform this update enough times, then the Q-function will converge and represent the true Q-value.

- Deep Q-learning: We could represent our Q-function with a neural network, that takes the state (four game screens) and action as input and outputs the corresponding Q-value. Alternatively we could take only game screens as input and output the Q-value for each possible action. Q-values can be any real values, which makes it a regression task, that can be optimized with simple squared error loss.
    + Do a feedforward pass for the current state s to get predicted Q-values for all actions.
    + Do a feedforward pass for the next state s’ and calculate maximum overall network outputs max a’ Q(s’, a’).
    + Set Q-value target for action to r + γmax a’ Q(s’, a’) (use the max calculated in step 2). For all other actions, set the Q-value target to the same as originally returned from step 1, making the error 0 for those outputs.
    + Update the weights using backpropagation.

- A bunch of tricks are used in practice. The most important trick is experience replay. During gameplay all the experiences < s, a, r, s’ > are stored in a replay memory. When training the network, random minibatches from the replay memory are used instead of the most recent transition. This breaks the similarity of subsequent training samples, which otherwise might drive the network into a local minimum. Also experience replay makes the training task more similar to usual supervised learning, which simplifies debugging and testing the algorithm. One could actually collect all those experiences from human gameplay and then train network on these.


- when a Q-table or Q-network is initialized randomly, then its predictions are initially random as well. If we pick an action with the highest Q-value, the action will be random and the agent performs crude “exploration”. As a Q-function converges, it returns more consistent Q-values and the amount of exploration decreases. So one could say, that Q-learning incorporates the exploration as part of the algorithm. But this exploration is “greedy”, it settles with the first effective strategy it finds. A simple and effective fix for the above problem is ε-greedy exploration – with probability ε choose a random action, otherwise go with the “greedy” action with the highest Q-value.

### We can use RL when ALL following conditions are met:
    1. [States] When data comes in the form of **trajectories** (there is dependency in data so iid assumption doesn't hold)
    2. [Actions] Need to make a sequence of **decisions** (that change the trajectory)
    3. [Observations] We can observe **feedbacks** of actions (the feedback can be sparse)

