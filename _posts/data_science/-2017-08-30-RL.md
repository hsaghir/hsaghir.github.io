---
layout: article
title: The unreasonable elegance of deep generative models
comments: true
categories: data_science
image:
  teaser: jupyter-main-logo.svg
---

## Background
Reinforcement learning is the story of an agent that knows nothing and wants to learn by interacting with its environment. The environment is usually a black box from agent's point of view that can only provide the agent with an observation for every action the agent takes.

At the start of interaction, the agent might not know what it wants to do since it doesn't know much about its environment. However, if we go the Freud's route and say that an agent's goal in life should be to maximize its pleasure, then the agent will have an objective (to maximize its pleasure in life whatever that is). 5

The agent probably doesn't know itself either as it starts interacting with the environment and starts enjoying certain things as it explores. So in some sense it is developing a models of its own hidden desires (reward function) based on what provides it with pleasure. Therefore, it learns an objective function (accumulated reward over time) that it needs to maximize to get the most pleasure in life.

Now that the agent knows what it wants, it starts thinking about what series of actions (policy) it should take to maximize what it wants. In doing so, it will need to develop a model of its environment in its head to know what happens if it takes an action (model environment) and whether that action and its consequences are good or bad (value function) from maximum total reward perspective. 

[Joell Pineau gives a very nice overview of RL](http://videolectures.net/deeplearning2017_pineau_reinforcement_learning/)


## formulating the problem
To be able to create agents and let them learn, we need to formulate the problem. Let's use the amazing tools of probability theory to formulate the agent's life story. 

Suppose you are an agent, situated in an environment. The environment is in a certain state. The agent can perform certain actions in the environment. These actions sometimes result in a reward. Actions transform the environment and lead to a new state.

### environment:
- The environment consists of a set of possible states $$S = {s_0, s_1, ... s_n}$$.
- States transition to one another as time goes by based on the past visited states, and the actions received by environment $$p(s_{i+1}|s_0, s_1, .., s_i, a_0, a_1, .. , a_i)$$. 




-  Policy: a sequence of actions to go from an initial state to a target state (conditional probability of actions given past states).
-  Model: Agent's model of the environment (conditional probability of next state given past states)
-  Reward: objective function assigning signal of success/failure.
-  Return: weighted sum of future rewards (determining how important are future rewards vs. now).
-  Value Function: how good each state/action is (averaged future reward accumulated from each particular state/action)
-  credit assignment problem: which of the preceding actions was responsible for getting the reward and to what extent.
-  explore-exploit dilemma: Once you have figured out a strategy to collect a certain number of rewards, should you stick with it or experiment with something that could result in even bigger rewards?



## modeling

We can think of RL as a structured semi-supervised problem whereas in supervised learning one has a label for each training example and in unsupervised learning one has no labels at all, in RL one has sparse and time-delayed labels – the rewards. 


As with all other real problems we make simplifying assumptions to be able to model this phenomenon.

An agent at state s, takes action a, and the environment produces a reward r, and an observation that shifts the state of the agent to s+1. RL methods are employed to address two related problems:
    + prediction: learn value function for a certain policy to predict how far it is from its objective at every state.
        * dynamic programming:
        * Monte Carlo method:
            - calculate the gradient using reinforce gradient
        * evolution:
            - this can estimate reinforce gradient
        * temporal difference (TD) learning: updating a guess on the basis of another guess.
    + control: learn policy function.

If we know state transition function (agent model) and the reward function (environment model), algos are called model-based algos. It that case, dynamic programming based algorithms i.e. 1)value iteration and 2) policy iteration can be used to solve for the value function and the optimal policy respectively. If we don't know models (transition and reward functions), then the optimal value function and/or the optimal policy will have to be learned. For example:
    + Actor-critic methods are adaptive policy iteration algos which approximate value function using TD.
    + Q-learning: a unifying algorithm which allows for simultaneous value function and policy optimization.








There are two scenarios in RL, off-policy vs on-policy:
    + on-policy: the agent follows the policy that it is trying to optimize
    + off-policy: the agent isn't following the policy it is trying to optimize. For example, Q-learning. Therefore we can have an importance sampling ratio which is basically policy ratio.





- The other way to view eligibility traces is more mechanistic. From this perspective, an eligibility trace is a temporary record of the occurrence of an event, such as the visiting of a state or the taking of an action. The trace marks the memory parameters associated with the event as eligible for undergoing learning changes. When a TD error occurs, only the eligible states or actions are assigned credit or blame for the error. Thus, eligibility traces help bridge the gap between events and training information. Like TD methods themselves, eligibility traces are a basic mechanism for temporal credit assignment.



## Formal problem setup:
The set of states and actions, together with rules for transitioning from one state to another, make up a Markov decision process (MDP). MDPs have no memory meaning that the decision at each state depends only on the current state and not on past states. An agent starts at a certain state s_0, takes action a_0, collects reward r_0 and visits new state s_1 and repeat. The goal of the agent is to maximize the expected cumulative reward.
    + A set of states [s1,s2,…,sM][]
    + A set of actions [a1,a2,…,aN][]
    + A set of rewards [r1,r2,…,rL][]
    + A set of transition probabilities [s11,s12,…,s1M,s21,s22,…,s2M,…,sMM][]

- The environment provides: 
    + A reward function R(s,a) which gives us the reward that comes from taking an action a_t in state s_t at time t. 
    + A transition function S(s,a) which will give us the next state . The actions a_t are generated by the agent by following one or several policies. 

- Find a policy function (i.e. density function of joint P(s,a)) that generates a series of actions to get maximum reward in the future.


- To perform well in the long-term, we need to take into account not only the immediate rewards, but also the future rewards we are going to get. because our environment is stochastic, we can never be sure, if we will get the same rewards the next time we perform the same actions. The more into the future we go, the more it may diverge. For that reason it is common to use discounted future reward (from point t onward). 

$$Return_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + ..$$

- If we set the discount factor γ=0, then our strategy will be short-sighted and we rely only on the immediate rewards. If we want to balance between immediate and future rewards, we should set discount factor to something like γ=0.9. If our environment is deterministic and the same actions always result in same rewards, then we can set discount factor γ=1.


## Q-Learning

- Q-learning attempts to solve the credit assignment problem – it propagates rewards back in time, until it reaches the crucial decision point which was the actual cause for the obtained reward.

- In Q-learning we define a function Q(s, a) as the value function representing the maximum discounted future reward when we perform action a in state s, and continue optimally from that point on. it is “the best possible score at the end of the game after performing action a in state s“. It is called Q-function, because it represents the “quality” of a certain action in a given state. At each time point You want to select the action that results in the highest score at the end of game. Once you have the magical Q-function, the answer (policy) becomes really simple – pick the action with the highest Q-value!


- Maximum future reward for this state and action is the immediate reward plus maximum future reward for the next state. Therefore, we can express the Q-value of state s and action a in terms of the Q-value of the next state s' as $$Q(s,a) = r + \gamma max_{a'} Q(s',a')$$. This is called the Bellman equation. The main idea in Q-learning is that we can iteratively approximate the Q-function using the above Bellman equation by randomly choosing actions and exploring different states. 

- The max_{a'} Q(s',a') that we use to update Q(s,a) is only an approximation and in early stages of learning it may be completely wrong. However the approximation gets more and more accurate with every iteration and it has been shown, that if we perform this update enough times, then the Q-function will converge and represent the true Q-value.

- Deep Q-learning: We could represent our Q-function with a neural network, that takes the state (four game screens) and action as input and outputs the corresponding Q-value. Alternatively we could take only game screens as input and output the Q-value for each possible action. Q-values can be any real values, which makes it a regression task, that can be optimized with simple squared error loss.
    + Do a feedforward pass for the current state s to get predicted Q-values for all actions.
    + Do a feedforward pass for the next state s’ and calculate maximum overall network outputs max a’ Q(s’, a’).
    + Set Q-value target for action to r + γmax a’ Q(s’, a’) (use the max calculated in step 2). For all other actions, set the Q-value target to the same as originally returned from step 1, making the error 0 for those outputs.
    + Update the weights using backpropagation.

- A bunch of tricks are used in practice. The most important trick is experience replay. During gameplay all the experiences < s, a, r, s’ > are stored in a replay memory. When training the network, random minibatches from the replay memory are used instead of the most recent transition. This breaks the similarity of subsequent training samples, which otherwise might drive the network into a local minimum. Also experience replay makes the training task more similar to usual supervised learning, which simplifies debugging and testing the algorithm. One could actually collect all those experiences from human gameplay and then train network on these.


- when a Q-table or Q-network is initialized randomly, then its predictions are initially random as well. If we pick an action with the highest Q-value, the action will be random and the agent performs crude “exploration”. As a Q-function converges, it returns more consistent Q-values and the amount of exploration decreases. So one could say, that Q-learning incorporates the exploration as part of the algorithm. But this exploration is “greedy”, it settles with the first effective strategy it finds. A simple and effective fix for the above problem is ε-greedy exploration – with probability ε choose a random action, otherwise go with the “greedy” action with the highest Q-value.

### We can use RL when ALL following conditions are met:
    1. [States] When data comes in the form of **trajectories** (there is dependency in data so iid assumption doesn't hold)
    2. [Actions] Need to make a sequence of **decisions** (that change the trajectory)
    3. [Observations] We can observe **feedbacks** of actions (the feedback can be sparse)

