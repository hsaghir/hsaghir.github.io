---
layout: article
title: The unreasonable elegance of deep generative models
comments: true
categories: data_science
image:
  teaser: jupyter-main-logo.svg
---

## Background
Reinforcement learning is learning by interacting with an environment. The environment provides a numerical reward to the agent. Agent seeks to learn a policy for selecting actions that maximize the accumulated reward over time. A policy is a sequence of actions that the agent can take to go from an initial state to a target state.

An agent at state s, takes action a, and the environment produces a reward r, and shifts the state of the agent to s+1. Return is the discounted(determining how important are future rewards vs. now) sum of future rewards. RL methods are employed to address two related problems:
    + prediction: RL is used to learn the value function for a certain policy. Therefore in some sense the agent learns to predict how far it is from its objective at every state.
        * dynamic programming:
        * monte carlo medthod:
            - calculate the gradient using reinforce gradient
        * evolution:
            - this can estimate reinforce gradient
        * temportal difference (TD) learning: updating a guess on the basis of another guess.
    + control: RL used to learn policy function.

If we know state transition function (agent model) and the reward function (environment model), algos are called model-based algos. It that case, dynamic programming based algorithms i.e. 1)value iteration and 2) policy iteration can be used to solve for the value function and the optimal policy respectively. If we don't know models (transition and reward functions), then the optimal value function and/or the optimal policy will have to be learned. For example:
    + Actor-critic methods are adaptive policy iteration algos which approximate value function usign TD.
    + Q-learning: a unifying algorithm which allows for simultaneous value function and policy optimization.








There are two scenarios in RL, off-policy vs on-policy:
    + on-policy: the agent follows the policy that it is trying to optimize
    + off-policy: the agent isn't following the policy it is trying to optimize. For example, Q-learning. Therefore we can have an importance sampling ratio which is basically policy ratio.





- The other way to view eligibility traces is more mechanistic. From this perspective, an eligibility trace is a temporary record of the occurrence of an event, such as the visiting of a state or the taking of an action. The trace marks the memory parameters associated with the event as eligible for undergoing learning changes. When a TD error occurs, only the eligible states or actions are assigned credit or blame for the error. Thus, eligibility traces help bridge the gap between events and training information. Like TD methods themselves, eligibility traces are a basic mechanism for temporal credit assignment.



## Formal problem setup:
The best studied case is when RL can be formulated as class of Markov Decision Problems (MDP). An agent starts at a certain state, takes actions, visits states and collects rewards. Each state has a value which is determined by the averaged future reward accumulated from that particular state. The goal of the agent is to maximize the expected cumulative reward.


    + A set of states [s1,s2,…,sM][]
    + A set of actions [a1,a2,…,aN][]
    + A set of rewards [r1,r2,…,rL][]
    + A set of transition probabilities [s11,s12,…,s1M,s21,s22,…,s2M,…,sMM][]

- The environment provides: 
    + A reward function R(s,a) which gives us the reward that comes from taking an action a_t in state s_t at time t. 
    + A transition function S(s,a) which will give us the next state . The actions a_t are generated by the agent by following one or several policies. 

- Find a policy function (i.e. density function of joint P(s,a)) that generates a series of actions to get maximum reward in the future.






