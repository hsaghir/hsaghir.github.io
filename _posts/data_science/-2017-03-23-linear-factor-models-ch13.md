---
layout: article
title: Linear factor models Ch13
comments: true
categories: data_science
image:
  teaser: jupyter-main-logo.svg
---

- Two challenges in unsupervised learning (probabilistic modelling) due to curse of dimensionality, first statistical challenge, we need exponentially more data to find probability distribution functions, second computational challenge, we need exponentially more computations to calculate intractable partition functions. 

- Linear factor models might be used as the building blocks of mixture or deep probabilistic models. A linear factor model describes the data generation process as, First, sampling from the explanatory factors h from a distribution p(h) which is factorized as product of p(h_i). Then x is obtained by a linear combination of independent latent factors h, plus some noise $x = Wh + b + noise$. Different models make different choices about the form of the noise and of the prior p(h), such as probabilistic PCA, factor analysis or ICA,.

- In factor analysis, the latent variable is just unit variance Gaussian, and observed variable noise comes from a diagonal covariance Gaussian with an independent variance for each observed variable, meaning that observed variables are assumed to be iid. Therefore, the observed variables are simply multivariate Gaussian variables with $b$ as mean and $w*w^T+observed noise$. The latent variables capture the dependencies in the observed variables inside the covariance matrix. 

- If the observed noise in factor analysis is assumed to be equal to each other, $\sigma*I$, we get probabilistic PCA where the observed variables are mulitvariate Gaussians with mean $b$, and covariance matrix of $w*w^T+\sigma*I$. This probabilistic PCA model takes advantage of the observation that most variations in the data can be captured by the latent variables h, up to some small residual reconstruction error $\sigma$. Probabilistic PCA becomes PCA when $\sigma$ goes to zero. The density model defined by probabilistic PCA becomes very sharp around the d dimensions spanned by the columns of the eigenvectors of the observations matrix. 

- ICA is an approach to modeling linear factors that seeks to separate an observed signal into many underlying signals that are scaled and added together to form the observed data. These signals are intended to be fully independent, rather than merely decorrelated from each other. The motivation for this approach is that by choosing p(h) to be independent, we can recover underlying factors that are as close as possible to independent. This is commonly used, not to capture high-level abstract causal factors, but to recover low-level signals that have been mixed together. In this setting, each training example is one moment in time, each xi is one sensor’s observation of the mixed signals, and each hi is one estimate of one of the original signals. For example, we might have n people speaking simultaneously. If we have n different microphones placed in different locations, ICA can detect the changes in the volume between each speaker as heard by each microphone, and separate the signals so that each h i contains only one person speaking clearly. 

- The prior distribution over the underlying factors, p(h ), must be fixed ahead of time by the user. The model then deterministically generates x = W h. Learning the model then proceeds using maximum likelihood. All variants of ICA require that p(h) be non-Gaussian. This is because if p(h) is an independent prior with Gaussian components, then W is not identifiable. We can obtain the same distribution over p(x) for many values of W. Typical choices of these non-Gaussian distributions have larger peaks near 0 than does the Gaussian distribution, so we can also see most implementations of ICA as learning sparse features.

- Many other variants of ICA are possible. Some add some noise in the generation of x rather than using a deterministic decoder. Most do not use the maximum likelihood criterion, but instead aim to make the elements of $h = W^−1*x$ independent from each other. Just as PCA can be generalized to the nonlinear autoencoders, ICA can be generalized to a nonlinear generative model, in which we use a nonlinear function f to generate the observed data.

- Another nonlinear extension of ICA is the approach of nonlinear independent components estimation, or NICE (Dinh et al., 2014), which stacks a series of invertible transformations (encoder stages) that have the property that the determinant of the Jacobian of each transformation can be computed efficiently. This makes it possible to compute the likelihood exactly and, like ICA, attempts to transform the data into a space where it has a factorized marginal distribution, but is more likely to succeed thanks to the nonlinear encoder. Because the encoder is associated with a decoder that is its perfect inverse, it is straightforward to generate samples from the model (by first sampling from p(h) and then applying the decoder).

- Slow feature analysis (SFA) is a linear factor model that uses information from time signals to learn invariant features (Wiskott and Sejnowski, 2002).Slow feature analysis is motivated by a general principle called the slowness principle. The idea is that the important characteristics of scenes change very slowly compared to the individual measurements that make up a description of a scene. For example, in computer vision, individual pixel values can change very rapidly. If a zebra moves from left to right across the image, an individual pixel will rapidly change from black to white and back again as the zebra’s stripes pass over the pixel. By comparison, the feature indicating whether a zebra is in the image will not change at all, and the feature describing the zebra’s position will change slowly. We therefore may wish to regularize our model to learn features that change slowly over time.

- The slowness principle can be used as a regularization technique by adding a term to the cost function that sums up a loss (e.g. MSE) as a function of the distance between consequent time steps (e.g. f(t+1)-f(t)) of a feature extractor, f, we are regularizing. SFA uses linear transformation $w*x$ as the feature extractor and MSE as the loss function, applies the slowness regularization and solves the optimization problem while constraining the features to have zero mean and unit variance. To learn multiple features we also add the constraint that the features be orthogonal which specifies that the learned features must be linearly decorrelated from each other. Like PCA, the SFA features are ordered, with the first feature being the slowest. 

- SFA is typically used to learn nonlinear features by applying a nonlinear basis expansion to x before running SFA. For example, it is common to replace x by the quadratic basis expansion. Linear SFA modules may then be composed to learn deep nonlinear slow feature extractors by repeatedly learning a linear SFA feature extractor, applying a nonlinear basis expansion to its output, and then learning another linear SFA feature extractor on top of that expansion.

- When trained on small spatial patches of videos of natural scenes, SFA with quadratic basis expansions learns features that share many characteristics with those of complex cells in V1 cortex (Berkes and Wiskott, 2005). When trained on videos of random motion within 3-D computer rendered environments, deep SFA learns features that share many characteristics with the features represented by neurons in rat brains that are used for navigation (Franzius et al., 2007). SFA thus seems to be a reasonably biologically plausible model.

- So far, the slowness principle has not become the basis for any state of the art applications. It is unclear what factor has limited its performance. We speculate that perhaps the slowness prior is too strong, and that, rather than imposing a prior that features should be approximately constant, it would be better to impose a prior that features should be easy to predict from one time step to the next. The position of an object is a useful feature regardless of whether the object’s velocity is high or low, but the slowness principle encourages the model to ignore the position of objects that have high velocity.

- Sparse coding (Olshausen and Field, 1996) is a linear factor model that has been heavily studied as an unsupervised feature learning and feature extraction mechanism. Like most other linear factor models, it uses a linear decoder plus noise to obtain reconstructions of x. Sparse coding models are similar to probabilistic PCA in that they typically assume observed noise to be Gaussian with variances equal to each other. However, the distribution p(h) is chosen to be one with sharp peaks near 0 including factorized Laplace, Cauchy or factorized Student-t distributions. Training sparse coding with maximum likelihood is intractable. Instead, the training alternates between encoding the data and training the decoder to better reconstruct the data given the encoding. 

- The encoder that we use with sparse coding is not a parametric encoder. Instead, the encoder is an optimization algorithm, $max p(h/x)$,  that solves an optimization problem in which we seek the single most likely code value. The non-parametric encoder can in principle minimize the combination of reconstruction error and log-prior better than any specific parametric encoder and there is no generalization error to the encoder. The primary disadvantage of the non-parametric encoder is that it requires greater time to compute h given x because the non-parametric approach requires running an iterative algorithm. The parametric autoencoder approach, uses only a fixed number of layers, often only one. Another disadvantage is that it is not straight-forward to back-propagate through the non-parametric encoder.

- Linear factor models, often produces poor samples, even when the model is able to reconstruct the data well and provide useful features. The reason is that each individual feature may be learned well, but the factorial prior on the hidden code results in the model including random subsets of all of the features in each generated sample. That's why things like VAE are useful since they constrain the hidden to a subspace for beter generative performance. 

- Linear factor models and linear autoencoders including PCA and factor analysis can be interpreted as learning a manifold. Linear factor models are some of the simplest generative models and some of the simplest models that learn a representation of data and may be extended to autoencoder networks and deep probabilistic models. 
