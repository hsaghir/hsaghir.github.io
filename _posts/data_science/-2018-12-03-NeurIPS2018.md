---
layout: article
title: Neurips 2018
comments: true
categories: finance
image:
  teaser: jupyter-main-logo.svg
---



# Deep Unsupervised Learning Tutorial (Mon)

- Types of learning: 
 	- active: 
		+ with teacher: RL/Active learning
		+ without teacher: Instrinsic motivation
	- passive:
		+ with teacher: supervised learning
		+ w/o teacher: unsupervised learning

- why Unsupervised?	
	- labels / rewards difficult to obtain
	-  Unsupervised more human like 
	- we want rapid generalization to new tasks and situations


- Transfer learning: 
	- kind of works but never seems to transfer (generalize) as far or as fast as we want (one hypothesis: there isn't enough info there. Humans don't learn tasks that transfer, they learn skills)

- Basic challenge of unsupervised learning is that the task is undefined

- simplest this is to do max likelihood on data (density estimation) instead of targets (supervised learning).
	- max likelihood says basically learn everything about the data

- no consensus on what to do for unsupervised learning. maybe trying to learn everything about data not good, shouldn't we focus on learning things that will be useful in future?
	- problem: density estimation is hard
	- problem: log-likelihood depend more on how low-level details (pixels) than on highlevel structure (image content, semantics)
	- problem: even if we learn structure, it's difficult to access them. need to learn representations. 

- modeling densities also give us generative models of the data. 
	- allow us to understand what the model hasn't learned. 


- models (Autoregressive models):
	- basic trick: split high dim data up into a seq of small pieces, predict each from those before. 
	- conditioning done using (lstm, masked convs, transformers), output layer parameterizes predictions. 
	- they are simple, easy to generate samples, best log-likelihoods for many types of data. but, they are very expensive for high dim data, order-dependant, teacher forcing (only learning to predict one step ahead, potentially brittle representation)

---

- tip1 : always look at the data before modeling
- tip2 : PCA and K-means are very often very strong baselines
- learning representations (self-supervised learning):
	- taking different parts of an image and classifying them (Dorsch 2015) as way to learn representations and use as initialization.
	- predict whether a video is playing forward or backward (D. Wei 2018 CVPR) and use as initialization. 
	- learning by clustering

---
# NLP session (Tuesday)

- on the dimensionality of word embeddings
	- prove mathematically that there is an optimal dimensionality
	- uses a pairwise 

- 





