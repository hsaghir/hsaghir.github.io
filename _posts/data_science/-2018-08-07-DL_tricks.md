---
layout: article
title: Tricks and trades of making a neural network work.
comments: true
categories: data_science
image:
  teaser: jupyter-main-logo.svg
---

## Debugging ML code:
- start with a small model and small data and evolve both together. If you can't overfit a small amount of data you've got a simple bug somewhere. 
    + Start with all zero data first to see what loss you get with the base output distribution, then gradually include more inputs (e.g. try to overfit a single batch) and scale up the net, making sure you beat the previous thing each time.
    + also if zero inputs produces a nice/decaying loss curve, this usually indicates not very clever initialization.
    + initialize parameters with truncated normal or xavier.
    + also try tweak the final layer biases to be close to base distribution
    + for classification, check if the loss started at ln(n_classes)

### sanity checks
- remember to toggle train/eval mode for the net. 
- remember to .zero_grad() (in pytorch) before .backward(). 
- remember not to pass softmaxed outputs to a loss that expects raw logits.
- pytorch `.view()` function reads from the last dimension first and fills the last dimension first too
- when comparing tensors, the results are `ByteTensor`s. ByteTensors have a buffer of `255` after which it is zeroed out. Although this issue seems to be fixed in newer pytorch versions, beware that a `sum()` on ByteTensors is likely to result in wrong answer. First convert them to `float()` or `long()` and then `sum()`


### making the loss go down
If your network isn’t learning (meaning: the loss/accuracy is not converging during training, or you’re not getting results you expect), try these tips:

- Overfit! The first thing to do if your network isn’t learning is to overfit a training point. Accuracy should be essentially 100% or 99.99%, or an error as close to 0. If your neural network can’t overfit a single data point, something is seriously wrong with the architecture, but it may be subtle. If you can overfit one data point but training on a larger set still does not converge, try the following suggestions.
- Lower your learning rate. Your network will learn slower, but it may find its way into a minimum that it couldn’t get into before because its step size was too big. (Intuitively, think of stepping over a ditch on the side of the road, when you actually want to get into the lowest part of the ditch, where your error is the lowest.)
- Raise your learning rate. This will speed up training which helps tighten the feedback loop, meaning you’ll have an inkling sooner whether your network is working. While the network should converge sooner, its results probably won’t be great, and the “convergence” might actually jump around a lot. (With ADAM, we found ~0.001 to be pretty good in many experiences.)
- Decrease (mini-)batch size. Reducing a batch size to 1 can give you more granular feedback related to the weight updates, which you should report with TensorBoard (or some other debugging/visualization tool).
- Remove batch normalization. Along with decreasing batch size to 1, doing this can expose diminishing or exploding gradients. For weeks we had a network that wasn’t converging, and only when we removed batch normalization did we realize that the outputs were all NaN by the second iteration. Batch norm was putting a band-aid on something that needed a tourniquet. It has its place, but only after you know your network is bug-free.
- Increase (mini-)batch size. A larger batch size—heck, the whole training set if you could—reduces variance in gradient updates, making each iteration more accurate. In other words, weight updates will be in the right direction. But! There’s an effective upper bound on its usefulness, as well as physical memory limits. Typically, we find this less useful than the previous two suggestions to reduce batch size to 1 and remove batch norm.
- Check your reshaping. Drastic reshaping (like changing an image’s X,Y dimensions) can destroy spatial locality, making it harder for a network to learn since it must also learn the reshape. (Natural features become fragmented. The fact that natural features appear spatially local is why conv nets are so effective!) Be especially careful if reshaping with multiple images/channels; use numpy.stack() for proper alignment.
- Scrutinize your loss function. If using a complex function, try simplifying it to something like L1 or L2. We’ve found L1 to be less sensitive to outliers, making less drastic adjustments when hitting a noisy batch or training point.
- Scrutinize your visualizations, if applicable. Is your viz library (matplotlib, OpenCV, etc.) adjusting the scale of the values, or clipping them? Consider using a perceptually-uniform color scheme as well.


## Optimize the optimization process of your neural net

1- earlier layers of a deep NN usually converge much faster than later layers meaning that later layers need much more changing before they settle on a converged representation. Therefore, using multiple learning rates (larger for later layers) should help with the total convergence time. 

2- find optimal learning rate by doing a trial epoch. use a low learning rate and increase it exponentially with each batch and record loss for every learning rate. then plot learning rate vs. loss. Optimum learning rate is the highest value of the learning rate where the loss is still decreasing and hasn't plateaued. 

3- As training progresses, model gets closer to the minimum and therefore the learning rate should be decreased to prevent overshooting (oscillatory behavior). Cosine annealing achieves this by having the learning rate decrease with a cosine function usually between [0.1, 1].  $$(LR * cosine(batch_number/(total_batch * num_epochs)) $$.

4-  SGD might get stuck in a local minimum during training, increasing learning rate suddenly might help it hop out of local minima. This is usually done by restarting the annealing cosine scheduling of learning rate. This forms a cycle. We then make this cycle longer as the training evolves by for example starting with restarting every 1 epoch, then every 2 epochs, then every 3, and so on. 

5- know that activation functions have their own characteristics that should be considered when making design decisions. Softmax likes to pick just one thing. Sigmoid wants to know where you are between -1 and 1, and beyond these values won’t care how much you increase. Relu is a club bouncer who won’t let negative numbers through the door.

6- On structured data, you can simply embed categorical variables with embedding vectors like word vectors and use NNs instead of dummy or binary variables and RF-based models.

7- to overcome overfitting:
    - use dropout,
    - train on smaller image sizes, then increasing the size of input and train again. 

8- ensemble your model prediction:
    - test time augmentation (TTA): feed different versions of the test input (e.g. crops or zooms) and pass through the model, use average output as the final output score.



## Use Normalization to ease optimization

Normalization does not reduce the expressive power of the network but normalizes the statistics of the network according to the dataset statistics in order to make the optimization of the network easier. 
    - Batch Norm : computes the mean and variance of each mini-batch and normalizes each feature. the mean and variance will differ for each mini-batch. This dependency causes two main problems:
        + Ideally, we want to use the global mean and variance to normalize the inputs to a layer so may be problematic with small batch_sizes.
        + makes batch normalization difficult to apply to recurrent connections. 
        + an alternative is mean-only batch normalization which only normalizes the mean and skips the variance scaling.
    - Layer Norm: a mini-batch consists of multiple examples with the same number of features. 
        + Batch normalization normalizes the input features across the batch dimension. The key feature of layer normalization is that it normalizes the batch across the features.
        + In batch normalization, the statistics are computed across the batch and are the same for each example in the batch. In contrast, in layer normalization, the statistics are computed across each feature and are independent of other examples.
    - Weight Norm: instead of normalizing the mini-batch, normalizes the weights of the layer. 
        + separates the norm of the weight vector from its direction by rewriting the weights as $$ {w} = \frac{g}{v} v$$ and optimizes both g  and v using SGD.
        + mean and variance are independent of the batch, 
        + weight normalization is often much faster than batch normalization.
        + more computationally efficient in CNNs since number of weights are much smaller than inputs. 
    - Spectral Norm: Normalizes each layer using the eigen values of its weight matrix.

## design your experimental and reproducability pipeline:

- use [Sacred](http://sacred.readthedocs.io/en/latest/quickstart.html) to record and reproduce your experiments:
    + install Sacred with `pip install sacred`
    + instantiate the Experiment() class from sacred 
    ```py
     ex = Experiment()
    ```
    + put all your argument inside a config function and decorate it with `@ex.config`
    ```py
    @ex.config
    def my_config():
        args = parse_args()
    ```
    + then define your main function and pass the config you defined to it and decorate it with `ex.automain`
    ```py
    @ex.automain
    def main(args):
        ...
    ```
    + Sacred has a command line interface that lets you interact with and modify your experiment. 


- Make a spreadsheet in style of a tree. Each node in the tree will be a grouping of experiments. Each group will only have a child if there is a question that needs to be answered and the experiment will only answer that question. Each leaf node will then be the result of the experiment with the value of the desired metric and a one-sentence answer to the question of the experiment. 

1- define a metric that you are trying to beat or optimize for (e.g. accuracy)

2- hyperparameters used for the experiment

3- Sub-Project: group of ideas you are exploring

Context: The context may be the specific objective such as beating a baseline, tuning, a diagnostic, and so on.
Setup: The setup is the fixed configuration of the experiment.
Name: The name is the unique identifier, perhaps the filename of the script.
Parameter: The parameter is the thing being varied or looked at in the experiment.
Values: The value is the value or values of the parameter that are being explored in the experiment.
Status: The status is the status of the experiment, such as planned, running, or done.
Skill: The skill is the North Star metric that really matters on the project, like accuracy or error.
Question: The question is the motivating question the experiment seeks to address.
Finding: The finding is the one line summary of the outcome of the experiment, the answer to the question.