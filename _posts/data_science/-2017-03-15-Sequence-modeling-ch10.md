---
layout: article
title: ConvNets Deep Learning book Ch9
comments: true
categories: data_science
image:
  teaser: jupyter-main-logo.svg
---

- Parameter sharing in RNNs and ConvNets is key to have input/outputs of arbitrary size since learned patterns are repeatedly applied to different inputs. ConvNet shares params in a single layer while RNN shares params across layers of a deep net (unfolded in time). 

- There are three basic transformations in an RNN (1) from the input to the hidden state, (2) from the previous hidden state to the next hidden state, and (3) from the hidden state to the output. In a simple RNN, when the network is unfolded, each of these corresponds to a shallow transformation. Thinking about RNNs this way, it's possible to make each of these transformations arbitrary complex (e.g. a deep net instead of shallow or even a different operation!). For example, the recurrence transformation can be a deep Autoencoder that is simply feeding its output back into input (long path might make optimization difficult) or several layers of recurrence stacked on top of each other (short path). 

- the unfolded recurrent structure allows us to factorize the function g(x1,x2,..,xt) of the whole past sequence into repeated application of a function f. Since hidden units are a function of earlier
time steps, the BPTT algorithm is necessary which is the same as BP for the completely unfolded network through time with gradients summed up through shared parameters across time steps (e.g. hidden weights).

- The hidden-to-hidden recurrence is powerful since it contains information from past inputs in the sequence but the training procedure is costly and cannot be parallelized due to the time dependance. We can make a recurrence from output-to-hidden $p(y1,y2/x1,x2)$. The advantage of eliminating hidden-to-hidden recurrence is that, for any loss function based on comparing the prediction at time t to the training target at time t, all the time steps are decoupled.

- Output-to-hidden recurrence can be trained with teacher forcing. At train time, we feed the correct output y(t) drawn from the train set as input to h(t+1). At test time, we approximate the correct output y(t) with the modelâ€™s output o(t), and feed the output back into the model. Teacher forcing may still be applied to models that have hidden-to-hidden connections as long as there is a output-to-hidden connection.

- RNN can be interpreted as a probabilistic graphical model in two ways. First by marginalizing out the hidden units and seeing the RNN as an autoregressive process where each point in time is a random variable dependent on points in the past. Second way of graphical model intrepretation is incorporating hidden nodes in the graphical model as deterministic nodes. This graphical representation will decouple past and future given the hidden units. Additionally in this view, the hidden recurrent function acts as an amortized way of representing the probability table for combining the hidden state and the observed value. Imagine if the observed takes k values in a series of length l, in the autoregressive case we would need k to power l possibilities in a table to describe all possible cases but the second view uses a single recurrent function (i.e. the hidden unit, Wh*h) to amortize this which doesn't depent on length of the series. 

- Amortization through parameter sharing in an RNN has a cost which is stationary assumption of conditionl distribution over variables meaning that it assumes the conditional distribution over parameters doesn't change through time. However, In principle, it would be possible to use t as an extra input at each time step and let the learner discover any time-dependence while sharing as much as it can between different time steps. Another way to attack this is change point detection and using seperate RNNs for each change point. 

- Sampling from the RNN is easy give the graphical model interpretation. However, we need to also know when to stop generating the sequence. This can be done using an additional symbol indicating sequence end at the end of each example data and then when this sequence is sampled we stop. Another way is to just input a time index of how close to the sequence end the RNN is at each time step plus an additional output that directly estimates a length number .Another more general way is to use an additional bernoulli output that estimates the probability of continuing or stoping at each time step. It can be a sigmoid unit trained with cross-entropy loss. 

- RNN might take in multiple inputs or model different conditional probability distributions. For example in the case of image captioning, an RNN models the conditional probability of the caption given an image. It might take in a single image for all time steps while taking in a sequence of words. This way, the image input is effectively similar to a bias for all time steps since it doesn't change with the sequence. An RNN might also model the conditional distribution of a sequence given another sequence for example machine translation. It takes in two sequences at the same time as it unrolls in time while the next time point output functions as the target value for the loss function. 

- In some cases, the prediction at time steps might depend on the whole sequence before and after that time step for example, word detection from phonemes. This happens when the units of time in the sequence is smaller thatn the minimum semantic unit of time in reality, for example speach needs a full sentence to make sence of the meaning and smaller units have lots od uncertainty. I think this also speaks to different time scales involved in the dynamics of a sequence. For example in language, an ambiguity might need a whole paragraph to be resolved while a word might need only a few letters to be resolved (thus the need to uncertainty in language models). In such cases we need a bi-directional RNN that models 1-to-t and t-to-1 relationships using two parallel RNN models with seperate hidden states that contribute to a single output; one starting from beginning and one starting at the end. 

- When the lengths of input and output sequences differ, RNN encoder-decoder (seq-2-seq) architechtures can be used. Seq-2-seq is very simple, (1) an encoder RNN processes the input sequence and produces an output representation as a function of its hidden state (context). (2) a decoder RNN is conditioned on that fixed-length vector to generate the output sequence. (3) both encoder and decoder are jointly trained to maximize coditional probability P(y/x). fixed-size encoder representation might be a limitation. 

- Recursive nets use a tree structure of recursively applying a fixed set (e.g. V, U, W) of parameters to a sequence. For example, it might apply V to each time point in the input sequence to make the leaves of the tree, then apply U and W recursively to form a binary tree up to the root. The advantage of recursive nets over RNN is that their depth for a fixed input lenth, T, is O(log(T)) while an RNN is O(T). This reduction in depth (number of compositions of nonlinear operations) can make optimization easier and help with long term dependancies. An open question is how to best structure the tree. 

- When composing many nonlinear functions (like the linear-tanh layer), the result is highly nonlinear, typically with most of the values associated with a tiny derivative, some values with a large derivative, and many alternations between increasing and decreasing. In an RNN, this is equivalent to a multiplying the recurrence matrix by itself T number of times which is equivalent to doing the same with its eigenvalues. Eigenvalues with magnitude less than one to decay to zero and eigenvalues with magnitude greater than one to explode. Very deep feedforward networks with carefully chosen scaling can avoid the vanishing and exploding gradient problem. 

- Recurrent networks involve the composition of the same function multiple times. Therefore, there are challenges in modelling long term dependencies in RNNs due to 1- vanishing/exploding gradienst caused by propagating the gradient through a lot of stages when RNN is unrolled in time, and 2- Even if gradients didn't vanish, the weights assigned to long range dependancies would be exponentially smaller than short term ones due to multiplications of many Jacobians. There are strategies for learning long term dependencies upto 100s of time steps but the problem in general is still open. 

- Echo State Network (ESN) or liquid state machines set the recurrent weights such that the recurrent hidden units do a good job of capturing the history of past inputs, and only learn the output weights (reservoir computing). Theses are inspired by the observation that in an RNN learning, earlier layers don't change much due to vanishing and small gradients and most change happens at the output layer anyway. One way to think about these reservoir computing recurrent networks is that they are similar to kernel machines: they map an arbitrary length sequence into a fixed-length vector (the recurrent state h(t)), on which a linear predictor can be applied to solve the problem of interest. how do we set the input and recurren weights so that a rich set of histories can be represented in the recurrent neural network state? The answer proposed in the reservoir computing literature is to view the recurrent net as a dynamical system, and set the input and recurrent weights such that the dynamical system is near the edge of stability.



