---
layout: article
title: ConvNets Deep Learning book Ch9
comments: true
categories: data_science
image:
  teaser: jupyter-main-logo.svg
---

- Parameter sharing in RNNs and ConvNets is key to have input/outputs of arbitrary size since learned patterns are repeatedly applied to different inputs. ConvNet shares params in a single layer while RNN shares params across layers of a deep net (unfolded in time). 

- There are three basic transformations in an RNN (1) from the input to the hidden state, (2) from the previous hidden state to the next hidden state, and (3) from the hidden state to the output. In a simple RNN, when the network is unfolded, each of these corresponds to a shallow transformation. Thinking about RNNs this way, it's possible to make each of these transformations arbitrary complex (e.g. a deep net instead of shallow or even a different operation!). For example, the recurrence transformation can be a deep Autoencoder that is simply feeding its output back into input (long path might make optimization difficult) or several layers of recurrence stacked on top of each other (short path). 

- the unfolded recurrent structure allows us to factorize the function g(x1,x2,..,xt) of the whole past sequence into repeated application of a function f. Since hidden units are a function of earlier
time steps, the BPTT algorithm is necessary which is the same as BP for the completely unfolded network through time with gradients summed up through shared parameters across time steps (e.g. hidden weights).

- The hidden-to-hidden recurrence is powerful since it contains information from past inputs in the sequence but the training procedure is costly and cannot be parallelized due to the time dependance. We can make a recurrence from output-to-hidden $p(y1,y2/x1,x2)$. The advantage of eliminating hidden-to-hidden recurrence is that, for any loss function based on comparing the prediction at time t to the training target at time t, all the time steps are decoupled.

- Output-to-hidden recurrence can be trained with teacher forcing. At train time, we feed the correct output y(t) drawn from the train set as input to h(t+1). At test time, we approximate the correct output y(t) with the model’s output o(t), and feed the output back into the model. Teacher forcing may still be applied to models that have hidden-to-hidden connections as long as there is a output-to-hidden connection.

- RNN can be interpreted as a probabilistic graphical model in two ways. First by marginalizing out the hidden units and seeing the RNN as an autoregressive process where each point in time is a random variable dependent on points in the past. Second way of graphical model intrepretation is incorporating hidden nodes in the graphical model as deterministic nodes. This graphical representation will decouple past and future given the hidden units. Additionally in this view, the hidden recurrent function acts as an amortized way of representing the probability table for combining the hidden state and the observed value. Imagine if the observed takes k values in a series of length l, in the autoregressive case we would need k to power l possibilities in a table to describe all possible cases but the second view uses a single recurrent function (i.e. the hidden unit, Wh*h) to amortize this which doesn't depent on length of the series. 

- Amortization through parameter sharing in an RNN has a cost which is stationary assumption of conditionl distribution over variables meaning that it assumes the conditional distribution over parameters doesn't change through time. However, In principle, it would be possible to use t as an extra input at each time step and let the learner discover any time-dependence while sharing as much as it can between different time steps. Another way to attack this is change point detection and using seperate RNNs for each change point. 

- Sampling from the RNN is easy give the graphical model interpretation. However, we need to also know when to stop generating the sequence. This can be done using an additional symbol indicating sequence end at the end of each example data and then when this sequence is sampled we stop. Another way is to just input a time index of how close to the sequence end the RNN is at each time step plus an additional output that directly estimates a length number .Another more general way is to use an additional bernoulli output that estimates the probability of continuing or stoping at each time step. It can be a sigmoid unit trained with cross-entropy loss. 

- RNN might take in multiple inputs or model different conditional probability distributions. For example in the case of image captioning, an RNN models the conditional probability of the caption given an image. It might take in a single image for all time steps while taking in a sequence of words. This way, the image input is effectively similar to a bias for all time steps since it doesn't change with the sequence. An RNN might also model the conditional distribution of a sequence given another sequence for example machine translation. It takes in two sequences at the same time as it unrolls in time while the next time point output functions as the target value for the loss function. 

- In some cases, the prediction at time steps might depend on the whole sequence before and after that time step for example, word detection from phonemes. This happens when the units of time in the sequence is smaller thatn the minimum semantic unit of time in reality, for example speach needs a full sentence to make sence of the meaning and smaller units have lots od uncertainty. I think this also speaks to different time scales involved in the dynamics of a sequence. For example in language, an ambiguity might need a whole paragraph to be resolved while a word might need only a few letters to be resolved (thus the need to uncertainty in language models). In such cases we need a bi-directional RNN that models 1-to-t and t-to-1 relationships using two parallel RNN models with seperate hidden states that contribute to a single output; one starting from beginning and one starting at the end. 

- When the lengths of input and output sequences differ, RNN encoder-decoder (seq-2-seq) architechtures can be used. Seq-2-seq is very simple, (1) an encoder RNN processes the input sequence and produces an output representation as a function of its hidden state (context). (2) a decoder RNN is conditioned on that fixed-length vector to generate the output sequence. (3) both encoder and decoder are jointly trained to maximize coditional probability P(y/x). fixed-size encoder representation might be a limitation. 

- Recursive nets use a tree structure of recursively applying a fixed set (e.g. V, U, W) of parameters to a sequence. For example, it might apply V to each time point in the input sequence to make the leaves of the tree, then apply U and W recursively to form a binary tree up to the root. The advantage of recursive nets over RNN is that their depth for a fixed input lenth, T, is O(log(T)) while an RNN is O(T). This reduction in depth (number of compositions of nonlinear operations) can make optimization easier and help with long term dependancies. An open question is how to best structure the tree. 

- When composing many nonlinear functions (like the linear-tanh layer), the result is highly nonlinear, typically with most of the values associated with a tiny derivative, some values with a large derivative, and many alternations between increasing and decreasing. In an RNN, this is equivalent to a multiplying the recurrence matrix by itself T number of times which is equivalent to doing the same with its eigenvalues. Eigenvalues with magnitude less than one to decay to zero and eigenvalues with magnitude greater than one to explode. Very deep feedforward networks with carefully chosen scaling can avoid the vanishing and exploding gradient problem. 

- Recurrent networks involve the composition of the same function multiple times. Therefore, there are challenges in modelling long term dependencies in RNNs due to 1- vanishing/exploding gradienst caused by propagating the gradient through a lot of stages when RNN is unrolled in time, and 2- Even if gradients didn't vanish, the weights assigned to long range dependancies would be exponentially smaller than short term ones due to multiplications of many Jacobians. There are strategies for learning long term dependencies upto 100s of time steps but the problem in general is still open. 

- Vanishing gradients make it difficult to know which direction the parameters should move to improve the cost function, while exploding gradients can make learning unstable.

- Echo State Network (ESN) or liquid state machines set the recurrent weights such that the recurrent hidden units do a good job of capturing the history of past inputs, and only learn the output weights (reservoir computing). Theses are inspired by the observation that in an RNN learning, earlier layers don't change much due to vanishing and small gradients and most change happens at the output layer anyway. One way to think about these reservoir computing recurrent networks is that they are similar to kernel machines: they map an arbitrary length sequence into a fixed-length vector (the recurrent state h(t)), on which a linear predictor can be applied to solve the problem of interest. how do we set the input and recurren weights so that a rich set of histories can be represented in the recurrent neural network state? The answer proposed in the reservoir computing literature is to view the recurrent net as a dynamical system, and set the input and recurrent weights such that the dynamical system is near the edge of stability.

- Spectral radius of a Jacobian is the absolute value of the largest eigen value. Due to repeated application of the recurrent function, after passing through the unrolled recurrence, only the input in the direction of the spectral radius will matter. In a linear network, if two gradient values with small difference is back-propagated through time (with Jacobian of recurrence), the values of gradients at early time points diverge exponentially to explode in cases where spectral radius of Jacobian is bigger than one and vanish in cases where spectral radius is smaller than one. However, When a nonlinearity is present, the derivative of the nonlinearity will approach zero on many time steps, and help to prevent the explosion resulting from a large spectral radius. Therefore, most recent work on echo state networks advocates using a spectral radius much larger than unity.  

- Similar to spectral radius of Jacobian of recurrence in back propagation is spectral radius of recurrence function in forward propagation. Repeated application of the same function will cause a the hidden state to be "contractive" and forget information if spectral radius is smaller than one. 

- One difference between the purely linear case and the nonlinear case is that the use of a squashing nonlinearity such as tanh can cause the recurrent dynamics to become bounded. The strategy of echo state networks is simply to fix the weights to have some spectral radius such as 3, where information is carried forward through time but does not explode due to the stabilizing effect of saturating nonlinearities like tanh.

- the techniques used to set the weights in ESNs could be used to initialize the weights in a fully trainable recurrent network helping to learn long-term dependencies. In this setting, an initial spectral radius of 1.2 performs well, combined with the sparse initialization. 

- One way to deal with long-term dependencies is to design a model that operates at multiple time scales, so that some parts of the model operate at fine-grained time scales and can handle small details, while other parts operate at coarse time scales and transfer information from the distant past to the present more efficiently for example, adding skip connections, “leaky units”, and removing connections. 

- Skip connections add direct connections from variables in the distant past to variables in the present. One way of doing it is introducing delay into recurrent dynamics. Instead of recurrence from t to t+1, we add a delay of d to form recurrence from t to t+d. So both time scales exist in the RNN at the same time. Skip connections through d time steps are a way of ensuring that a unit can always learn to be influenced by a value from d time steps earlier. 

- Leaky units allows this effect to be adapted more smoothly and flexibly by adjusting the real-valued self-connection parameter rather than by adjusting the integer-valued skip length. A leaky unit has a linear self-connections which integrate the signal with different time constants using a moving average $u(t)=\alpha*u(t-1)+(1-\alpha)*v(t)$. The time constant of leaky units can be fixed, for example by sampling their values from some distribution once at initialization time or learned. Having  leaky units at different time scales appears to help with long-term dependencies.

- Removing some of the connections used to model fine-grained time scales is another approach to deal with long-term dependency. This idea differs from the skip connections because it involves actively removing length-one connections and replacing them with longer connections. Skip connections through time add edges. Units modified in such a way are forced to operate on a long time scale. One way of doing this is to make the recurrent units leaky, but to have different groups of units associated with different fixed time scales. Another option is to have explicit and discrete updates at different times, with a different frequency for different groups of units.

- Like leaky units, gated RNNs (i.e. LSTM/GRU) are based on the idea of creating paths through time that have derivatives that neither vanish nor explode. Leaky units did this with connection weights that were either manually chosen constants or were parameters. Gated RNNs generalize this to connection weights that may change at each time step. Leaky units allow the network to accumulate information (such as evidence
for a particular feature or category) over a long duration. However, once that information has been used, it might be useful for the neural network to forget the old state. Instead of manually deciding when to
clear the state, we want the neural network to learn to decide when to do it. This is what gated RNNs do.

- An LSTM cell has the same inputs and outputs as an ordinary RNN, but has a system of gating units that controls the flow of information in the recurrent operation to avoid vanishing/exploding gradients. The most important component is the state unit s(t) that has a linear self-loop similar to the leaky units. The self-loop weight is controlled by a forget gate unit f(t) that sets this weight to a value between 0 and 1 (using a sigmoid). The external input gate unit g(t) is computed similarly to the forget gate. The main difference of GRU with the LSTM is that a single gating unit simultaneously controls the forgetting factor and the decision to update the state unit. 

- An interesting idea is that second derivatives may vanish at the same time that first derivatives vanish.  If the second derivative shrinks at a similar rate to the first derivative, then the ratio of first and second derivatives may remain relatively constant (Hessian free optimization). Therefore, second-order optimization might be useful since these algorithms may roughly be understood as dividing the first derivative by the second derivative. However, second order optimization is computationally costly and since it jumps to extrema has a tendency to get stuck in saddles. Simpler methods like Nestrov momentum with careful initialization has shown similar resutls. General practice is using LSTM with SGD. 

- The objective function landscape of an RNN is usually characterized by cliffs. SGD takes us down the direction of steepest gradient and a step size that is appropriate for a relatively linear part of the landscape is often inappropriate and causes uphill motion if we enter a more curved part of the landscape on the next step (where there is upward curve in direction of steepest gradient). Therefore, we do gradient clipping to avoid traversing away from low cost regions to higher cost regions. If the explosion is so severe that the gradient is numerically Inf or Nan (considered infinite or not-a-number), then a random step can be taken and will typically move away from the numerically unstable configuration. Put another way,  raditional stochastic gradient descent uses an unbiased estimate of the gradient, while gradient descent with norm clipping introduces a heuristic bias that we know empirically to be useful. With elementwise clipping, the direction of the update is not aligned with the true gradient or the minibatch gradient, but it is still a descent direction.

- To deal with vanishing gradients we can use LSTMs and other self-loops and gating mechanisms. Another idea is to regularize or constrain the parameters so as to encourage “information flow.” In particular, we would like the gradient vector being back-propagated to maintain its magnitude, even if the loss function only penalizes the output at the end of the sequence. Computing the gradient of this regularizer may appear difficult, but we can use an approximation in which we consider the back-propagated vectors as if they were constants (so that there is no need to back-propagate through them). If combined with the norm clipping heuristic (to avoid grad explosion that prevents learning), this regularizer can considerably increase the span of the dependencies that an RNN can learn. A key weakness of this approach is that it is not as effective as the LSTM for tasks where data is abundant, such as language modeling.

- Neural networks excel at storing implicit knowledge. However, they struggle to memorize facts. Graves argued this is because neural networks lack the equivalent of the working memory. Memory would allow our systems not only to rapidly and “intentionally” store and retrieve specific facts but also to sequentially reason with them. Neural Turing machine is able to learn to read from and write arbitrary content to memory cells without explicit supervision about which actions to undertake using a soft addressing mechanism. Each memory cell can be thought of as an extension of the memory cells in LSTMs and GRUs. The difference is that the network outputs an internal state that chooses which cell to read from or write to, just as memory accesses in a digital computer read from or write to a specific address.

- It is difficult to optimize functions that produce exact, integer addresses. To alleviate this problem, NTMs actually read to or write from many memory cells simultaneously. To read, they take a weighted average of many cells. To write, they modify multiple cells by different amounts. The coefficients for these operations are chosen to be focused on a small number of cells, for example, by producing them via a softmax function. Using these weights with non-zero derivatives allows the functions controlling access to the memory to be optimized using gradient descent.

- RNNs have been found to preform similarly to LSTMs (Le, Jaitly, Hinton 2015) for some toy problems that involve very long range dependence. The trick is to initialize the hidden-to-hidden weight matrix xwith identity matrix so that it tends to preserve the hidden states. 







# Conditional Random Fields (CRF)

- We know how to perform classification, P(y/x), for example predict a label for an input image in MNIST. What if we want to do sequence classification, P(y1,y2,..yT/x1,x2,..xT)? for example, recognize the number on a cheque from an image (sequence of MNIST numbers). In this case the information from past are important in predicting the current label since something like 0000 is very unlikely. CRFs are probabilistic graphical models that are able to formulate this sequence classification problem.

- In regular classification, we pass network activation to softmax to make a probability p(y/x) for the label given data. In the simplest case of a CRF, linear chain, we add an extra term representing the sequence of labels before passing to softmax. This extra label is the likelihood of two labels appear after each other which comes from a matrix of how likely is one label to be followed by another label. This matrix is equivalent to the recurrent operation in an RNN that is applied to sequence of labels. 

- A linear chain CRF (a probabilistic graphical model) can be written as the product of 4 factors. 


- A Linear chain CRF is an autoregressive formulation expressing the dependency of a label on adjacent labels in the sequence. This formulation is equivalent to an RNN on labels where the classifier is the input to hidden transformation operation. The recurrent operation works the labels and not the inputs. We can represent all kinds of dependencies using RNNs on the labels, for example, a bi-directional RNN on labels. 




