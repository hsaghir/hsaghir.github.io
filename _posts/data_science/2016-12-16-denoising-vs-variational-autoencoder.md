---
layout: article
title: Denoising autoencoders vs. variational autoencoders 
comments: true
image:
  teaser: Autoencoder_structure.png
---

I love the simplicity of autoencoders as a very intuitive unsupervised learning method. They are in the simplest case, a three layer neural network. In the first layer the data comes in, the second layer typically has smaller number of nodes than the input and the third layer is similar to the input layer. These layers are usually fully connected with each other. Such networks are called auto-encoders since they first "encode" the input to a hidden code and then "decode" it back from the hidden representation. They can be trained by simply measuring the reconstruction error and back-propagating it to the network's parameters. If you add noise to the input before sending it through the network, they can learn better and in that case, they are called denoising autoencoders. They are useful because they help with understanding the data by trying to extract regularities in them and can compress them into a lower dimensional code.

![alt text](/images/Autoencoder_structure.png "A simple autoencoder")


There is another version of autoencoders, called "variational autoencoder - VAE" that has recently received a lot of attention. The neural network architecture is very similar to a regular autoencoder but the difference is that the hidden code comes from a statistical distribution that is learned during the training. Since it is not very easy to navigate through the math and equations of VAEs, I want to dedicate this post to explaining the intuition behind them. With the intuition in hand, you can understand the math and code if you want to. 

I start with a short history. In the 90s, a few researchers suggested a probabilistic interpretation of neural network models that was very promising since they offered a proper bayesian approach, robustness to over-fitting, uncertainty estimates, and could easily learn from small datasets. These are great properties that all machine learning practitioners strive for, so people were excited! However, learning parameters for such models proved to be very challenging, until recently. New advancements in deep learning research has led to more efficient parameter learning methods for such probabilistic methods. Therefore the excitement is back and the bayesian approaches to probabilistic reasoning have gained popularity again.

The probabilistic interpretation relaxes the rigid constraint of a single value for each parameter in the network by assuming a statistical distributions for each parameter. So for example, if in classical neural networks we calculated a weight as w_i=0.7, in the probabilistic version we calculate a Gaussian distribution around mean u_i=0.7 and some variance v_i=0.1, i.e. w_i=N(2, 0.1). This is done for all the weights but not biases of the network. This assumption will convert the inputs, hidden representations, and the outputs of a neural network to probabilistic random variables within a directed graphical model. Such a network is called a bayesian neural network or BNN.

![alt text](/images/weight_2_dist.png "parameters to distributions")


The goal of learning would now be to find the parameters of the mentioned distributions instead of single-value weights. This learning is now called "inference" in probabilistic terms since we want to infer distributions for weights from our data distribution. Inference in a Bayes net corresponds to calculating the conditional probability of latent variables with respect to the data, or put simply, finding the mean and variance for Gaussian distributions over parameters. 

It has been shown that exact inference in Bayes nets is NP-hard. So such models were not used much with big data until recently, where a variational approximate inference approach was introduced that transformed the problem into an optimization problem which could in turn be solved using stochastic gradient descent. “variational” is an umbrella term for optimization-based formulation of problems. It has historical roots in the calculus of variations and thus the name. A variational representation of a problem is its re-formulation in the form of a cost function that is ideally convex ([see conjugate duality for more on this](https://en.wikipedia.org/wiki/Convex_conjugate)) and can be optimized. For example, solving a linear matrix equation involves a matrix inversion which is hard, so we can solve this problem in the variational sense by reformulating it as an optimization problem that can be solved computationally using a method like gradient descent.

Let's get back to the bayesian net, since parameters now have distributions, the network can be re-parameterized based on the parameters of the distributions instead of single weight values. In a variational autoencoder, these distributions are only assumed on the hidden code not all parameters of the network. So the encoder becomes a variational inference network, that maps the data to the distributions for the hidden code and the decoder becomes a generative network that maps the hidden code back to distribution of the data. 

![alt text](/images/VAE_inf_gen.jpg "A Variational Autoencoder")

We need to sample the hidden code from its distribution to be able to generate data (hidden code is a distribution not a single value anymore). Therefore to make it differentiable, we treat mean and variances of the distributions as traditional network parameters and multiply the variance by a sample from a normal noise generator to add randomness. By parameterizing the hidden distribution this way, we can back-propagate the gradient to the parameters of the encoder and train the whole network with stochastic gradient descent. This procedure will allow us to learn mean and variance values for the hidden code and it's called the "re-parameterization trick". It is important to appreciate the importance of the fact that the whole network is now differentiable. This means that optimization techniques can now be used to solve the inference problem efficiently. 

In classic version of neural networks we could simply measure the error of network outputs with desired target value using a simple mean square error. But now that we are dealing with distributions, MSE is no longer a good error metric. So instead, loosely speaking, we use another metric for measuring the difference between two distributions i.e. [KL-Divergence](https://www.quora.com/What-is-a-good-laymans-explanation-for-the-Kullback-Leibler-Divergence). It turns out that this distance between output and data distributions is not very easy to minimize either. However using some math, we know that this distance comprises of two main parts and that it is always positive. So instead of minimizing the whole thing, we can maximize a lower bound (ELBO) on the smaller term. If after maximizing, the lower bound is close to the output distribution, then the distance is close to zero and voila! we have minimized the error distance. The algorithm we use to maximize the lower bound is the exact opposite of gradient descent. Instead of going in the reverse direction of the gradient to get to the minimum, we go toward the positive direction to get to the maximum, so it's now called gradient ascent! This whole algorithm is called "autoencoding variational bayes"!

Now that the intuition is clear, take a look at [this]() Jupyter notebook that puts these intuitions into high-level code and makes the understanding more concrete. The code is useful even if you have never worked with Keras. This is based on the implementation of a [VAE in Keras](https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder.py)


references:
[1] Kingma, Diederik P., and Max Welling. "Auto-encoding variational bayes." arXiv preprint arXiv:1312.6114 (2013).

[2] Rezende, Danilo Jimenez, Shakir Mohamed, and Daan Wierstra. "Stochastic backpropagation and approximate inference in deep generative models." arXiv preprint arXiv:1401.4082 (2014).