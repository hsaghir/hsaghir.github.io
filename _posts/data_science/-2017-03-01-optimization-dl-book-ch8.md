---
layout: article
title: Optimization Deep Learning book Ch8
comments: true
categories: data_science
image:
  teaser: jupyter-main-logo.svg
---

- Learning is different from pure optimization in that, in learning we usually indirectly optimize the performance measure we are interested in. We want to optimize a performance measure (minimize risk) over the true data generation process, but since don't know the true distibution, we minimize the risk over the emprical distribution defined by training set (emprical risk minimization).

- Additionally, The performance measure we care about is not always differentiable (e.g. binary classification) and with emprical distributions there is always the problem of over-fitting. Therefore, we define a surrogate loss function (e.g. negative log-likelihood of positive class with regularization) over emprical distribution to estimate the optimization of a performance measure over the true data distribution. Therefore, instead of stopping when we reach at a local minimum, we stop when early stopping says we are at a minimum wrt the true data distribution.

- Another differentiating aspect of learning from optimization is that the loss function usually decomposes as a sum over examples. Additionally we estimate properties like gradients and updata parameters in a stochastic way over a minibatch of examples as opposed to over all examples($std err=\sigma/n^.5$ meaning calculating properties on small number of examples is not that bad). In the extreme case where we update parameters with minibatch of only one example, we call it online learning (examples may be an online stream). 

- Minibatch size is usually bounded by memory issues. In a multicore system smaller than a minimum batch size doesn't increase learning speed and bigger than a maximum batch size adversly affects learning due to memory issues. For GPUs we usually use batch sizes of power 2 due to memory issues between 32-256. Smaller batch sizes have regularization effects due to noise but require smaller learning rates. First order methotds like SGD work well with minibatch sizes in the order of 100 but second order methods need much larger batch sizes (like 10,000) due to accumulated errors!

- When using SGD, only the first epoch provides an unbiased estimate of the error. But we use more epochs since they reduce the variance of the estimator although they are biased.

- Optimization, even convex, is challenging in high dimensions. One common problem is different curveture of the loss function in different directions (ill-conditioned Hessian). While we might have strong gradient that helps going downhill, we might have large curveture in the direction of the gradient that makes the cost go up inadvertantly. SGD is blind to the curveture, and an SGD step will increase the cost with even a very small step (Taylor expansion of cost adds $0.5*lr^2*g^T*H*g - lr*g^T*g$ in each step) and learning slows down since lr has to be made smaller to compensate for the bigger curveture. Plotting and monitoring these two terms during training helps find if the first term gets bigger than the second. Momentum can help if curveture in all directions are close. 

- Another problem happens when we have different curveture in different directions. Nestrov momentum can help with this to some extent but it gets harder as the difference in curveture in different directions gets high. 

- Due to the exchangability of weights of a layer in a NN, and exchangability of layers, and the possibilityf of scaling weights in consecutive layers using ReLU, there are many ways the network parameters can achieve the same loss value (many local minima). Therefore, NNs have weight space symmetry and it's non-identifiable meaning no single one setting of params is best. Since the loss value for these types of local minima are same, they are not problematic. If the loss value for local minima are high compared to global minima, we have a problem. In a suffciently large NN, it is believed that the loss value for local and global minima are close so we don't need to find global minima for learning to work. Plotting and monitoring norm of gradient during training helps. If the norm doesn't shrink to insignificant value the problem is not local critical points. 

- Think of loss surface as a mountain. In high dimensions there are many more saddles than local minima/maxima, and critical points with high cost are more likely to be saddles than local minima. There are also flat regions where both gradient and curvetre are zero which are hard to deal with. There might also be cliff edges (usually produced by repeated multiplications of weights corresponding to vanishing/exploding gradients due to non-one eigenvalues), where jumping from them will take the parameters very far from where they are. Gradient clipping (exploding gradient) can limit the distance we go from current positions. Cliffs are dangerous since in vanishing gradient we don't know the direction to move to and in exploding gradient we get unstable learning.  

- Gradient methods are designed to go downhill without regard to a special critical point but second order methods are designed to jump onto critical points. Therefore, second order methods are prone to saddles and hard to scale (there is a saddle-free newton method that might be useful if it can be scaled). Gradient methods have emprically shown to be able to escape saddles.

- Gradients are usually not exact since we might use stochastic estimates based on a minibatch or the loss might be intractable and we use a stochastic approximate of gradient. 

- Training time is dependant on the length of the trajectory from initial points to final point. Many existing works are focused on finding good initial points for optimization than algorithms that move non-locally. While characterizing critical points is a focus of research, many deep nets learn mappings in regions that gradients are not small and regions are not critical.

- Optimization ALGOs:


- Batch norm is not an optimization algo. It simply calculates the mean and std of the activations in a minibatch for each layer. Then simply normalizes the activation before feeding to nonlinearity by subtracting the mean and dividing by the std. It then adds two new parameters $\alpha*z+ \beta$ to be able to control the mean and std of the layer as it wishes. The intuition behind batch norm is that consequent layers multiplicatively confound the statistics of the data and by the time we get to middle layers in a deep net the statistics are already very different. This causes problems for the gradients by making the gradient too big or too small after multiplicative interactions. Usually higher order derivatives are able to give insight into how these multiplicative intractions have confounded the statistics but calculating higher order gradients in not feasible in deep nets. Batch norm makes sure the statistics remains unchanged at each layer and provides two new variables $\alpha, \beta$ for SGD to be able to scale the statistics as it wishes in a smart way that doesn't need the knowledge of higher order derivatives. Since batch norm works on batches instead of all the training set, very much like SGD it introduces some noise (additive noise using mean and multiplicative noise using std) to the hidden units which helps with robustness as a regularizer. However, batch norm mainly helps with optimization and if dataset is small we might want to use stronger regularizers like dropout as well. 