% for final publication use:
% \documentclass[aps,pre,twocolumn,showpacs,showkeys,superscriptaddress,groupedaddress]{revtex4}

% for review and submission use preprint:
\documentclass[aps,preprint,showpacs,superscriptaddress,groupedaddress]{revtex4}  %

\usepackage{graphicx}  % needed for figures
\usepackage{dcolumn}   % needed for some tables
\usepackage{bm}        % for math
\usepackage{amssymb}   % for math
\usepackage{caption}
\usepackage{hyperref}

% avoids incorrect hyphenation, added Nov/08 by SSR
\hyphenation{ALPGEN}
\hyphenation{EVTGEN}
\hyphenation{PYTHIA}


\graphicspath{ {images/} }

\begin{document}
\hspace{5.2in} \mbox{DRAFT}

\title{Dynamical systems view of Neural networks bridges deterministic and probabilitic view of NNs}

\author{Hamidreza Saghir}
\author{Tom Chau}
\author{Azadeh Kushki}

\affiliation{Institute of Biomaterials and Biomedical Engineering, University of Toronto, Toronto, Ontario, Canada}
\affiliation{Bloorview Research Institute, Holland Bloorview Kids Rehabilitation Hospital,Toronto, Ontario, Canada}


\date{\today}

\begin{abstract}

\end{abstract}

\keywords{Scaling maps, multi-fractal spectrum, fractal analysis, complexity analysis, local exponents}

\pacs{05.45.Tp, 05.40.Ca, 87.19.Hh}
\maketitle

\section{\label{sec1}Background}


\section{Bayesian Probability and Inference}
When we setup a Bayesian inference problem with N unknowns, we are implicitly creating an N dimensional space of parameters for the prior distributions to exist in. Associated is an additional dimension, that reflects the prior probability of a particular point in the N dimensional space. Surfaces describe our prior distributions on the unknowns, and after incorporating our observed data X, the surface of the space changes by pulling and stretching the fabric of the prior surface to reflect where the true parameters likely live. The tendency of the observed data to push up the posterior probability in certain areas is checked by the prior probability distribution, so that lower prior probability means more resistance. More data means more pulling and stretching, and our original shape becomes mangled or insignificant compared to the newly formed shape. Less data, and our original shape is more present. Regardless, the resulting surface describes the posterior distribution.
 



\section{Unsupervised Literature}
As mentioned what unsupervised learning methods have in common is that they all define an energy function (loss) which possesses parameters (model) and they minimize the energy on the data. This is equivalent to pushing the energy down on the manifold of the data. However, the challenge of unsupervised learning is how to push up the energy outside the manifold to enable it to learn data manifold. This is where unsupervised methods differ and in short, this is how methods compare in terms of their approach to pushing the energy up for non-data points:

1. Build models so that the volume of the low energy stuff (dataset) is constant. These don't explicitly push up on energy outside manifold but limit the space of possible manifolds 
- PCA (limit to linear manifolds only)
- k-means (Limit the manifold to prototypes only) 
- GMM (Limits possible manifolds to Gaussian)
- square ICA (Limits to )

2. If $E(Y)=|Y-G(Y)|^2$, Make the model, $G(Y)$ as constant as possible 
- Contracting autoencoder, saturating autoencoder

3. Use a regularizer that limits the volume of the low energy space (representation space). We push down the energy for representations corresponding to data points. If the representation capacity is in the same ballpark as the data, other non-data points will have no other way than to be mapped to this representation as well which will cause them to to inccur a higher reconstruction cost automatically since the representation capacity is already filled with data and the non-data cannot have new representations and must use the same representations. Therefore, this scheme is equivalent to pushing energy down on energy manifold and up on everywhere else. 

- examples: Sparse coding, sparse autoencoder, PSD, MAP estimate. it's done by restricting the information content of the code using a regularization constraint like sparsity. 

4. Minimize the gradient and maximize the curveture around data points
- Score matching

5. Train a dynamical system so that the dynamics goes to the manifold
- Denoising autoencoder: train inference dynamics so that noisy samples go to (are mapped to) clean samples. 

6. Push down the energy of data poits, push up everywhere else
- Bayesian inference pushes energy down on data and needs tractable partition function for everywhere else to push up. One way to get around this is variational inference which assumes a general approximation for the space using a function that is then pushed down on data points and up every else. Consider the duality of energy and probability here; Where energy is low, probability is high and where energy is high probability is low. 

- Maximum likelihood only pushes down energy on data and forgoes pushing up, MAP estimate constrains space where low energy can happen by enforcing a prior. 

7. Push down the energy of data points, push up on chosen locations. Needs generating contrastive points for the energy function to push up

- MCMC methods (non-parametric way of generating points continued until mixing of markov chain)

- Contrastive divergence (RBM, start from a random point, push the energy down there; stochastically move away from the poiny a bit down on energy landscape and push up there. It is similar to MCMC but doesn't continue until mixing and convergence and instead uses EM algorithm to switch between two optimizations and iteratively do what MCMC seeks to do in a single step after mixing)

- Adversarial training (Can think of it as a sort of parametric way of doing MCMC where a generative network learns to generate contrastive points for pushing energy up)

- Ratio matching, Noise contrastive estimation, Minimum probability flow


\subsection{parameter learning}

- The variational family used in VI or the type of loss function used in learning  (e.g. cross-entropy loss = Gaussian family?) is an electrically charged goo that we throw on probability/energy surface and let physics (optimization) do its thing to reshape the goo in the form of true data energy/probability manifold. We define a distance measure (used in the loss function/VI distance e.g. KL) that determines the amount of electric charge of the goo and data and therefore the difference in potential of the goo and the data energy manifold. This distance defines a force field, the integral of which defines the potential energy. Letting the system run its physics (or use optimization) will take the system to regions with low energy (high probability).

- Additional constraints may be added to the loss function to limit the space of possible goo for regularization purposes or to minimize the volatility of parameters through kinetic energy (uncertainty in Bayesian). Letting the dynamics run on the total energy (i.e. potential + kinetic), will move the system to a region where both the distance between the goo and data manifold is small (low potential) and where the volatility of parameters is also small (low kinetic).

- To train parameters of the model, we can unroll the recurrency procedure and propagate the gradient through time much like BPTT and use SGD. There seems to be a very general equivalency between Monte Carlo sampling (contrastive divergence) and recurrently cycling data through a network. Alternatively, we can form the Hamiltonian of a network and find the equations of motion on the energy surface for the network. We can then use the equations of motions on the energy surface to update network prameters!?


- Yashua Benjio professes equilibrium prop which is very close to the energy and dynamical system view I've been thinking about as an alternative to using SGD and BPTT; there’s a potentially more biologically plausible alternative to backprop called equilibrium propagation that requires neither an explicit loss function nor gradients (Bengio). Seems similar to contrastive divergence in RBMs, where we stochastically pave the energy surface down to low energy regions if we let the system converge. This is a dynamical system in MCMC senses.  Training works something like this: (1) Clamp the input of the system to some input value. (2) Let the system converge until there is a stable predicted output. (3) Measure some stats within the system. (4) Clamp the output to the true output value. (5) Let the system converge again. (6) Measure the same stats as before. (7) Update the system’s parameters based on the difference in stats. 

- The potential energy of an AE as in memisevic's work is a great idea to interpret a whole neural net as a dynamical system with potential and kinetic energy that are dependent on data and the net architecture. Potential energy seems to correspond to learning the manifold and kinetic energy to stability of the system staying in a region of the state space (uncertainty). The goal of learning corresponds to minimizing the total energy of the network to get both a GOOD and STABLE result. This has parallels to the bayesian interpretation of neural nets where weights are not just a single number (potential) but also have variance (kinetic?) - see Yarin Gal's PhD thesis for more on Bayesian NN interpretation.  

- We might be able to throw an initial momentum (how? what is mass? what is velocity? data/architecture?) on our parameters (noise?) and calculate the kinetic energy along with the potential energy of the network by integrating the force field and momentum of the network. Minimizing the total energy would be simply running the dynamics of the network by passing the output in repeatedly until convergence. 

- Theoretically studying energy landscape instead of a random objective functions makes it possible to know the properties of a class of landscape prior to optimization, effectively removing the need to know about second order gradients to estimate the objective function landscape at each point. 

- Gradient Descent Learns Linear Dynamical Systems. See paper!

\subsection{Kinetic energy can be interpreted in the context of inference or regularization}
-- Regularization techniques similar to constraining the space of possible potential energy manifold (model capacity) have same effect as priors. For example, We know that in MAP inference, the priors appear as regularization terms. 

-- Regularization techniques involving resampling (i.e. Monte Carlo) are very similar to recurrence in the model and seem to have parallels to kinetic energy of the model.

-- Another strategy is to add noise to inputs or hidden representations to generate new data and make model robust to noise. Adding noise to input is equivalent to imposing a norm regularization on weights. Another way of using noise for regularization is adding it to weights which is equivalent to stochastic implementation of Bayesian inference over weights. Can we calculate an stochastic estimation of the kinetic energy using the same strategy?

-- If we add iid Gaussian noise to weights at each presentation of input, the minimization of the objective function will be the same as before with the addition of a new regularization term in the form of gradient of output wrt weights to power 2. This has similarity to kinetic energy where we see velocity (gradient).

-- Double backprop regularizes the Jacobian (gradient wrt all weights) to be small. This is has similarities to capping the kentic energy of the network by capping the weight velocities to be small! Adversarial training finds inputs near original inputs and trains the model to produce same output for them as well. The same way data augmentation is non-infinitesmall version of tangent prop, adversarial training is the non-infinitesmall version of double back-prop. Tangent prop and dataset augmentation require that model to be invariant in certain directions while adversarial training and double backprop encourage the changes to be small in all directions when input change is small. 

\subsection{Hamiltonian Monte Carlo}
Hamiltonian : potential +kinetic energy. \url{http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html}

hmc move: a symbolic Python function which given a starting position, generates $\chi$ by randomly sampling a velocity vector. It then calls simulate dynamics and determines whether the transition $\chi$

simulate dynamics: a symbolic Python function which, given an initial position and velocity, will perform n steps leapfrog updates and return the symbolic variables for the proposed state $\chi' \rightarrow \chi$' is to be accepted.

hmc updates: a Python function which, given the symbolic outputs of hmc move, generates the list of updates for a single iteration of HMC.

HMC sampler: a Python helper class which wraps everything together. \url{http://deeplearning.net/tutorial/hmc.html}


We start by thinking of our function as a kind of a valley. If you squint just a little at the plot above, that shouldn't be too hard. And we imagine a ball rolling down the slope of the valley. Our everyday experience tells us that the ball will eventually roll to the bottom of the valley. Perhaps we can use this idea as a way to find a minimum for the function? We'd randomly choose a starting point for an (imaginary) ball, and then simulate the motion of the ball as it rolled down to the bottom of the valley. We could do this simulation simply by computing derivatives (and perhaps some second derivatives) of C - those derivatives would tell us everything we need to know about the local "shape" of the valley, and therefore how our ball should roll.


\subsection{emprical demonstration}

- Recurrent Autoencoder uses a feedback loop from the decoder output to its input and shows that performs better in learning the distribution of data than even a VAE! 

- For a test case, we made a simple set of 3D data where the first two dimensions were sampled from IID unit-norm Gaussian distributions, but the third dimension was equal to $(x_1-x_2)^2$. A pure autoencoder encodes these samples quite accurately, but if we pick random points in the latent space they seem to have nothing to do with the distribution at all. On the other hand, the reconstruction error isn’t great with the variational autoencoder and while it has some sense of the distribution, its still not quite right. There’s too much weight centered at the vertex, and furthermore the distribution seems to be biased in $x_3$ (probably being pulled towards the mean value of $x_3$). A few rounds of Monte-Carlo resampling of the plain autoencoder does seem to make it behave much better as a generative model, but it still isn’t converging to the actual distribution. On the other hand, the recurrent autoencoder ends up learning a very sharp model of the correct distribution.

- The pattern we want to make note of here is that there seems to be a very general equivalency between Monte Carlo sampling (contrastive divergence?) and recurrently cycling data through a network. This was used in \url{http://www.robots.ox.ac.uk/~szheng/papers/CRFasRNN.pdf}

\subsection{Advantages of NNs as dynamical systems}
- A direct result of viewing NNs as dynamical systems is that we can study their characteristics like architecture in terms of the properties of the potential and kinetic energy functions. Their optimization dynamics will also be defined as the dynamics of the running dynamical system. Therefore, we can study things like stability of different architectures and find architectures with ill-posed (training) dynamics. 

- A vast literature of mathematical tools for studying linear and non-linear dynamical systems will also be available to us. Things like saddle and attractor points will be the characteristics of the model (potential energy landscape) rather than the optimization problem. We might be even able to reverse-engineer finding desired network architectures by designing the energy landscape first and then deriving the network architecture from that (architecture search)!

- Natural brain-like learning algorithms like equiblirium prop are the natural way of thinking about dynamical systems in terms of letting the dynamics run instead of hard optimization. 

- The new advances in learning to learn and in new successful methods profess the conflation of modeling and optimization or model and inference. Don't think of them seperately. Find a combination that works. I think the concept of potential energy in a dynamical system which directly depends on the model architecture relays this very well. Instead of thinking about model and optimization in seperate ways we just think about models with well-behaved potential functions or we can even design nice potential functions and then reverse-engineer the model architecture. 

- In deep learning models, we only get point estimates without uncertainty knowledge, it's hard to score models, select models, perform complexity penalization. All these become natural with the dynamical systems view of neural nets. 

- In Bayesian framework, There is a unified framework for model building, inference, prediction and decision making, explicit accounting for uncertainty and variabibility of the outcomes, and robust to overfitting, and there are tools for model selection and composition. However, on the other hand, mainly conjugate and linear models are used, otherwise intractable inference leads to expensive computations or long simulation times. Thinking about NNs in terms of dynamical systems keeps the good things about Bayesian thinking and removes the problems using a variational inference framework that comes naturally by running the dynamics of the system and estimating energy functions. 

- It remains an open question whether there are many local minima of high cost for networks of practical interest and whether optimization algorithms encounter them. Studying the shape of loss surfaces from the point of view potential energy derived from the integral of a force field, it would be easier to study the theoretical characteristics of the loss function and answer questions like this.

- In performing optimizations, we only look at the value of loss and maybe keep track of only a small history for the likes of AdaGrad. From the dynamical systems perspective, the state of the NN and the gradient at each step determines the shape of the loss/potential surface. Why don't we keep track of them of reconstruct the surface? We might even be able to use another deep NN to approximate the un-explored loss/potential surface using the training and state results we already have computed using SGD !?


- A DIM is simply a deep neural network with random noise injected at certain layers. That could also be interpreted as an emprical Bayesian neural net. From the point of view of dynamical systems, this is a stochastic dynamical system and the noises can help with figuring out the kinetic energy. The learning in DIM is done through Variational Inference using a modified ELBO based on denisty ratio estimate. Therefore, effectively, what ends up happening is throwing a variational goo on the energy/probability space and optimizing it to match the energy/probability surface. This goo is an implicit model (a transformation function from a noise to a surface) which has intractable likelihood. Intractable likelihood means that we can't use the common ELBO we use in evidence estimation, therefore, we use density ratio estimation to get the posterior (i.e. match the goo to the energy surface). Note that denisty ratio estimation is different from the running the dynamics analogy since we use a classifer to do denisty ratio estimation and not recurrently feed the output back into input to run the dynamics (or do SGD). Therefore, density ratio estimation is a method for fitting a goo onto a surface. This fitting happens iteratively in two phases. The classifier first defines a surface to tell the goo and the desired surface apart. Then the goo(transformation function) adjusts itself to breach the classifier surface as much as it can and therefore reshaping itself closer to the desired surface. Then the classifier tries again to tell the goo and the real surface apart capturing more intricacies in the difference of the surface and the goo. The goo then again tries to reshape itself to be able to reshape itself as much as it can in the form of the surface to fool the classifier. This goes on until the goo matches the surface very closely! The cool thing is that both the goo and real surface are intractable and yet we are still able to find a mapping function from a simple noise to a close surrogate of the surface (i.e. matched goo)!

- Imagine we match a transformation function (goo) to the potential energy surface. We additionaly use a conditioned transformation function that is able to navigate the minimums of the surface of the goo by varying only a sinle dimension of the noise. This way we don't need to do SGD anymore! We can simply use density ratio estimation to match a conditioned goo and then vary the conditioned dimention to navigate the minimum of our cost function! 

- Interpreting a NN as a dynamical system, makes it easier to open the black-box of neural nets to see how a decision is made by the system. This can be done in the same fashion as system identification methods or dynamic analysis of system. We actually do this in control theory very well, we analyze the dynamics of a system using its phase portrait to get a map for the all possible dynamics of the system. 


\subsection{Energy shaping}

- The dynamics of a system is a treasure map for getting from one state to another state. A phase portraits of a dynamical system shows  trajectory paths that a dyamical system naturally takes due to its physics to get from one state to another. The gradient descent method we use in NNs is equivalent to physics doing its job in helping the dynamical system pave its trajectories. Therefore, (1) initialization and (2) input to recurrent function (i.e. the neural net)  are means for shaoing the energy of a NN. Energy shaping can help put the system on a trajectory path that will take us to our desired location. For that to happen we need to know more about the treasure map (phase portraits) of a specific family or type of NNs.

A phase portraite might reveal the orbits of the dynamical system as contours of constant energy. One very special orbit, known as a homoclinic orbit, is the orbit which passes through the unstable fixed point. In fact, visual inspection will reveal that any state that lies on this homoclinic orbit must pass into the unstable fixed point. Therefore, if we seek to design a nonlinear feedback control policy which drives the simple pendulum from any initial condition to the unstable fixed point, a very reasonable strategy would be to use actuation to regulate the energy of the pendulum to place it on this homoclinic orbit, then allow the system dynamics (aka gradient descent) to carry us to the unstable fixed point. 

In the case of a simple inverse pendulum, the brute force control strategy is to use a very high torque actuator to take the pendulum from downward position to upright position in a very conservative move. However, we can use a simple actuator to add enough energy to the system that running the dynamics (gradient descent) would swing the pendulum upward. If we write the energy function, we see that adding/removing energy is simple. We simply apply torque in the direction of velocity to add energy and in the reverse direction to remove energy.


In fully-actuated control theory, we brute force the system to go from one state to another by usually making the dynamics of our controllers cancel out the systems' dynamics. In fully-actuated control we don't use system dynamics to achieve goals. But under-actuated control presents ideas like above to use the dynamics of a system to achieve goals. Using the dynamics of a system leads to agile movements with surprisingly small control effort. The payoff is that we need to understand the dynamics very well to be able to use it to achieve goals. 



\newpage
\bibliographystyle{apsrev4-1}
%\bibliography{library}
\begin{thebibliography}{}

\bibitem[Costa et~al., 2005]{Costa2005}
Costa, M., Goldberger, A., and Peng, C.-K. (2005).
\newblock {Multiscale entropy analysis of biological signals}.
\newblock {\em Physical Review E}, 71(2):021906.

\bibitem[Goldberger et~al., 2000]{Goldberger2000}
Goldberger, a.~L., Amaral, L. a.~N., Glass, L., Hausdorff, J.~M., Ivanov,
  P.~C., Mark, R.~G., Mietus, J.~E., Moody, G.~B., Peng, C.-K., and Stanley,
  H.~E. (2000).
\newblock {PhysioBank, PhysioToolkit, and PhysioNet : Components of a New
  Research Resource for Complex Physiologic Signals}.
\newblock {\em Circulation}, 10 1(23):e215--e220.

\bibitem[Goldberger et~al., 2002]{Goldberger2002}
Goldberger, A.~L., Amaral, L. a.~N., Hausdorff, J.~M., Ivanov, P.~C., Peng,
  C.-K., and Stanley, H.~E. (2002).
\newblock {Fractal dynamics in physiology: alterations with disease and aging.}
\newblock {\em Proceedings of the National Academy of Sciences of the United
  States of America}, 99 Suppl 1:2466--72.

\bibitem[Ihlen, 2012]{Ihlen2012}
Ihlen, E. a.~F. (2012).
\newblock {Introduction to multifractal detrended fluctuation analysis in
  matlab.}
\newblock {\em Frontiers in physiology}, 3(June):141.

\bibitem[Ivanov et~al., 1999]{Ivanov1999}
Ivanov, P.~C., Amaral, L.~a., Goldberger, a.~L., Havlin, S., Rosenblum, M.~G.,
  Struzik, Z.~R., and Stanley, H.~E. (1999).
\newblock {Multifractality in human heartbeat dynamics.}
\newblock {\em Nature}, 399(6735):461--5.

\bibitem[Kantelhardt and Koscielny-Bunde, 2001]{Kantelhardt2001}
Kantelhardt, J. and Koscielny-Bunde, E. (2001).
\newblock {Detecting long-range correlations with detrended fluctuation
  analysis}.
\newblock {\em Physica A: Statistical \ldots}, 295(2):441--454.

\bibitem[Kantelhardt, 2008]{Kantelhardt2008}
Kantelhardt, J.~W. (2008).
\newblock {Fractal and Multifractal Time Series}.
\newblock {\em Encyclopedia of Complexity and Systems Science}, page~59.

\bibitem[Kasdin, 1995]{Kasdin1995}
Kasdin, N.~J. (1995).
\newblock {Discrete Simulation of Colored Noise and Stochastic Processes and
  1/f\^{}a Power Law Noise Generation}.
\newblock {\em Proceedings of the IEEE}, 83(5):802 -- 827.

\bibitem[Moody et~al., 2001]{Moody2001}
Moody, G., Goldberger, a., McClennen, S., and Swiryn, S. (2001).
\newblock {Predicting the onset of paroxysmal atrial fibrillation: the
  Computers in Cardiology Challenge 2001}.

\bibitem[Peng et~al., 1994]{Peng1994}
Peng, C.~K., Buldyrev, S.~V., Havlin, S., Simons, M., Stanley, H.~E., and
  Goldberger, a.~L. (1994).
\newblock {Mosaic organization of DNA nucleotides}.
\newblock {\em Physical Review E}, 49(2):1685--1689.

\bibitem[Porter, 2008]{Porter2008}
Porter, F.~C. (2008).
\newblock {Testing Consistency of Two Histograms}.
\newblock page~35.

\bibitem[Struzik, 2000]{Struzik2000}
Struzik, Z.~R. (2000).
\newblock {Determining Local Singularity Strengths and Their Spectra With the
  Wavelet Transform}.
\newblock {\em Fractals}, 08(2):163--179.

\bibitem[West, 2013]{West2013}
West, B. (2013).
\newblock {\em {Fractal physiology and chaos in medicine}}.

\bibitem[West, 1994]{West1994}
West, B.~J. (1994).
\newblock {\em {Fractal Physiology}}, volume~3.
\newblock Oxford University Press.

\end{thebibliography}

\end{document}
