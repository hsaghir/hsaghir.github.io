% for final publication use:
% \documentclass[aps,pre,twocolumn,showpacs,showkeys,superscriptaddress,groupedaddress]{revtex4}

% for review and submission use preprint:
\documentclass[aps,preprint,showpacs,superscriptaddress,groupedaddress]{revtex4}  %

\usepackage{graphicx}  % needed for figures
\usepackage{dcolumn}   % needed for some tables
\usepackage{bm}        % for math
\usepackage{amssymb}   % for math
\usepackage{caption}
\usepackage{hyperref}

% avoids incorrect hyphenation, added Nov/08 by SSR
\hyphenation{ALPGEN}
\hyphenation{EVTGEN}
\hyphenation{PYTHIA}


\graphicspath{ {images/} }

\begin{document}
\hspace{5.2in} \mbox{DRAFT}

\title{solving amortization gap with contextual bandit inference neworks - multi-amortized VAEs}

\author{Hamidreza Saghir}

\affiliation{Borealis AI, Toronto, Ontario, Canada}


\date{\today}

\begin{abstract}

\end{abstract}

\keywords{Scaling maps, multi-fractal spectrum, fractal analysis, complexity analysis, local exponents}

\pacs{05.45.Tp, 05.40.Ca, 87.19.Hh}
\maketitle

\section{\label{sec1}Background}


\section{Amortization Error}


- [Inference Suboptimality in Variational Autoencoders](https://openreview.net/forum?id=Bki4EfWCb): The quality of posterior inference is largely determined by two factors: a) the ability of the variational distribution to model the true posterior and b) the capacity of the recognition network to generalize inference over all datapoints. We analyze approximate inference in variational autoencoders in terms of these factors. We find that suboptimal inference is often due to amortizing inference rather than the limited complexity of the approximating distribution. We show that this is due partly to the generator learning to accommodate the choice of approximation. Furthermore, we show that the parameters used to increase the expressiveness of the approximation play a role in generalizing inference rather than simply improving the complexity of the approximation.

- [On the challenges of learning with inference networks on sparse, high-dimensional data](https://arxiv.org/abs/1710.06085):

- [Iterative Amortized Inference](https://arxiv.org/abs/1807.09356): We aim toward closing this gap by proposing iterative inference models, which learn to perform inference optimization through repeatedly encoding gradients.

-  [Amortized Inference Regularization](https://arxiv.org/abs/1805.08913): However, it is often overlooked that an overly-expressive inference model can be detrimental to the test set performance of both the amortized posterior approximator and, more importantly, the generative density estimator. In this paper, we leverage the fact that VAEs rely on amortized inference and propose techniques for amortized inference regularization (AIR) that control the smoothness of the inference model. We demonstrate that, by applying AIR, it is possible to improve VAE generalization on both inference and generative performance.

- [Avoiding Latent Variable Collapse With Generative Skip Models](https://arxiv.org/abs/1807.04863):  we propose a new way to avoid latent variable collapse. We expand the model class to one that includes skip connections; these connections enforce strong links between the latent variables and the likelihood function. We study these generative skip models both theoretically and empirically. Theoretically, we prove that skip models increase the mutual information between the observations and the inferred latent variables

\section{Bandit and reward}


- [On-line Adaptative Curriculum Learning for GANs](https://arxiv.org/abs/1808.00020): In order to provide meaningful feedback for learning efficient mixtures of encoders, we consider different reward functions to generate R_i(t). We argue that progress (i.e., the learning slope (Graves et al., 2017; Matiisen et al., 2017)) of the encoder is a more sensible way to evaluate our policy. Let Î¸(t) be the encoder parameters at episode t. We define the two following quantities for measuring encoder


- [MGAN: TRAINING GENERATIVE ADVERSARIAL NETS WITH MULTIPLE GENERATORS](https://openreview.net/forum?id=rkmu5b0a-): encoders create parameters that are intended to form the posterior distribution, whilst the decoder decodes a sample from the posterior to the reconstruction, and the classifier specifies which encoder a set of posterior parameters come from (classifier has mostly tied weights with the decoder). The distinguishing feature is that internal samples are created from multiple decoders, and then one of them will be randomly selected as final sample similar to the mechanism of a probabilistic mixture model.

- [Semi-Amortized Variational Autoencoders](https://arxiv.org/abs/1802.02550): recent empirical work suggests that inference networks can produce suboptimal variational parameters. We propose a hybrid approach, to use AVI to initialize the variational parameters and run stochastic variational inference (SVI) to refine them. Crucially, the local SVI procedure is itself differentiable, so the inference network and generative model can be trained end-to-end with gradient-based optimization. 


\section{under-utilization of latent variables}

- [Tackling Over-pruning in Variational Autoencoders](https://arxiv.org/abs/1706.03643)


\newpage
\bibliographystyle{apsrev4-1}
%\bibliography{library}
\begin{thebibliography}{}

\bibitem[Costa et~al., 2005]{Costa2005}
Costa, M., Goldberger, A., and Peng, C.-K. (2005).
\newblock {Multiscale entropy analysis of biological signals}.
\newblock {\em Physical Review E}, 71(2):021906.

\end{thebibliography}

\end{document}
