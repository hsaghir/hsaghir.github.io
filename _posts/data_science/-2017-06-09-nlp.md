---
layout: article
title: NLP
comments: true
categories: data_science
image:
  teaser: jupyter-main-logo.svg
---

## Representation
- Natural language consists of discrete units of letters, words, and sentences. In order to be able to perform machine learning on natural language, we have to first represent it in a numerical format. The central problem in NLP seems to be representation of natural language as numbers. After than the problem is simply sequence and structure learning where RNN-like models shine. 

- Basic representation started by defining a dictionary as the list of all allowed words or letter combinations. Common representations are:
    + Ascii code representation: Doesn't tell you anything about the meaning of the word. 
        * Distance between words are not well defined.
    + one-hot encoding: a vector with the size of the dictionary where all entries are zero except the single entry corresponding to the word in a location. 
        * The distance between all words is a constant one without encoding relationships between words, no notion of meaning is present, huge dimensionality problem.
    + Bag of words: add up one-hot encodings of words in each document (word counts) and normalize. 
        * Latent semantic analysis (LSA) models would do matrix factorization (SVD) on the bag of words representations and pick the eigen-vectors as word representations. 
        * The same linear relationships observed in word2vec were also observed in LSA to a lower extent with proper scaling of bag of words frequencies. 
    + word2Vec encoding: Based on the notion that words appearing in similar contexts are related to each other semantically. We learn word vector representations by defining a model that predicts a word given its context and context given a word. We condition a word on its neighbors. The probability of a word (center) given its neighbors (n) is determined by the normalized softmax of the distance between two word vectors (dot product distance for skip-gram) $$p(n|c)=\frac{\exp{u_n^T . u_c}}{\sum_c \exp{u_n^T . u_c}}$$ (i.e. angle between the vectors). 
        * We let the word embeddings be parameters. Training an LSTM with softmax to predict context from a word learns word2vec representations.
        * After learning the word embeddings, word vectors will be stored in a lookup table with an index for each word.
        * The similarity of two words is represented by the angle between their vectors (cosine similarity). Therefore, similar words will be parallel and very dissimilar words will be orthogonal. In one-hot all words are orthogonal which doesn't make sense given their semantic similarities.
        * the result is a dense vector representation of words that embeds words appearing in each others context in the same region of the vector space (low distance).
        * There usually is a linear relationship between word vectors for example relationship between king/queen is similar to man/woman
    + Glove: goes from word-counts (bag-of-words) to nice meaningful embedding properties of the word2vec model. The crucial insight is that co-occurrence probabilities of words might be volatile but the ratio of co-occurrence of words is a much more stable measure for embedding meaning. 
        * the way this ratio is mapped to word vectors is equating the probability with distance $$w_i.w_j=\log p(i|j)$$.
        * can we use density ratio estimation here?
    + Skip Thought Vectors: Generate sentence codes in the style of word embeddings to predict context sentences. One encoder and two decoder to predict previous and next sentences.
        * sentence_t -> sentence_{t-1} sentence_{t+1} [Kiros et al.,2015]
    + Paragraph vector: A paragraph is represented as a vector in the same space as the single-word embeddings.
        * non-RNN model. A paragraph matrix D is a bag-of-sentences representation and a vocabulary matrix W is a bag-of-words representation. we train on the bag-of-words W to predict next words using bag-of-sentences D as context.


- Obviously the number of words in a language is much smaller than the combination of the letters in that language. Why not learn a language-specific word manifold that maps characters to words? Then we can use the char-level language models instead of word-level models and reduce the dimensionality of the problem since number of chars are multiple orders of magnitude smaller than words. 

- a measure: can we take these word vectors and use them in other task like language modeling, translation, etc

## Generating sentences from a continuous space

- Auto-encoders: Typically composed of two RNNs, The first RNN encodes a sentence into an intermediate vector, The second RNN decodes the intermediate representation back into a sentence, ideally the same as the input.
    + Regular auto-encoders learn only discrete mappings from point to point spanning the whole lower dimensional space. However, if we want to learn holistic information about the structure of sentences, we need to be able to fill sentence space better as a lower dimensional manifold thus VAEs are better. 
    + In a VAE, we replace the hidden vector z with a posterior probability distribution q(z|x) conditioned on the input, and sample our latent z from that distribution at each step. We ensure that this distribution has a tractable form by enforcing its similarity to a defined prior distribution, typically some form of Gaussian. Alternatively we can use an implicit distribution which is much more flexible and can better model the real posterior. 
    + Optimization problems: Decoder too strong, without any limitations just doesn’t use z at all – Fix: KL annealing – Fix: word dropout
        * Word dropout – Keep rate too low: sentence structure suffers – Keep rate too high: no creativity, stifles the variation
    + Used VAE to create language models on the Penn Tree-bank dataset, with RNNLM as baseline
        * Task: train an LM on the training set and have it designate the test set as highly probable. RNNLM outperformed the VAE in the traditional setting. However, when handicaps were imposed on both models (input-less decoder), the VAE was significantly better able to overcome them.
        * Task: infer missing words in a sentence given some known words (imputation).
            - Place the unknown words at the end of the sentence for the RNNLM –  RNNLM and VAE performed beam search (VAE decoding broken into three steps) to produce the most likely words to complete a sentence
            - Precise evaluation of these results is computationally difficult
            - Instead, create an adversarial classifier, trained to distinguish real sentences from generated sentences, and score the model on how well it fools the adversary
            - Adversarial error is defined as the gap between chance accuracy (50%) and the real accuracy of the adversary.ideally this error will be minimized
            - Several other experiments in the appendix showed the VAE to be applicable to a variety of tasks – Text classification – Paraphrase detection – Question classification
        * Task: sample from the VAE, interpolate between sentences by exploring the latent dimension.





