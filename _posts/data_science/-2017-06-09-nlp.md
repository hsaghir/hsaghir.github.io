---
layout: article
title: NLP
comments: true
categories: data_science
image:
  teaser: jupyter-main-logo.svg
---

# NLP (Natural Language Processing)

## Representation
- Natural language is the means of communication for humans. It represents concepts, entities, and the relationships among them in the real word and therefore is quite complex. However, on the representation level it consists of discrete units of letters, words, and sentences. 

-  Mathematically speaking, a language model is the joint probability distribution of words such that correct and meaningful sequence of words/characters have a high probability distribution while incorrect sequences have low probability. This joint distribution can be factorized further based on assumptions like n-gram models meaning that only n sequences depend on each other and not more. 

- In order to be able to perform machine learning on natural language, we have to first represent it in a numerical format. The central problem in NLP seems to be representation of natural language as numbers. After that the problem is simply sequence and structure learning where a vast array of tools from machine learning exist. 

- the process of turning language symbols into sth we can work with is called tokenization. 
    + Manual tokenization (example my TextRep class)
    + Many of the best practices for tokenizing raw text have been captured and made available in a Python library called the Natural Language Toolkit or NLTK for short.

- Basic representation started by defining a dictionary as the list of all allowed words or letter combinations. Common representations are:
    + Ascii code representation: Doesn't tell you anything about the meaning of the word. 
        * Distance between words are not well defined.
    + tf-idf score: tf is a normalized term frequency in the document while idf is a weight that corrects for more frequent words like 'the'. The tf-idf score is the product $$tf.idf$$$.
        * tf-idf is a measure of the relative importance of a word in a document. 
        * It doesn't bear any information about semantic relationships of words.
    + one-hot encoding: a vector with the size of the dictionary where all entries are zero except the single entry corresponding to the word in a location. 
        * The distance between all words is a constant one without encoding relationships between words, no notion of meaning is present, huge dimensionality problem.
    + Bag of words (BoW): add up one-hot encodings of words in each document (word counts) and normalize. 
        * Latent semantic analysis (LSA) models would do matrix factorization (SVD) on the bag of words representations or bigram word frequency matrix. They pick the eigen-vectors as word representations which can encode some semantic and syntactic (part of speech) information. 
        * The same linear relationships between word vectors in word2vec were also observed in LSA to a lower extent with proper scaling of bag of words frequencies. 
            - scaling involves: weighing the co-occurrence count based on distance between the words in the document, ignoring function words like "he", "she", etc.
- Word2Vec models: a class of NN models that learn a vector representation for words from an unlabeled corpus of text. 
    + Skip-Gram: Based on the notion that words appearing in similar contexts are related to each other semantically. Skip-gram predicts a window of neighboring words from a single word.
        * We convert the corpus into samples with a neighboring window of n-grams on both sides of a word.
        * input is one-hot representation of a single words. each neighbor word has a corresponding output. Therefore, outputs are n words in the neighborhood (both sides), each having a one-hot representation.
        * The model has a bottleneck hidden layer between the input word and output neighboring words. The weights between one-hot input and hidden layer will be used as m-dimensional word embedding matrix (W) after training. 
        * no nonlinearity is used in the bottleneck layer since inputs are one-hot and only activate a single row of the weight matrix.
        * the weight matrix between the bottleneck layer and each output node is shared.
        *  a softmax is used for each neighboring word output to produce a one-hot vector for that neighboring word.
            -  In the two-class logistic regression, we pass the single output through a sigmoid function, $$\frac{1}{1+exp(-x)}$$, to crush it into the $$[0,1]$$ range and interpret it as a probability i.e. $$P(Y=0)= sigmoid(x), P(Y=1)=1-P(Y=0)$$. 
            -  In the multi-class logistic regression, with have $$K$$ outputs so we can't just say one minus probability of one class. Therefore, we need a generalization of the sigmoid to crush all the outputs into the $$[0,1]$$ range to interpret them as probabilities. If we write the output equations for the other class in the two class logistic regression, we can [derive the generalization](https://stats.stackexchange.com/questions/233658/softmax-vs-sigmoid-function-in-logistic-classifier) to softmax function $$P(Y=j)=\frac{exp(z_j)}{\sum_k exp(z_k)}$$.
        * The loss function is negative log-likelihood loss i.e. $$L= -\frac{1}{n} \sum_k \log(a)$$
            - In two class logistic regression, we can use cross-entropy loss with the sigmoid output since there are only two outputs i.e. $$L= - \frac{1}{n} \sum_x (y \log(o) + (1-y)\log(1-o))$$

            - In the multiclass logistic regression, the equivalent of cross-entropy for softmax output is negative log-likelihood loss i.e. $$L= - \frac{1}{n} \sum_x \log(o_y)$$.

            - The problem is using softmax with thousands of outputs (one-hot words) as a multi-class classification is very costly. So the idea of training word-vectors with negative sampling comes from density ratio estimation using noise contrastive estimation (NCE). The basic idea is to convert the density estimation problem (i.e. predicting a distribution over all words as a huge-muliclass classification problem) to a density ratio estimation problem that can be solved with a binary classification of distinguishing true sequences of words (data) from fake sequences of words (keeping central word and randomly sampling the context). 
                + The negative samples are chosen from a slightly modified distribution that favors rare words.

        * Training Algo for word embeddings: 
            - Represent each word as a d dimensional vector (or connect one-hot word input to a d-dim bottleneck so that weight matrix (W) becomes a d-dim word representation).
            - Represent each context word as a d dimensional vector (or connect the d-dim bottleneck layer to n one-hot outputs for contexts).
            - Initalize all vectors to random weights.
            - Arrange vectors in two matrices, W and C.
            - feed in the corpus formatted as bi-ngrams. 
            - Negative sampling objective is based on NCE which basically constructs a fake dataset of bi-ngrams from the original bi-ngrams by replacing the input word with a random one while keeping the context the same. Objective then tries to assign high probability to correct bi-ngrams while assigning low probability to fake bi-ngrams.
            - After training, Throw away the C matrix and use the W matrix as word-embeddings. 
        * If we don't throw away the C matrix, the product $$W.C$$ will produce the co-occurrence frequency matrix for each word and its ngrams. Therefore, word2vec is essentially doing a matrix factorization on that matrix in a more algorithmically efficient way than LSA. Even LDA can also be interpreted as a matrix factorization.
       
    + Continuous BoW (CBoW): It's the exact mirror of skip-gram model where a word is predicted from it's neighbors instead. We train a simple one-layer logistic regression model to predict the one-hot encoding of a word from the one-hot representation of its neighbors. 
        * After the training with maximum likelihood, the weight matrix can be used as a word embeddings. 
        * typically used as a pre-training for initialization of the word-vectors before learning an embedding. 
    + word2Vec encoding: Based on the notion that words appearing in similar contexts are related to each other semantically. We learn word vector representations by defining a model that predicts a word given its context and context given a word. We condition a word on its neighbors. The probability of a word (center) given its neighbors (n) is determined by the normalized softmax of the distance between two word vectors (dot product distance for skip-gram) $$p(n|c)=\frac{\exp{u_n^T . u_c}}{\sum_c \exp{u_n^T . u_c}}$$ (i.e. angle between the vectors). 
        * We let the word embeddings be parameters. Training an LSTM with softmax to predict context from a word learns word2vec representations.
        * After learning the word embeddings, word vectors will be stored in a lookup table with an index for each word.
        * The similarity of two words is represented by the angle between their vectors (cosine similarity). Therefore, similar words will be parallel and very dissimilar words will be orthogonal. In one-hot all words are orthogonal which doesn't make sense given their semantic similarities.
        * the result is a dense vector representation of words that embeds words appearing in each others context in the same region of the vector space (low distance).
        * There usually is a linear relationship between word vectors for example relationship between king/queen is similar to man/woman
    + Glove: goes from word-counts (bag-of-words) to nice meaningful embedding properties of the word2vec model. The crucial insight is that co-occurrence probabilities of words might be volatile but the ratio of co-occurrence of words is a much more stable measure for embedding meaning. 
        * the way this ratio is mapped to word vectors is equating the probability with distance $$w_i.w_j=\log p(i|j)$$.
        * can we use density ratio estimation here?
    + Skip Thought Vectors: Generate sentence codes in the style of word embeddings to predict context sentences. One encoder and two decoder to predict previous and next sentences.
        * sentence_t -> sentence_{t-1} sentence_{t+1} [Kiros et al.,2015]
    + Paragraph vector: A paragraph is represented as a vector in the same space as the single-word embeddings.
        * non-RNN model. A paragraph matrix D is a bag-of-sentences representation and a vocabulary matrix W is a bag-of-words representation. we train on the bag-of-words W to predict next words using bag-of-sentences D as context.
    + Word-vector domain adaptation: Adapting word-vectors from a domain with large data to one with less data is a useful thing. However, each domain usually has it's own specific set of terms which makes using of general purpose embeddings challenging. Is it possible adapt general-purpose word embeddings to a specific domain?
        * The idea is to train word-vectors first on the larger domain, then use a regularizer on the word2vec objective that penalizes the L2 distance between the embeddings of the shared words in the source and target domains. (see Yang et al 2017)
        * These distances are weighted by a significance function based on the notion that words that are similar in frequency of occurence are probably semantically not domain-specific (similar to Glove objective) and are very transferable (should have similar embeddings). 
        * Words that have different frequencies of occurance are domain-specific and are less transferable (less similar embeddings).
    + cross-lingual word embeddings: The idea is to align embedding spaces of words instead of lexicons. (Connea et al 2017) did unsupervised word alignment:
        * First two sets of word embeddings are trained independently on the two languages (X, Y). Model learns a mapping between them such that translations are close in the common space.
        * A GAN is used similar to cyclaGAN? The generator is (W.X) and the discriminator tells languages apart. They first train discriminator and then the generator.
        * Two extra steps are: 
            - remove noise that rare words introduce 
            - build translations using the learned mapping and a distance measure

- LDA: While word2vec tries to predict a word from its local context, LDA tries to predict the word from the global document. LDA is a mixture model, meaning that each document will have mixed membership in a core set of topics. While word2vec representations are dense, LDA representations are sparse (only a few topics).
    + LDA treats documents as a bag-of-words vector. concatenating BoW vectors makes a matrix representation of corpus. 
    +  Clustering with Drichlet prior finds which words are part of the same cluster based on the closeness of their frequencies. similar frequencies, are assigned to a topic.
    +  assigns each column (BoW of a document) a membership value of the found set of clusters aka topics (e.g. doc1 is 20% topic 1, 10% topic 2, etc).
    +  Assuming M topics exist, we want to represent each document with an M-dimentional vector.
    + LDA2Vec builds document representations on top of word embeddings using topic and document vectors. In LDA2Vec, we have an additional document vector (informing on the context of the document) as an additional input to skip-gram model which predicts context word vectors from a pivot word vector. 
    + a document vector is the multiplication of document weight vector and a topic matrix (concat topic vectors). 
    + Can LDA2Vec document vector be used as an additional attention mechanism in seq2seq models that attends to a specific topic existing in the doc?

- Obviously the number of words in a language is much smaller than the combination of the letters in that language. Why not learn a language-specific word manifold that maps characters to words? Then we can use the char-level language models instead of word-level models and reduce the dimensionality of the problem since number of chars are multiple orders of magnitude smaller than words. 

- a measure: can we take these word vectors and use them in other task like language modeling, translation, etc

## NLP tasks
Easy
• Spell Checking
• Keyword Search
• Finding Synonyms
Medium
• Parsing information from websites, documents, etc.
Hard
• Machine Translation (e.g. Translate Chinese text to English)
• Semantic Analysis (What is the meaning of query statement?)
• Coreference (e.g. What does "he" or "it" refer to given a document?)
• Question Answering (e.g. Answering Jeopardy questions)


### Language modeling:
A language model predicts the probability of next word given the history of words seen so far i.e. P(next word | history). Given a proposed probability model q, one may evaluate q by asking how well it predicts a separate test sample. 

The best known metrics for evaluating a language model is model perplexity on test data $$T = {w_1, ..., w_t}$$. Perplexity quantifies the average number of bits needed to represent a test event if one uses a code based on the model. It is defined as 2 or e to the power of the average model entropy (i.e. average probability assigned to each word in the test set). 

Cross entropy is the belief of the model about the likelihood of targets (real next words) under the model and shows the discrepency between reality and model beliefs. If we average the cross_entropy losses of all the words in the test set, we get average test set entropy under the model. Therefore exp(cross_entropy_loss) is perplexity. 

#### Regularization: Recurrent Weight dropout
- using nn.dropout(), a new binary dropout mask is sampled each and every time the dropout function is called even if the given connection is repeated (e.g. different input time steps recieve different dropout masks). 
- Basically what weight-drop does is to use the same dropout mask for all steps in both forward and backward directions(variational dropout). Applied to input-layer uses same mask for all inputs, and applied to a recurrent layer, uses the same dropout mask for recurrent steps. The important part is using same mask for recurrent steps that avoids drowning out the signal.
- For recurrent connections DropConnect (zeroing out individual weights as opposed to ) is recommended

- zeroing out a percentage of weights of the embedding weight matrix helps with regularization of language models. 

#### Regularization: TwinNet
- run a second RNN in a reverse direction and make a soft constraint between cotemporal forward and backward states regularizing the RNN. 
- This forces the forward states to hold information about the longer-term future, regularizing the RNN and helping with long term prediction.
-  Only the forward network is used during test

[Twin Networks: Matching the Future for Sequence Generation]

#### Cache pointer
After a word appears once in a document, it is much more likely to appear again. For example in a word like tiger with low frequency in the whole corpus of wikipedia is much more likely in an article about tigers. Cache pointer models have a cache component, which contain the words that appeared in the
recent history (e.g. a fixed number of words).

- Save the history (cache) of hidden activations and target words. Then perform a matrix vector product of the history matrix and the current hidden vector and use it as an attention score. 
- apply softmax to this attention score to get a distribution over the history
- apply this distribution to the history of target words to get relevant words that have actually appeared in text

#### Embeddings factorizations

- low rank of word embeddings limits expressivity. replaceing softmax with a mixture of softmaxes helps. "Breaking the Softmax Bottleneck: A High-Rank RNN Language Model"
- compress word embeddings over 94% w/o hurting performance by approximate matrix factorization and Gumbel-softmax. "Compressing Word Embeddings via Deep Compositional Code Learning"

#### Teacher forcing:
“Teacher forcing” is the concept of using the real target outputs as each next input, instead of using the decoder’s guess as the next input. Using teacher forcing causes it to converge faster but when the trained network is exploited, it may exhibit instability.

You can observe outputs of teacher-forced networks that read with coherent grammar but wander far from the correct translation - intuitively it has learned to represent the output grammar and can “pick up” the meaning once the teacher tells it the first few words, but it has not properly learned how to create the sentence from the translation in the first place.






### [Summarization](https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/)

#### Extractive:
Most extractive methods to date identify sentences based on human-engineered features such as sentence position and length (Radev et al., 2004), the words in the title, the presence of proper
 nouns, content features such as word frequency (Nenkova et al., 2006), and event features such as action nouns. Then it's treated as a retrieval task using sth like a classifier.

LexRank outperforms Gensim’s TextRank by a narrow margin (Blue and Rouge scores) but sometimes TextRank gives higher quality of summaries.

- TextRank unsupervised graph based approach in gensim
    + Pre-process the text: remove stop words and stem the remaining words.
    + Create a graph where vertices are sentences.
    + Connect every sentence to every other sentence by an edge. The weight of the edge is how similar the two sentences are (gensim uses Okapi BM25 function to see how similar the sentences are).
    + Run the PageRank algorithm on the graph.
    + Pick the vertices(sentences) with the highest PageRank score
- [LexRank: similar to TextRank](https://pypi.python.org/pypi/sumy)
    + uses IDF-modified Cosine as the similarity measure between two sentences.
    + also does a post-processing to make sure that top sentences chosen for the summary are not too similar to each other.

- PyTextRank: TextRank with enhancements (lemmatization, POS tags and NER, extracting meaningful key phrases)
    + POS Tags and lemmatization for every sentence.
    + extract key phrases and their counts, normalize.
    + score sentence with jaccard distance between the sentence and key phrases.
    + Summarizes based on most significant sentences and key phrases.
    
- old school huristic approach, (PyTeaser)
    + assigns a score to sentences which is linear combination of features extracted from that sentence
    + titleFeature: The count of words which are common to title of the document and sentence.
    + sentenceLength: Authors of TextTeaser defined a constant “ideal” (with value 20), which represents the ideal length of the summary, in terms of number of words. 
    + sentenceLength is calculated as a normalized distance from this value.
    + sentencePosition: Normalized sentence number (position in the list of sentences).
    + keywordFrequency: Term frequency in the bag-of-words model (after removing stop words).


- [Neural extractive summarization](Cheng and Lapata,2016; Nallapati et al., 2017; Narayan et al., 2017; Yasunaga et al., 2017; Narayan et al., 2018;) Usually a hierarchical encoder to derive a meaning representation and an attention mechanism to directly select sentence/words from input (i.e. pointer networks). Neural-based approaches usually have document length limits.

    + Sentence encoder: use a CNN or RNN on the sequence of word embeddings in a sentence, and either average or concatenate the intermediate representations to get a sentence vector. 
    + Yasunaga et al., 2017 form a graph of similarities between sentences and apply a graph convnet on the adjacency matrix of the graph. Then they use a greedy heuristic to extract salient sentences.
    + Document encoder: send the sequence of sentence embeddings into the encoder of the seq2seq model.
    + Sentence Extractor: the decoder of the seq2seq model is a sentence extractor that receives the sequence of sentences and assigns a 0/1 label to them using a Softmax (i.e. cross entropy loss). 
    + Sentence Scoring: Instead of pure classification, use the softmax score as a sentence score and assemble the top m ranking sentences to get the summary
    + RL: Instead of pure maximum likelihood training (i.e. cross-entropy loss), we can also add a REINFORCE objective to the loss to minimize a pre-defined distance (e.g. a distance that maximizes the ROUGE score) of the extracted vs. gold summaries. 

#### Abstractive:

- [SOTA](https://www.salesforce.com/products/einstein/ai-research/tl-dr-reinforced-model-abstractive-summarization/)

- GoogleBrain's TextSum, is a seq2seq model that generates headline (summary) from the first 2 lines in an article trained on GigaWord dataset for 4000 GPU hrs. [ref](https://hackernoon.com/how-to-run-text-summarization-with-tensorflow-d4472587602d)

- Rush et al. (2015), generating headline from first sentence of article using seq2seq with attention on Gigaword corpus (news articles from number of publishers) 

- Nallapati et al. (2016), proposes abstractive summarization dataset by modifying a QA dataset of news articles paired with story highlights from Daily Mail and CNN. 
    * More difficult than headline generation since information contained in highlights(summary parallel) are spread across the article. 

- [A Deep Reinforced Model for Abstractive Summarization] combines attention mechanism, pointer networks and REINFORCE to train a seq2seq model for text summarization.

- [Generating Wikipedia by Summarizing Long Sequences] casts the wikipedia generation task as a multi-document summarization to write a wikipedia article based on its sources. 
    + First relavant paragraphs are extracted from reference documents and documents retrieved through search engine queries through a TD-IDF-based ranking. Then abstractive summarization is performed using a modification of Transformer networks ([Vasvani et al 2017 ](https://arxiv.org/abs/1706.03762)). A mixture of experts layer further improves performance. 
        * (Vasvani et al 2017) present training sequence-2-sequence models based solely on attention mechanisms without lstm/conv encoder/decoders. [link](https://medium.com/@sharaf/a-paper-a-day-24-attention-is-all-you-need-26eb2da90a91)
    + Data: Linked articles as well as the results of an external web search query are used as input documents, from which first section of Wikipedia article must be generated

### Neural Machine Translation
It consists of a seq2seq model with attention that translates a sequence to another sequence. Therefore, it requires a supervised dataset of parallel sequences. For example translating a sentence from English to French. In practice, 4 areas are important in making NMT work:
    +  Using gated RNNs such as LSTM, GRU (for long term features)
    +  Usually needs large stacked RNNs in both encoder and decoder (e.g. 2-16 RNN layers)
    +  Input feeding where the predicted word is fed back into the input as well as the previous attention vector.
    +  Test-time decoding is done through beam search where multiple hypothesis target predictions are considered at each time step. 

OpenNMT is a pytorch package that implements these stuff out of the box. A good tutorial is [here](https://sites.google.com/site/acl16nmt/home). 


- In NMT using seq2seq models, we want to arrive at a translation that has the best score. This is due to the fact that the decoder needs to make a sequence of decision about words to pick at each decoding time-step. The final score depends on the whole sequence and is delayed (RL?). a few simple decoding strategies are usually used in decoders of seq2seq models:
    + greedy search, 
        * argmax(logits)
        * picking the most likely symbol at each time-step. 
        * Efficient, but heavily suboptimal search
    + Ancestral sampling, 
        * Multinomial(logits)
        * sampling a word based on logit weights at each time-step 
        * It's unbiased but has high variance and it's pretty inefficient.
    + beam search,
        * Maintain K hypotheses at a time, Expand each hypothesis, Pick top-K hypotheses from the union of the expanded hypotheses (greedy decision).
        * It's asymptotically exact as K goes to infinity, 
        * K should be selected to maximize the translation quality on a validation set.
        * Usually provides better results than greedy search/ancestral sampling but is computationally expensive.
    + Decoding from an ensemble of encoder-decoders
    + maybe RL? (beyond maximum likelihood estimation of sequence)

- Word Coverage to make sure all input words are translated can be enforces using a regularizer on the matrix of all attentions weights. 
    + By definition, the rows of this matrix sum to one since each row is a softmax applied to the output of the comparison function. The sum of the set of alignment weights on the encoder outputs are then one. for example if max_length of the input sequence is 1200, then the alignment weight vector is 1x1200.
    + There is vector of weights for each word in the decoder sequence. For example, if the max_length of the decoder sequence is 135, then the matrix of all attention weight vectors will have a 135x1200 dimension. 
    + In this matrix each row sums to one by definition of softmax. If we now enforce a regularization that each column also sums to one, that would be a way to enforce the condition that each word in the input sequence is attended to at least once or in other words every word in the input sequence should be equally attended to. Therefore, all words in the input sequence will be covered in the translation. 
        * This condition is implemented as a regularizer for the objective function that the optimizer will try to push toward zero. $$\lambda \sum_{across_input_word} (1 - \sum_{across_decoder_sequence})^2 $$

### Unsupervised Machine Translation Using Monolingual Corpora Only 
The paper uses two unparalleled corpus (e.g. English novel, Spanish news), to perform translation. 

- Uses two denoising autoencoders (seq2seq) to reconstruct perturbed inputs from each corpus (autoencoder loss). (Noisifies input by randomly dropping a word or shift each word from its position with the constrained that a token can shift from its position at most k tokens to the left or to the right)

- Training the autoencoder to produce a sentence in corpus B from a sentence in corpus A and vice-versa. adding the BCE loss of the two together (translation loss). 

- Additionally a discriminator is used on the output of the encoder that discriminates which language the sentence is from (adversarial cost).

- The three terms of, autoencoder loss, translation loss , and discriminator loss are added up

Training loop:
1- Obtain translation using encoder of Language A and Decoder of Language B (translation)
2- Train each Autoencoder to regenerate an uncorrupted sentence when given a corrupted sentence (autoencoders)
3- corrupt the translation obtained in Step 1 , and recreating it. For this step the encoder of Language A , and Decoder of Language B are trained together (and also encoder of Language B and Decoder of Language A )

### Question answering:
SQuAD (Stanford Question Answering Dataset)[3][4] formulates a machine learning problem where the model receives a question and a passage and is tasked with answering the question using the passage. The answers are limited to spans of text. The training data consists of (question, paragraph, answer span) triplets. Due to the nature of the task, combining the information contained in the passage with the question posed is paramount to achieve good performance (See references for more information). Recurrent neural networks that combine the information from the question and paragraph using coattention mechanisms such as the Dynamic Coattention Network [1] and its deeper and improved version [2] have achieved the best results in the task so far.

SOTA models:

- BiLSTM + DCN-like Coattention + Naive decoder
- DCN+ encoder combines the question and passage using a dot-product(coattention)
- The decoder is application specific, specifically made for finding an answer span within a passage


## Interesting Models for NLP
### Quasi-RNN (QRNN):
QRNN simply applies a convolution to a sequence, but then considers the sequence nature in the the pooling operation. The way QRNN does it is to use three convolution operations in parallel to form three information gates, i.e. forget gate (f), input gate (i) and candidate gate (z). Here are the steps. 

1- We convolve the input with three convolutions corresponding to input gate, forget gate, and the candidate gates. These are simple convolutions that can be parallelized and have no RNN-like time dependency. 

2- the output of each convolution is then locally pooled by linearly combining each two neighboring points and assiging them their corresponding time index. After that, for candidate convolution, the output is squashed to $$[-1, 1]$$ using a $$tanh$$ and gates are squashed to $$[0,1]$$ using a sigmoid, i.e. $$z_1 = tanh(W_z . x_0 + W_z . x_1) $$. Combining two neighboring points is equivalent to an LSTM since in a simple RNN, the hidden only depends on its previous hidden i.e. $$h_t = W_{hh} . h_{t-1}$$. We can combine more than two sequence to form a longer than Markov dependency which is not the case in a regular RNN.

3- the candidate and the gates are then combined to form an LSTM-like operation, i.e. the gates and candidate vectors are multiplied element-wise. For example, if we want our hidden state to forget an amount and add new information to the same amount, we might use only a forget gate and a candidate gate to make a hidden state like: $$h_1 = f_1 * h_0 + (1 - f_1) * z_1 $$ with element-wise multiplication. If we want to use output and input gates as well, we combine the gates differently like an LSTM. 

### Phased LSTM
Phased LSTM is just a sparser-updating version of LSTM which extends the LSTM unit by adding a new time gate. This gate is controlled by a parametrized oscillation with a frequency range that produces updates of the memory cell only during a small percentage of the cycle (updates memory $$c_t$$ and hidden $$h_t$$ only when the gate is open). It seems to be a good model for learning very long dependencies and irregularly-timed sequences.

- The opening and closing of this gate is controlled by an independent rhythmic oscillation specified by three parameters; The first parameter, $$\tau$$ , controls the real-time period of the oscillation. The second, $$r_{on}$$, controls the ratio of the duration of the “open” phase to the full period. The third, $$s$$, controls the phase shift of the oscillation to each Phased LSTM cell. 

- A linearized formulation of the time-gate is $$\phi = \frac{(t-s) mod \tau}{\tau}$$. The time gate output $$k_t$$ is a formula like $$\frac{2 \phi_t}{r_{on}}$$. Then we simply treat the phased updates of the memory cell and hidden state as an additional gate i.e. $$h_i = k_i * h'_i + (1 − k_i ) * h_{i−1}$$. 


### Attention Mechanism

- Attention serves the need for translating long sequences to long sequences since a single last vector of encoder can't remember everything about the sequence. However, attention mechanism is simply giving the network access to its internal memory, which is the hidden state of the encoder. In this interpretation, instead of choosing what to “attend” to, the network chooses what to retrieve from memory.
- how do models decide which positions in input seq/memory to focus their attention on? They actually use a combination of two different methods: content-based attention and location-based attention. 
    + Content-based attention allows model to search through their memory and focus on places that match what they’re looking for. 
        * In this case, attention required a query and a similarity function. It has to compare the similarity of the query and every item in memory (e.g. using a dot-product).
    + while location-based attention allows relative movement in memory, enabling the models to loop.
- In seq2seq models, the attention distribution is usually generated with content-based attention. The attending RNN generates a query describing what it wants to focus on. Each item is dot-producted with the query to produce a score, describing how well it matches the query. The scores are fed into a softmax to create the attention distribution.
    + First we need a function to compare target and source hidden states. e.g.
        * $$h_t . h_history$$, 
        * $$h_t . W_att . h_history$$, 
        * $$v_att . tanh(W_att[h_t; h_history])$$
    + Second, we need to convert the output of comparison into relevence weights. Hard attention is a one-hot representation of relevent word to each word but since that's discrete and non-differentiable, people usually use soft attention by applying a softmax to the outputs of the comparison function to get alignment weights. 
    + Then a context vector is built using the weighted averages of the input sequence hidden vectors with above-mentioned alignment weights
    + Conditioned on the concatenation of the context vector and decoder input, the decoder now can compute its next state. 

- Fast weights [Hinton] act like a kind of attention to the recent past but with the strength of the attention being determined by the scalar product between the current hidden vector and the earlier hidden vector. The input x(t) is the context used to compare to previously stored values h.
    + In a fast associative memory there is no need to decide where or when to write to memory and where or when to read from memory. The fast memory is updated all the time and the writes are all superimposed on the same fast changing component of the strength of each synapse
    + Every time the input changes there is a transition to a new hidden state which is determined by a combination of three sources of information:
        * The new input via the slow input-to-hidden weights, C,
        * The previous hidden state via the slow transition weights, W_h,
        * And the recent history of hidden state vectors via the fast weights, A.
    + The effect of the first two sources of information on the new hidden state can be computed once every time point, while the effect of fast weights involves a brief iterative process at each time step. 
    +  Assuming that the fast weights decay exponentially, the effect of the fast weights on the hidden vector during the iterative process is to provide an additional input.
        * This additional input is proportional to the sum over all recent hidden activity vectors weighted by the decay rate raised to the power of how long ago that hidden vector occurred.
    + The update rule for the fast memory weight matrix, A, is simply to multiply the current fast weights by a decay rate, λ, and add a proportion (learning rate) of the outer product of the hidden state vector, h(t).
    + The next vector of hidden activities, h(t + 1), is computed in two steps. The “preliminary” vector h0(t + 1) is computed like a normal LSTM, . The preliminary vector is then used to initiate an “inner loop” iterative process which runs for S steps and progressively changes the hidden state into h(t + 1) = h_S(t + 1), i.e. $$h_{s+1}(t + 1) = f(h0(t + 1) + A(t)h_s(t + 1))$$.

- attention comes at a cost. We need to calculate an attention value for each combination of input and output word. If you have a 50-word input sequence and generate a 50-word output sequence that would be 2500 attention values.
    + This can be solved by attending to both the input and output, the way that DRAW does. 
    + An alternative approach to attention is to use RL to predict an approximate location to focus to. That sounds a lot more like human attention, and that’s what’s done in Recurrent Models of Visual Attention.



#### Transformer network (Attention is all you need) 

- Transformer network replaces the sequential processing part of seq2seq networks (i.e. LSTM or CNN) with a **multihead self-attention** to solve the problem of relating two symbols from input/output sequences to a constant O(1) number of operations. It the  consists of two main parts:
    + Multihead attention
        * In terms of encoder-decoder, the query (Q) is usually the hidden state of the decoder. Values (V) are encoder hidden states that need to be attended to, and the keys (K) are learned parameters of the attention matrix that produce a distribution representing how much attention each value gets. Output is calculated as a wighted sum of values.
        * Multihead attention simply projects the Q, K, and V into a smaller embedding space $$d_v$$ using h=8 different linear mappings and applies the attention function there in parallel. Then concatenate the h=8 embedded attentions and map them back to the original dimension.
        * In self-attention, queries,keys and values that comes form same place i.e. the output of previous layer in encoder. In decoder, self-attention enables each position to attend to all previous positions in the decoder.
            - self-attention connects all positions with O(1) number of sequentially executed operations. 
            - The shorter the path between any combination of positions in the input and output sequences, the easier to learn long-range dependencies.
```python
def attention(Q, K, V):
    num = np.dot(Q, K.T)
    denum = np.sqrt(K.shape[0])
    return np.dot(softmax(num / denum), V)
```
    + Feed forward network
        * In RNN (LSTM), the notion of time step is encoded in the sequence as inputs/outputs flow one at a time. In FNN, the positional encoding must be preserved to represent the time in some way to preserve the positional encoding.
            - One way is to embed the absolute position of input elements (as in ConvS2S).
            - In case of the Transformer authors propose to encode time as sine wave, as an added extra input. Such signal is added to inputs and outputs to represent time passing.


```
Stage1_out = Embedding512 + TokenPositionEncoding512
Stage2_out = layer_normalization(multihead_attention(Stage1_out) + Stage1_out)
Stage3_out = layer_normalization(FFN(Stage2_out) + Stage2_out)

out_enc = Stage3_out
```

```
Stage1_out = OutputEmbedding512 + TokenPositionEncoding512

Stage2_Mask = masked_multihead_attention(Stage1_out)
Stage2_Norm1 = layer_normalization(Stage2_Mask) + Stage1_out
Stage2_Multi = multihead_attention(Stage2_Norm1 + out_enc) +  Stage2_Norm1
Stage2_Norm2 = layer_normalization(Stage2_Multi) + Stage2_Multi

Stage3_FNN = FNN(Stage2_Norm2)
Stage3_Norm = layer_normalization(Stage3_FNN) + Stage2_Norm2

out_dec = Stage3_Norm
```


### rnnVAE - Generating sentences from a continuous space

- Auto-encoders: Typically composed of two RNNs, The first RNN encodes a sentence into an intermediate vector, The second RNN decodes the intermediate representation back into a sentence, ideally the same as the input.
    + Regular auto-encoders learn only discrete mappings from point to point spanning the whole lower dimensional space. However, if we want to learn holistic information about the structure of sentences, we need to be able to fill sentence space better as a lower dimensional manifold thus VAEs are better. 
    + In a VAE, we replace the hidden vector z with a posterior probability distribution q(z|x) conditioned on the input, and sample our latent z from that distribution at each step. We ensure that this distribution has a tractable form by enforcing its similarity to a defined prior distribution, typically some form of Gaussian. Alternatively we can use an implicit distribution which is much more flexible and can better model the real posterior. 
    + Optimization problems: Decoder too strong, without any limitations just doesn’t use z at all – Fix: KL annealing – Fix: word dropout
        * Word dropout – Keep rate too low: sentence structure suffers – Keep rate too high: no creativity, stifles the variation
    + Used VAE to create language models on the Penn Tree-bank dataset, with RNNLM as baseline
        * Task: train an LM on the training set and have it designate the test set as highly probable. RNNLM outperformed the VAE in the traditional setting. However, when handicaps were imposed on both models (input-less decoder), the VAE was significantly better able to overcome them.
        * Task: infer missing words in a sentence given some known words (imputation).
            - Place the unknown words at the end of the sentence for the RNNLM – RNNLM and VAE performed beam search (VAE decoding broken into three steps) to produce the most likely words to complete a sentence
            - Precise evaluation of these results is computationally difficult
            - Instead, create an adversarial classifier, trained to distinguish real sentences from generated sentences, and score the model on how well it fools the adversary
            - Adversarial error is defined as the gap between chance accuracy (50%) and the real accuracy of the adversary.ideally this error will be minimized
            - Several other experiments in the appendix showed the VAE to be applicable to a variety of tasks – Text classification – Paraphrase detection – Question classification
        * Task: sample from the VAE, interpolate between sentences by exploring the latent dimension.


https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/

http://llcao.net/cu-deeplearning17/pp/class7_%20Elsbeth_Fei-Tzin.pdf




## pytorch techniques for NLP

### Batching variable length sequences:
- There are two ways of batching variable length sequences:
    + Packing sequences of same size together in a minibatch and sending that into the LSTM, but that's not always possible. 
    + Padding sequnces with zero so that all have the same maximum seq-len size. This can be done two ways:
        * Simply feeding the padded sequences in a minibatch and get a fixed length of output. The desired output should have different lengths since sequences had different lengths and RNN should have unrolled only for the real sequences, so we have to mask them manually.
            - mask = (time < length).float().unsqueeze(1).expand_as(h_next)
            - h_next = h_next*mask + hx[0]*(1 - mask)
            - c_next = c_next*mask + hx[1]*(1 - mask)
        * Using pytorch "pack_padded_sequence" which does a combination of packing and masking so that the output of each example will have different length. The RNN output then has to be unpacked using "pad_packed_sequence". 
            - Pad variable length sequences in a batch with zeros 
            - Sort the minibatch so that the longest sequence is at the beginning
            - pass the tensor and the lengths of sequences to "pack_padded_sequence"
            - the output of "pack_padded_sequence" goes to the RNN
            - RNN output is passed to "pad_packed_sequence" to map the output back to a zero-padded tensor corresponding to the right seq sizes.

- pack_padded_sequence removes padded zeros and packs data in a smaller tensor containing all contents of the minibatch. Instead, it makes the minibatch sizes variable for each time step. The RNN is still taking the maximum length number of steps, e.g. if the maximum sequence length is 35, the RNN will take 35 time steps for all the minibatches, but inside a minibatch,  there are different length samples. So pack_padded_sequence adapts the batch_size inside each minibatch for each time step to accomodate different length samples. 
    + For example, if maximum seq_legth is 6 and batch_size is 4, then each minibatch is a 6x4 tensor with zeros for shorter sequences than 6.
    + Imagine a minibatch with sample_lengths of 6, 5, 4, 3. The RNN needs to unroll for 6 time steps.  The first time step includes 4 samples, the second time step also 4 and so on i.e. [4, 4, 4, 3, 2, 1]. This means that all four words of each sequence will be fed into the LSTM at timestep 1. Then another 4 until the shorted sequence which was length 3 is exhausted. We then go on with 3 , 2, and then only the one word for the longest sequence of length 6.

### Pytorch Dataset class:
Pytorch has a dataset class that provides some tools for easy loading of data. 
    - **Dataset class** your dataset class should inherit **"torch.utils.data.dataset"**.  At this point the data is not loaded on memory. Several methods need to be implemented:
        + __init__(self) load and preprocess the data here or in __getitem__ for memory efficiency.
        + __len__(self) returns the size of the dataset.
        + __getitem__(self) indexes into dataset such that dataset[i] returns i-th sample.
    - **"torch.utils.data.DataLoader"** This class is used to get data in batches from the dataset class and provides an iterator to go through the data. It can shuffle and minibatch the data and load into memory. It has a default **collate_fn** that tries to convert the batch of data into a tensor but we can specify how exactly the samples need to be batched by implementing **collate_fn**. Usage in training is as simple as instantiating the Dataloader class with the dataset instance and a for loop with **enumerate(dataloader)**.
    - **DataParallel**: Data Parallelism is when we split the mini-batch of samples into multiple smaller mini-batches and run the computation for each of the smaller mini-batches in parallel. One can simply wrap a model module in DataParallel and it will be parallelized over multiple GPUs in the batch dimension.

### using Tensorboard with pytorch or python in general
It's actually very easy to use tensorboard anywhere with python. 
```python
from tensorboard_logger import Logger as tfLogger
logdir = '/tmp/tb_files'
tflogger = tfLogger(logdir)
tflogger.scalar_summary('Training Accuracy', train_acc, epoch)
```

then run tensorboard server on your remote machine (similar to jupyter server)
```bash
tensorboard --logdir=/tmp/tb_files  --port=8008
```

And connect to it by tunneling the port to your local machine. Run is on your local machine and then visin tensorboard in browser at [](http://localhost:7003/)

```bash
ssh -A -N -f -L localhost:7003:localhost:8008 -J skynet hamid@compute006
```

### Getting a profile of the timing of the code

Profile your code with `sProfile` like the following command in order to understand which part of the code is taking how much time. 

```bash
python -m cProfile -o train_dialog_coherence.prof train_dialog_coherence.py --cuda --batch_size=16 --do_log --load_models --remove_old_run --freeze_infersent
```

After profiling, you can visualize it using `snakeviz`. But to run it on a remote machine, you need to deactivate running of browser and pipe the port to your local machine so that you can access the visulization in your local browser. On your remote machine do:

```bash
snakeviz train_dialog_coherence.prof -s --port=1539
```

to forward the port, make a pipe on your local:

```bash
ssh -A -N -f -L localhost:$local_host:localhost:$r_port -J skynet hamid@$remote_host
```


# NLG (Natural Language Generation)
NLG is the process of generating natural language from a data representation data2text, or from another textual representation text2text. It involves 6 main tasks that have traditionally been done in seperate phases:

1. Content determination: Deciding which information to include in the text under construction.
2. Text structuring: Determining in which order information will be presented in the text.
3. Sentence aggregation: Deciding which information to present in individual sentences.
4. Lexicalisation: Finding the right words and phrases to express information,
5. Referring expression generation: Selecting the words and phrases to identify domain objects.
6. Linguistic realisation: Combining all words and phrases into well-formed sentences.


- Levels of natural language generation
    + 1. simple fill in the blank systems (templates)
    + 2. integrating templates inside a scripting language with programming constructs (if, for, etc). Examples like jekyll markdown.
    + 3. Adding word-level grammatical functions to level 2 to make it easy to generate gramatically correct text and reduce the need for complex rules.  Deals with things like:
        * morphology (e.g. child/children),
        * morphophonology (a/an), 
        * orthography (one '.' instead of two '..' at the end of sentence "I like Washington D.C.")
    + 4. Dynamically create sentences (and perhaps paragraphs) from representations of the meaning to be conveyed by the sentence and/or its desired linguistic structure. 
        * Such systems can do sensible things in unusual (edge) cases, without needing the developer to explicitly write code for every edge case.
        * It also allows the system to linguistically “optimise” sentences in a number of ways, including reference, aggregration, ordering, and connectives.
        * For example, producing John was hungry, so he ate an apple.  He was also cold.   instead of   John was hungry.  John was cold.  John ate an apple.
        * These do good job of “micro-level” writing.  But more is needed to do an excellent job at “macro-level” writing
    + 5. add intelligence to the “macro-writing” task, that is to the task of producing a document which is relevant and useful to its readers, and also well-structured (for example as a narrative).
        * How this is done depends on the goal of the text.
        * For example, a text that is intended to be persuasive may be based on models of argumentation and behaviour change; 
        * while a text that summarises data for decision support may be based on an analysis of key factors that influence the decision, plus models of narrative and human decision-making.


## consistent and coherent conditional text generation
It's a language models with a decoding objective that is the sum of language model objective and the weighted (learned weights) sum of scores from various discriminative models that different factors(such as repetition, coherence, etc).
- Repetition: a classifier is trained to distinguish the ground truth sentence and a generated sentence. More specifically, a score is computed for each position in the sentence $$y$$ based on pairwise cosine similarity between word embeddings within a fixed window of the previous $$k$$ words. If the word $$y_i$$ is repeated in the window, the score will be higher. 
- Contradiction: train a classifier that takes two sentences a and b and predicts the relation between them as either contradiction, entailment or neutral. use the neutral class probability of the sentence pair as discriminator score, in order to discourage both contradiction and entailment. The classifier is trained on two large entailment datasets, SNLI and MultiNLI. every completed sentence in the generated text is scored against all preceding sentences in both the context and the generated text.
- Relevance: train the model to distinguish between true text and random text sampled from other (human-written) endings in the corpus, conditioned on the given context.
- Lexical diversity: discriminator based on observed lexical distributions which captures writing style as expressed through word choice.  trained with a ranking loss using negative examples sampled from the language model.


## Surface realization:
Surface realization task is the task of translating a tree (or alternative) representation of language to a sentence. Two common tree representation of language are deep trees and shallow trees:
- deep tree is intended to be an abstract representation of the meaning of a sentence. Unlike semantic input, where the nodes are semantic representations of input, deep input is more surface centric, with lemas for each word being connected by semantic labels. 
- A shallow syntactic trees, is similar to a deep tree with the differene that it includes function words as well

Deep inputs can more commonly occur as input of NLG systems where entities and content words are available, and one has to generate a grammatical sentence using them with only provision for inflections of words and introduction of function words. Such usecases include summarization, dialog generation etc.

Traditionally, the pipeline for language generation (surface relalization) has involved a few seperate tasks i.e.  predicting the correct word order, then deciding inflections (e.g. verb tense) and also filling in function words at the appropriate positions. 

Relevant work in 3 categories:
-  abstract word ordering: 
    +  De Gispert et al. (2014) compose phrases over individual words and permute the phrases to achieve linearization (generate sentence). Schmaltz et al. (2016) show that strong surface-level language models are more effective than models trained with syntactic information for the task of linearization.
-  applications of meaning-text theory
    +  Belz et al. (2011) organized a shared task on both shallow and deep linearization according to meaning-text theory,
    +  Song et al. (2014) achieved the best results for the task of shallow-syntactic linearization. 
    +   Using SVM models with rich features, Bohnet et al. (2011) achieved state-of-art results on the task of deep realization.
-  joint modelling of NLP tasks:
    +  Joint models have been proposed for word segmentation and POS-tagging (Zhang and Clark,2010), POS-tagging and syntactic chunking (Sutton et al., 2007), segmentation and normalization (Qian et al., 2015), syntactic linearization and morphologization (Song et al., 2014), parsing and NER (Finkel and Manning, 2009), entity and relation extraction (Li and Ji, 2014) 
    +  (Puduppully et al 2017) propose a joint model for deep realization, integrating linearization, function word prediction and morphological generation.
        *  i.e. a model that jointly performs: 1. prediction of function words, to form a shallow graph; 2. linearizing the shallow graph; 3. generating the inflection for each lemma in the string.
            - A classifier predicts the functions words of "to" for infinitives, "that" for complementing, and comma "," from the following set of features: word at each node W(n), its part of speech tag POS(n), and the children words W(c). They use a averaged perceptron classifier (a single layer NN with step function nonlinearity)
            - Transition-Based Tree Linearization: Uses stack and queue data structures and a hard-coded algorithm to represent a tree. They then form features set using another hard-coded algo to represent possible set of actions on each node. They use beam search on each node to narrow down the set of possible actions (beam search is a heuristic search algorithm that explores a graph by expanding the most promising node in a limited set). Then they again use averaged perceptron classifier to learn a function that assigns a score to each action.
            - After ordering the words, the last step is assign the right inflation to the lemmatized words. There are three POS categories (noun, verb and articles). They write a set of rules to find a set of candidate inflections from each lemma (they use wiktionary website for data) and then train an averaged perceptron classifier for each lemma with a set of features (like ngrams).
            - They combine these steps in a joint learning structure.


Approaches to solving the Surface Realization shared task 2018:
* linearize input tree as a long sequence
    - the sequence can be very long (upto 10*30/40 seq_len)
    - the hierarchical information will be lost and encoder has to learn it.
* multi-input approach where each column is a seperate input
    - this might require 10 seperate encoders for each of the column inputs
    - many params and not enough data?
* Conditional VAE approach where words are the input, and other columns of the table (tree) are categorical labels.
    - 10 categorical labels as condition in the VAE
        + pre-train decoder as a language model (possibly with external data)
        + pre-train encoder as a multi-task classifier that classifies target sentences with 10 categorical labels. For each word in the sentence, it should preform POS tagging, Sing/Plural classification, etc.
        + The encoder should be pre-trained as a parse-tree translator for a sentence. May be trained on external data
        + The encoder-decoder seq2seq model will need to be trained together at the last stage. 
    - Since labels have much smaller number of categories, it might make sense to make them one-hot categorical representation (orthogonal vectors). 
    - On the other hand, one-hot representation of labels might lead to information about closeness of labels to be lost (i.e. articles (a/an) are not orthogonal with nouns since they almost always appear together).
* Conditional seq2seq: the words are the input, and other columns of the table (tree) input are categorical labels.
    - learn a classifier with 10 categorical outputs as a parser and use it as the encoder in a seq2seq model.
        + pre-train an encoder as a multi-task classifier for each word (i.e. predict POS tag, Sing/Plural, etc). Possibility to train on external data
        + Since labels have much smaller number of categories, it might make sense to make them one-hot categorical representation (orthogonal vectors).
        + On the other hand, one-hot representation of labels might lead to information about closeness of labels to be lost (i.e. articles (a/an) are not orthogonal with nouns since they almost always appear together).
    - pre-train a decoder as a language model (possibly with external data)
    - fine-tune train the encoder-decoder model together at the last stage with tree inputs and target sentence outputs. 

* A hierarchy of a word-level LM and then a character-level LM. 
    - The word-level LM generates concepts and meanings to convey
    - character-level LM generates the language specific translation conditioned on the concept (word) input and past generations.
        + The character-level LM is trained with char2vec to learn the manifold of words in the language. 




- Universal dependency tree columns (conLL format)
    + col1, word idx
    + col2, lemmatized word
    + col3, POS tag
    + col4, old style PTB POS tag annotation
    + col5, morphological features
    + col6, head word in tree structure
    + col7, dependency relationship with head word
    + col8, other relationships in the tree


# Text and GANs
+ Adversarially regularized autoencoder is a AAE with recurrent encoder/decoder and with WGAN loss (https://arxiv.org/abs/1706.04223)
+ MASKGAN: better text generation by filling in the blank 
    * teacher forcing is a common trick (not max likelihood, in training, feed the true label to the next step for generating)
    * professor forcing (not Max likelihood, )
    * you mask some inputs words, the goal of generator is to fill in the blank conditioned on the mask.
        - solves the problem of seqGAN that Reinforce doesn't know where in the seq the generator has made a mistake to penalize it using a condition mask. 
        - trained with actor-critic.
        - pretrain seq2seq. pretrain on infilling. then use GAN. 

# Information extraction:
information extraction (IE), turns the unstructured information embedded in texts into structured data, for example for populating a relational database to enable further processing. As in most NLP tasks, we start by structuring the input, identifying various levels of constituents and relations, and then state the patterns in terms of these constituents and relations.

1. The first step in most IE tasks is to find the proper names or named entities mentioned in a text. The task of **named entity recognition (NER)** is to find each  mention of a named entity in the text and label its type. What constitutes a named entity type is application specific; examples: people, places, and organizations but also more specific entities from the names of genes and proteins (Cohen and Demner-Fushman, 2014) to the names of college courses (McCallum, 2005).

2. Having located all of the mentions of named entities in a text, it is useful to link, or cluster, these mentions into sets that correspond to the entities behind the mentions (i.e. **co-reference resolution**), for example inferring that mentions of United Airlines and United in the sample text refer to the same real-world entity.

3. The task of **relation extraction** is to find and classify semantic relations among entities, often binary relations like spouse-of, child-of, employment, partwhole, membership, and geospatial relations. Relation extraction has close links to populating a relational database

4. The task of **event extraction** is to find events in which these entities participate, like the fare increases by United and American and the reporting
events said and cite. We’ll also need to perform event co-reference to figure out which of the many event mentions in a text refer to the same event; in our running example the two instances of increase and the phrase the move all refer to the same event. 

5. To figure out when the events in a text happened we’ll do **recognition of temporal expressions** like days of the week (Friday and Thursday), months, holidays, etc., relative expressions like two days from now or next year and times such as 3:30 P.M. or noon. The problem of **temporal expression normalization** is to map these temporal expressions onto specific calendar dates or times of day to situate events in time.

6. Many texts describe recurring stereotypical situations. The task of **template filling** is to find such situations in documents and fill the template slots with appropriate material. These slot-fillers may consist of text segments extracted directly from the text, or concepts like times, amounts, or ontology entities that have been inferred from text elements through additional processing.

    + Lexical analysis, assigning POS tags and features to words and idiomtic phrases through morphological analysis and dictionary lookup. 
    + Named entity recognition, identifying names and other special lexical structures, such as dates, currency expressions, etc. 
    + Syntactic parsing, to identify noun groups, verb groups, and possibly head complement structures (SyntaxNet, Parsy McParseFace, and allenNLP parser). 
    + Defining event templates - A template defines a specific type of event (e.g., a bombing) with a set of semantic roles (or slots) to be filled for the typical entities involved in such an event (e.g., perpetrator, target, instrument).
        * May involve Trigger classification which decides whether a word is an event trigger and, if so, its event type.
        * If a trigger has been identified, an argument classifier is then applied to a pair consisting of the trigger and an entity in the same sentence. Again, this can be done in a single stage or a binary classifier (argument / non-argument) can be followed by a classifier for role assignment. 
    + Task-specific pattern matching to identify facts of interest. 
    + Integration (i.e. discourse analysis), to examine and combine facts from the entire document. 
        * co-reference resolution, to find multiple references to the same event. 
        * draw inferences from explicitly stated facts. 
    + an additional step of template generation may be involved to do NLG.

-  Information extraction has traditionally been done using symbol matching using linguistic annotation, structured world knowledge and semantic parsing.
    + frame-semantic parsing method
    + word distance benchmark method


### Frames
Identifying frames is an explicit or implicit prerequisite for many NLP tasks. 
    + Information extraction, for example, stipulates the types of events and slots that are extracted for a frame or template. 
    + Dialogue systems and personal-assistant applications also model users’ goals and subgoals using frame-like representations,
    + In natural-language generation, frames are often used to represent content to be expressed as well as to support surface realization.

Frames have traditionally been manually defined. But there is interest in automatically inducing frames from text. 
    + Chambers and Jurafsky (2011), first clusters related verbs to form frames, and then clusters the verbs’ syntactic arguments to identify slots. 
        * The clustering uses a customized similarity metrics, as well as an additional retrieval step from a large external text corpus for slot generation.
    + Cheung et al (2013) introduce a probabilistic approach to frame induction. They define a joint distribution over the words in a document and their frame assignments.
        * They do that by modeling frame and event transition, correlations among events and slots, and their surface realizations
        * Given a corpus, the model outputs a set of induced frames and their learned parameters. 
        * The numbers of events and slots are dynamically determined by a novel application of the split-merge approach from syntactic parsing (Petrov et al., 2006).

Two sets of manual work have traditionally been involved in IE. 
    +  First, the target representation (template) is defined manually by domain experts. 
        * distant supervision (Mintz et al., 2009) method combines the advantages of bootstrapping with supervised learning. Instead of just a handful of seeds, distant supervision uses a large database to acquire a huge number of seed examples, creates lots of noisy pattern features from all these examples and then combines them in a supervised classifier.
            - In the seed-based approach, we might have only 5 examples to start with. But Wikipedia-based databases like DBPedia or Freebase have tens of thousands of examples of many relations.
            
            - annotate the docs with slots (Ruihong Huang)
                - sample: 1000s 
                - heuristics and bootstrap 
                - sentence highlighting. 

    +  Then, manual effort is required to construct an extractor (or annotate examples to train a machine-learning system).
        * A popular approach to reduce annotation effort is bootstrapping from seed examples (Patwardhan and Riloff, 2007; Huang and Riloff, 2012).
            - For example, that we need to create a list of airline/hub pairs, and we know only that Ryanair has a hub at Charleroi. We can use this seed fact to discover new patterns by finding other mentions of this relation in our corpus. Then we can use those patterns for finding relations between e.g. airport/location pairs, and so on. Bootstrapping systems also assign confidence values to new tuples to avoid semantic drift (an erroneous pattern leading to the introduction of erroneous tuples).  basic algo:

            tuples←Gather a set of seed tuples that have relation R 
            iterate
                sentences←find sentences that contain entities in seeds
                patterns←generalize the context between and around entities in sentences
                newpairs←use patterns to grep for more tuples
                newpairs←newpairs with high confidence
                tuples←tuples + newpairs
            return tuples

        * OpenIE (Banko and Etzioni, 2008) reduces the manual effort to designing a few domain-independent relation patterns, which can then be applied to extract relational triples from text. 
            -  Open IE is to extract relations when we have no labeled training data, and not even any list of relations. OpenIE are simply strings of words (usually beginning with a verb). For example, ReVerb system (Fader et al., 2011) extracts a relation from a sentence s in 4 steps and accepts a relation if it meets syntactic and lexical constraints:
                +  Run a part-of-speech tagger and entity chunker over s
                +  For each verb in s, find the longest sequence of words w that start with a verb and satisfy syntactic and lexical constraints, merging adjacent matches.
                +  For each phrase w, find the nearest noun phrase x to the left which is not a relative pronoun, wh-word or existential “there”. Find the nearest noun phrase y to the right.
                +  Assign confidence c to the relation r = (x,w, y) using a confidence classifier and return it.


        * Unsupervised work has been done on unsupervised semantic induction, such as unsupervised semantic parsing (Poon and Domingos, 2009), unsupervised semantical role labeling (Swier and Stevenson, 2004) and induction (Lang and Lapata, 2011, e.g.), and slot induction from web search logs (Cheung and Li, 2012).
        





