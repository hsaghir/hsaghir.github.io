---
layout: article
title: NLP
comments: true
categories: data_science
image:
  teaser: jupyter-main-logo.svg
---

## Representation
- Natural language consists of discrete units of letters, words, and sentences. In order to be able to perform machine learning on natural language, we have to first represent it in a numerical format. The central problem in NLP seems to be representation of natural language as numbers. After than the problem is simply sequence and structure learning where RNN-like models shine. 

- Basic representation started by defining a dictionary as the list of all allowed words or letter combinations. Common representations are:
    + one-hot encoding: a vector with the size of the dictionary where all entries are zero except the single entry corresponding to the word in a location
    + Bag of words: average of one-hot encodings for a document representing the number of word counts 
    + word2Vec encoding: 

- Obviously the number of words in a language is much smaller than the combination of the letters in that language. Why not learn a language-specific word manifold that maps characters to words? Then we can use the char-level language models instead of word-level models and reduce the dimensionality of the problem since number of chars are multiple orders of magnitude smaller than words. 
