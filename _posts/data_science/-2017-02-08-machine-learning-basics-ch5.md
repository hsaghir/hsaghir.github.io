---
layout: article
title: Numerical Computation Deep Learning book Ch4
comments: true
categories: data_science
image:
  teaser: jupyter-main-logo.svg
---

- Statistics focuses on finding causal relationships or estimating confidence intervals of functions (usually simple / linear models). Machine learning focuses more on complex function estimation and prediction. Bayesian ML focuses on complex function estimations along with confidence estimations.  

- Learning is defined as using data points to improve on a task as measured by a performance measure. Designing a performance measure (loss function) is an art and case specific. Supervised Learning [Classification, Regression, Structured Output prediction] involves conditional probability estimation of outputs from inputs P(Y/X). Unsipervised learning [e.g. generative modelling, denoising, imputation, clustering, etc] involves a Density estimation i.e. learning P(X). In a modelling effort we might introduce model variables Z and try to do a density estimation on P(X,Z) instead. This involves finding a function from (N dimensional) data space to a probability surface in (N+1 dimension). Semi-Supervised learning is a combination of supervised and unsupervised data while multi-instance learning is the case where individual samples don't have a label but we know a certain type of sample exists among a group.

- Using the chain rule of probabilities we can transform an unsupervised learning problem to many supervised learning problems P(X)=P(x_i/x_1, x_2, ..). In the same sense, we can solve a supervised learning problem using the Bayes theorem by estimating P(Y/X)=P(X,Y)/P(X). 

- A generalized linear model (GLM) can be defined as $Y_out=WX$ where X is input data and w are parameters. Setting the gradient of quadratic cost for this model to zero to find optimum parameters that minimize quadratic cost will return least square solution analytically as $W= ((X X^T)^-1)(X^T)(Y_target)$ called normal equation. We can make polynomial or arbitrary nonlinear models of X by applying arbitrary nonlinear functions to X and extending the input matrix and corresponding number of variables W. However, since the model is still linear wrt to parameters W, it's still a linear model (GLM) and the least square solution is given by normal equation although it can model nonlinearity ! We might want to add L1 or L2 regularization terms to the squared error in which case we find parameters by stochastic gradient descent instead of normal equation.

- In machine Learning we minimize training error but we actually care about minimizing test error that we can't directly access. The least amount of error possible incurred by a perfect model that knows the data generation distribution is called Bayes error. If the data generating distribution for the training and test sets are different, we will have high error rate for our test set and we can't do much but to get more data that is more similar in data generation distribution to our test set. 

However, If we assume that both train and test sets are iid samples (independantly sampled from same distribution) of one data generating distribution, then the expectation of error for a fixed model would be exactly the same for both train and test samples. However, in the learning process, we sample the data generating distribution (batch) and modify model parameters to reduce training error. Therefore, the expectation of the error for samples drawn for training would be smaller (or equal) than the expectation of error for samples drawn for test (no learning). 

- A well-performing machine learning algorithm will have 1) low training error (bias) and 2) small gap between train and test errors (variance). These two correspond to model capacity, a simple model might underfit and have large bias while a complex model might overfit and have large variance. The recipe is that we usually increase model complexity if the bias is high to better fit the data. If the variance is high, we try applying regularization techniques to reduce overfitting. 

- The no free lunch theorem says if we average over all possible data generation distributions, all machine learning algorithms will have the same test error rate. This means that there is no best universal algorithm for all data sets (data generation distributions). We should therefore focus on finding data generation distributions we encounter in the real world, and the machine learning algorithms that work well for them. 

- Hyperparameters are parameters that are set but not learned e.g. learning rate. We use therefore, further divide the training set into training and validation sets to be able to set hyperparameters for the learning algorithm for example adaptive gradient descent (adagrad). Dividing the data to fixed test and training doesn't give much certainty about generalization if test set is small so we do k-fold cross validation (divide data to k segments and repeat learning k times keeping a different segment for test each time) to get mean and variance of generalization error.

- Point estimate finds single set of values for model parameters. For example, sample mean/std are point estimates for population mean/std under a Gaussian model. Point estimate from samples (as opposed to population) have error which are quantified by bias and variance of the estimator. Bias is $bias(param)=(E[sample param] - population param)$. For example, if we assume a Gaussian distribution model, sample mean estimator is unbiased estimate of real mean but sample variance estimator underestimates population variance. Consistency is when the bias converges to zero with increasing amount of data. 

- Variance of a point estimator is simply the variance of the estimate over data. Its square root is called standard error which provides a measure of how much variation in our point estimate we expect as we calculate the estimate on a different sample. Standard error for sample mean point estimator is $SE(sample mean)= (population std) / sqrt(number of samples)$. We don't know population std so we use sample std which we know is still biased, but if the number of samples is large it's reasonable. According to central limit theorem, which says the mean will be approximately distributed with a normal distribution, we can calculate 95% confidence interval using mean and standard error (95%CI= mean +- 1.96*SE).

- There is a trade-off between bias and variance of an estimator, therefore, we might use a biased point estimator in cases where we are interested in low estimator variance. Cross-validation helps with deciding about the tradeoff. An example of where this concept might be useful is batch normalization in deep nets. The statistics of each batch is slightly different from another and therefore as it propagates through a deep net, it changes drastically. This makes learning difficult and batch normalization solves it by normalizing each batch at each layer using sample mean and variance point estimates (low estimator variance is desired in training). However, since sample variance is biased, in testing where we care about low bias in our prediction we use the unbiased sample variance point estimator instead.

- Maximum likelihood principle is a popular point estimator which states that model parameters should be chosen that maximize the probability of data. Assuming independant samples, that's $max(\prod p_model(x_i))$. We take log to convert the product to sum and help with numerical underflow i.e. max [sum(log p_model(x_i))]. By dividing by constant m number of data, we can write the sum as the expectation of model distribution $max(E_{p_em}[log p_model(x)])$ over emprical distribution of data p_em(x). This way we can interpret maximum likelihood as minimizing the KL divergence between the model distribution p_model(x) and the emprical distribution p_em(x) i.e. KL(p_model/p_em)=E_{p_em}[log p_em(x) - log p_model(x)]. Any loss function consisting of a negative log model probability (likelihood) is a cross-entropy between the model likelihood and emprical distribution defined by the training set. Maximum likelihood is the best estimator assymptotically as the number of data tends to infinity if the true data distribution is in the model distribution family.

- In the supervised learning case, we write p_model(y/x) and follow the same procedure for maximum likelihood estimation. For example, in linear regression we assume a Gaussian model $p(y/x)=N(wx, \sigma^2)$ with a fixed variance and a linear function of mean. Performing maximum liklihood on this model results in optimizing mean squared error. When the number of examples is small in the model, we will overfit and have a large gap between train (sample) and test (population) errors (large variance). Therefore, we trade off a biased version of maximum likelihood (i.e. add regularization) for less variance in the model. 

- As opposed to point estimates for model parameters in frequentist stats, Bayesian approach views model parameters as random variables instead of data points. Therefore, it assumes a distribution for model parameters which is first defined by priors (usually a high entropy distribution like uniform or Gaussian), then data are plugged in and decrease the entropy by concentrating around likely values which establish the posterior belief. The Bayesian approach has two important difference; 1) every prediction will also be a probability distribution (belief) reflecting the model uncertainty about it (robust against overfitting) 2) Another difference is in the application of priors which shift the starting probability space to regions prefered in the beginning. In practice priors are often used to convey preference for models that are simple or smooth. 

- As an example, consider Bayesian linear regression. We consider multivariate Gaussian model $p(y/x)=N(wx, I)$. Assuming an iid Gaussian prior on weights, we can use the Bayesian formula to do Bayesian inference. Assuming a zero mean and alpha scaling for covariance on priors, we get the same result as maximum likelihood linear regression with a regularization of {alpha * w^Tw}. 

- MAP in an alternative point estimate to maximum likelihood that allows priors to influence parameters choice. MAP estimate states that one should choose parameters that maximize the posterior i.e. $max(p(param/x)=p(x/param)p(x))$. Applying log to convert product to sum and prevent underflow we get $max(log p(x/param)+ log p(x))$. The first term is the maximum likelihood term while the second term corresponds to prior distribution. MAP with a Gaussian prior on weights corresponds to maximum likelihood with weight decay regularization.

- We can perform classification using the linear regression approach by using a logistic sigmoid on the output of linear regression to squash it into [0,1] interval which can be interpreted as probability of either class 0 or 1. Optimizing the maximum likelihood (negative log likelihood) loss is done via SGD.

- Linear support vector machines are linear classifiers similar to logistic regression but use their wx+b as a hyperplane separating the two class instead of probabilities. SVMs try to find the hyperplane with the maximum seperation between classes (i.e. maximum k where wx+b>k for class 1 and wx+b<-k for class -1). Another way to think about this is to put a bubble around each data point in data space; as we increase the radius of these bubbles, the set of possible solutions that don't violate the data bubbles and seperate the two classes decreases until a single solution is found which is the optimum solution with maximum bubble radius possible. Note that in this analogy, only the data points from the two classes that are close to each other will determine the decision boundry. Other points are simply having fun with their bubbles far away from the hyperplane. These critical points are called support vectors. 

- A breakthrough in statistical theory of machine learning by Vapnik and Chernovsky (VC) was the finding that the test error is bounded by the training error plus a monotonicly increasing funtion of a quantity called VC dimension. VC dimension is the minimum of data dimensionality and a ratio of bubbles {a bubble around all data over the bubble of support vectors}. This is extremely important since if we can reduce the bubble ratio, we can get around the curse of dimensionality in data! Therefore, there are two ways in which we can minimize the ratio. 1) is to keep the numerator fixed by constraining the scale of the scale of the data to be one and the maximize the margine. 2) is to keep the denominator fixed by imposing the constraint that margin be one, [therefore the the output of the model (wx+b) would be bigger than one for class one and smaller than -1 for class -1; if d is labels +1 or -1 then $d(wx+b)>1$], and minimize the data coefficients (w) to best rescale the data bubble. So the optimization problem accordint to (2) would be $min: L=1/2 w^Tw subject to d(wx+b)>1$. This can be rewritten using lagrange multipliers as $min: J(w, b, \alpha)= 1/2 w^Tw - \sum(\alpha_i d_i (w^Tx_i+b)) + \sum(\alpha_i))$ where $\alpha_i$ are lagrange multipliers for data samples $x_i$. Imagine a horse seat; This objective has to be minimized wrt (w, b) to get to minimum of the seat and maximized wrt $\alpha$ to get to the top of the seat (saddle point). 

- By taking the partial derivative of J wrt [w,b] and equating to zero, we can get the optimal $w=\sum(\alpha_i d_i x_i)$. We also know from KKT optimization (ch4 DL book) for non-equality constraints that $\alpha_i [d_i (w^Tx_i+b)-1]=0$ which will give us b. This constraint also shows that for non support vectors the $\alpha$ is zero and therefore, in linear SVM, the boundary is a linear combination of support vectors. Replacing the optimum [w,b] into the cost function J will yield a convext cost in terms of only $\alpha$, which we can solve using a QP solver. 

- A key innovation in machine learning is the kernel trick which is based on the observation that most machine learning algorithms can be written in terms of dot product between examples. For example in linear SVM shown above, by replacing w with it's optimal value we can rewrite $ w^T x+b = b + \sum(\alpha_i x^T x_i) $. This makes it possible to just transform the example space, x, by some nonlinear function as a pre-processing before passing it through a linear SVM. The kernel trick introduces a kernel function $k(x,x_i)=\phi(x).\phi(x_i)$ which is similar to performing dot product on transformed version of input space x. The added value is that in most cases the nonlinear transformation $\phi(.)$ might be intractable, but we can evaluate the kernel k more efficiently. Additionally, since the $w^T x_prime +b$ on the transformed data space is still linear in coefficient w, we get a convex optimization problem with convergence gaurantee!

- The most common kernel is the Gaussian kernel $k(u,v)=N(u-v, \sigma^2 I)$, also known as radial basis function which corresponds to a dot product in infinite dimensional space! A drawback of kernel methods is that the cost of evaluating the decision boundary is high because the boundary is a linear combination of kernelled version of all examples. SVMs are a little better in this regard, since to classify a new example, they only need to evaluate the kernel for support vectors (other examples don't contribute to the decision boundary).

- Another type of supervised (and unsupervised) learning algorithms is K-nearest neighbors which basically doesn't do any learning. It just remembers all the points and interpolates between the opinion of k-nearest neighbors of each new point at test time. However, it can't understand patterns in a certain feature of the data. For example, if only one feature of a 10 dimensional input is relevant, it still will find the nearest neighbors based on all 10 dimensions. Therefore, it's not very well suited to small datasets. 

- Decision trees hierarchically partition the input space with partitions that are alligned along the input feature axes. For example, if we have 3d data, the first level nodes might partition the input space along x1 and x2 while the second level nodes partition along all three axes. Their problem is that if the decision boundary is not axis alligned (e.g. a linear non-perpendicular boundary), they will have problem and will have to do many partitions to zig-zag around the line! Their learning can be non-parametric if the tree size is not constrained but with regularization, they will have a finite set of parameters. 

- An unsupervised learning task is finding simpler representations by finding lower-dimensional, sparse, or independant representations. For example, PCA transforms the data into a representation that linearly disentangles unknown factors of variation underlying the data (each eigenvector is linearly uncorrelated with anotherss). We are also interested in represenations that disentangle nonlinear dependencies.

- Another unsupervised learning task is clustering where we assign every example to a cluster. K-means for example performs two step optimization to do clustering by first assigning the examples to random clusters based on their distance and in the second step refine the clusters by calculating the centroids of those clusters. This continues until convergence (EM algo). We might use clusters with a one-hot vector as a representation but that's not the best distributed representation since that doesn't convey enough information about similarity of two representations. 

- Gradient descent needs the expectation (sum) of gradients for all data points at every update which is costly O(m). Stochastic Gradient Descent (SGD) notes that the expectation can be approximated by a sample (minibatch) from the dataset O(1). 

- Prior to deep learning the main way of learning nonlinear models was to use the kernel trick with a linear model. This required constructing an mxm matrix K(x_i,x_j) which is not scalable O(m^2); but deep learning with SGD scales very well (amorized cost of O(1)).

- Remember that all machine learning algorithms consist of several parts; i.e. dataset, model, cost function, and optimization algo that are relatively independant of each other and can be combined to make new machine learning algorithms. 

- There are some problems with traditional machine learning that DL tries to solve. 1) Curse of dimensionality which means as the number of dimensions increase, the possible data configurations increase exponentially. Therefore, we encounter an statistical challenge as the number of examples in dataset would be much less than possible configs in high dimensions. 2) Another problem is the smoothness assumption of most traditional machine learning algos. This works well in low dimensions and when test data is in the same region as the training data where an interpolation can provide correct answer but face problem in cases of high dimensional data and when test data comes from another region of the input space. For example a checker board with alternating black and white color can be correctly predicted with traditional machine learning if the test data comes from the region where we have seen examples due to smoothness prior. However, extrapolation can not be done well. Deep learning solves these two problems by a different assumption. DL assumes that the data was generated by a composition of factors at multiple levels of a hierarchy. This assumption allows composing features at multiple scales and therefore, representing an exponential number of regions with not as many examples.

- A manifold is a set of connected points with lower dimension embedded in a higher dimensional space where each dimension corresponds to a local direction of variation (e.g. roads are 1d manifolds in 3d space). Usually one can traverse a manifold along these directions using transformations. There is evidence for existance of manifolds in AI tasks is that the probability distribution for many real images, sounds, etc are highly concentrated. Finding lower dimensional coordinate systems inside manifolds is desirable to be able to traverse them. 


