---
layout: article
title: Numerical Computation Deep Learning book Ch4
comments: true
categories: data_science
image:
  teaser: jupyter-main-logo.svg
---

- Statistics focuses on finding causal relationships or estimating confidence intervals of functions (usually simple / linear models). Machine learning focuses more on complex function estimation and prediction. Bayesian ML focuses on complex function estimations along with confidence estimations.  

- Learning is defined as using data points to improve on a task as measured by a performance measure. Designing a performance measure (loss function) is an art and case specific. Supervised Learning [Classification, Regression, Structured Output prediction] involves conditional probability estimation of outputs from inputs P(Y/X). Unsipervised learning [e.g. generative modelling, denoising, imputation, clustering, etc] involves a Density estimation i.e. learning P(X). In a modelling effort we might introduce model variables Z and try to do a density estimation on P(X,Z) instead. This involves finding a function from (N dimensional) data space to a probability surface in (N+1 dimension). Semi-Supervised learning is a combination of supervised and unsupervised data while multi-instance learning is the case where individual samples don't have a label but we know a certain type of sample exists among a group.

- Using the chain rule of probabilities we can transform an unsupervised learning problem to many supervised learning problems P(X)=P(x_i/x_1, x_2, ..). In the same sense, we can solve a supervised learning problem using the Bayes theorem by estimating P(Y/X)=P(X,Y)/P(X). 

- A generalized linear model (GLM) can be defined as $Y_out=WX$ where X is input data and w are parameters. Setting the gradient of quadratic cost for this model to zero to find optimum parameters that minimize quadratic cost will return least square solution analytically as $W= ((X X^T)^-1)(X^T)(Y_target)$ called normal equation. We can make polynomial or arbitrary nonlinear models of X by applying arbitrary nonlinear functions to X and extending the input matrix and corresponding number of variables W. However, since the model is still linear wrt to parameters W, it's still a linear model (GLM) and the least square solution is given by normal equation although it can model nonlinearity ! We might want to add L1 or L2 regularization terms to the squared error in which case we find parameters by stochastic gradient descent instead of normal equation.

- In machine Learning we minimize training error but we actually care about minimizing test error that we can't directly access. The least amount of error possible incurred by a perfect model that knows the data generation distribution is called Bayes error. If the data generating distribution for the training and test sets are different, we will have high error rate for our test set and we can't do much but to get more data that is more similar in data generation distribution to our test set. 

However, If we assume that both train and test sets are iid samples (independantly sampled from same distribution) of one data generating distribution, then the expectation of error for a fixed model would be exactly the same for both train and test samples. However, in the learning process, we sample the data generating distribution (batch) and modify model parameters to reduce training error. Therefore, the expectation of the error for samples drawn for training would be smaller (or equal) than the expectation of error for samples drawn for test (no learning). 

- A well-performing machine learning algorithm will have 1) low training error (bias) and 2) small gap between train and test errors (variance). These two correspond to model capacity, a simple model might underfit and have large bias while a complex model might overfit and have large variance. The recipe is that we usually increase model complexity if the bias is high to better fit the data. If the variance is high, we try applying regularization techniques to reduce overfitting. 

- The no free lunch theorem says if we average over all possible data generation distributions, all machine learning algorithms will have the same test error rate. This means that there is no best universal algorithm for all data sets (data generation distributions). We should therefore focus on finding data generation distributions we encounter in the real world, and the machine learning algorithms that work well for them. 

- Hyperparameters are parameters that are set but not learned e.g. learning rate. We use therefore, further divide the training set into training and validation sets to be able to set hyperparameters for the learning algorithm for example adaptive gradient descent (adagrad). Dividing the data to fixed test and training doesn't give much certainty about generalization if test set is small so we do k-fold cross validation (divide data to k segments and repeat learning k times keeping a different segment for test each time) to get mean and variance of generalization error.

- Point estimate finds single set of values for model parameters. For example, sample mean/std are point estimates for population mean/std under a Gaussian model. Point estimate from samples (as opposed to population) have error which are quantified by bias and variance of the estimator. Bias is $bias(param)=(E[sample param] - population param)$. For example, if we assume a Gaussian distribution model, sample mean estimator is unbiased estimate of real mean but sample variance estimator underestimates population variance. Consistency is when the bias converges to zero with increasing amount of data. 

- Variance of a point estimator is simply the variance of the estimate over data. Its square root is called standard error which provides a measure of how much variation in our point estimate we expect as we calculate the estimate on a different sample. Standard error for sample mean point estimator is $SE(sample mean)= (population std) / sqrt(number of samples)$. We don't know population std so we use sample std which we know is still biased, but if the number of samples is large it's reasonable. According to central limit theorem, which says the mean will be approximately distributed with a normal distribution, we can calculate 95% confidence interval using mean and standard error (95%CI= mean +- 1.96*SE).

- There is a trade-off between bias and variance of an estimator, therefore, we might use a biased point estimator in cases where we are interested in low estimator variance. Cross-validation helps with deciding about the tradeoff. An example of where this concept might be useful is batch normalization in deep nets. The statistics of each batch is slightly different from another and therefore as it propagates through a deep net, it changes drastically. This makes learning difficult and batch normalization solves it by normalizing each batch at each layer using sample mean and variance point estimates (low estimator variance is desired in training). However, since sample variance is biased, in testing where we care about low bias in our prediction we use the unbiased sample variance point estimator instead.

- Maximum likelihood principle is a popular point estimator which states that model parameters should be chosen that maximize the probability of data. Assuming independant samples, that's $max(\prod p_model(x_i))$. We take log to convert the product to sum and help with numerical underflow i.e. max [sum(log p_model(x_i))]. By dividing by constant m number of data, we can write the sum as the expectation of model distribution $max(E_{p_em}[log p_model(x)])$ over emprical distribution of data p_em(x). This way we can interpret maximum likelihood as minimizing the KL divergence between the model distribution p_model(x) and the emprical distribution p_em(x) i.e. KL(p_model/p_em)=E_{p_em}[log p_em(x) - log p_model(x)]. Any loss function consisting of a negative log model probability (likelihood) is a cross-entropy between the model likelihood and emprical distribution defined by the training set. Maximum likelihood is the best estimator assymptotically as the number of data tends to infinity if the true data distribution is in the model distribution family.

- In the supervised learning case, we write p_model(y/x) and follow the same procedure for maximum likelihood estimation. For example, in linear regression we assume a Gaussian model $p(y/x)=N(wx, \sigma^2)$ with a fixed variance and a linear function of mean. Performing maximum liklihood on this model results in optimizing mean squared error. When the number of examples is small in the model, we will overfit and have a large gap between train (sample) and test (population) errors (large variance). Therefore, we trade off a biased version of maximum likelihood (i.e. add regularization) for less variance in the model. 

- As opposed to point estimates for model parameters in frequentist stats, Bayesian approach views model parameters as random variables instead of data points. Therefore, it assumes a distribution for model parameters which is first defined by priors (usually a high entropy distribution like uniform or Gaussian), then data are plugged in and decrease the entropy by concentrating around likely values which establish the posterior belief. The Bayesian approach has two important difference; 1) every prediction will also be a probability distribution (belief) reflecting the model uncertainty about it (robust against overfitting) 2) Another difference is in the application of priors which shift the starting probability space to regions prefered in the beginning. In practice priors are often used to convey preference for models that are simple or smooth. 

- As an example, consider Bayesian linear regression. We consider multivariate Gaussian model $p(y/x)=N(wx, I)$. Assuming an iid Gaussian prior on weights, we can use the Bayesian formula to do Bayesian inference. Assuming a zero mean and alpha scaling for covariance on priors, we get the same result as maximum likelihood linear regression with a regularization of {alpha * w^Tw}. 

- MAP in an alternative point estimate to maximum likelihood that allows priors to influence parameters choice. MAP estimate states that one should choose parameters that maximize the posterior i.e. $max(p(param/x)=p(x/param)p(x))$. Applying log to convert product to sum and prevent underflow we get $max(log p(x/param)+ log p(x))$. The first term is the maximum likelihood term while the second term corresponds to prior distribution. MAP with a Gaussian prior on weights corresponds to maximum likelihood with weight decay regularization.

- We can perform classification using the linear regression approach by using a logistic sigmoid on the output of linear regression to squash it into [0,1] interval which can be interpreted as probability of either class 0 or 1. Optimizing the maximum likelihood (negative log likelihood) loss is done via SGD.

- Linear support vector machines are linear classifiers similar to logistic regression but use their wx+b as a hyperplane separating the two class instead of probabilities. SVMs try to find the hyperplane with the maximum seperation between classes (i.e. maximum k where wx+b>k for class 1 and wx+b<-k for class -1). Another way to think about this is to put a bubble around each data point in data space; as we increase the radius of these bubbles, the set of possible solutions that don't violate the data bubbles and seperate the two classes decreases until a single solution is found which is the optimum solution with maximum bubble radius possible. Note that in this analogy, only the data points from the two classes that are close to each other will determine the decision boundry. Other points are simply having fun with their bubbles far away from the hyperplane. These critical points are called support vectors. 

- A breakthrough in statistical theory of machine learning by Vapnik and Chernovsky (VC) was the finding that the test error is bounded by the training error plus a monotonicly increasing funtion of a quantity called VC dimension. VC dimension is the minimum of data dimensionality and a ratio of bubbles {a bubble around all data over the bubble of support vectors}. This is extremely important since if we can reduce the bubble ratio, we can get around the curse of dimensionality in data! Therefore, there are two ways in which we can minimize the ratio. 1) is to keep the numerator fixed by constraining the scale of the scale of the data to be one and the maximize the margine. 2) is to keep the denominator fixed by imposing the constraint that margin be one, [therefore the the output of the model (wx+b) would be bigger than one for class one and smaller than -1 for class -1; if d is labels +1 or -1 then $d(wx+b)>1$], and minimize the data coefficients (w) to best rescale the data bubble. So the optimization problem accordint to (2) would be $min: L=1/2 w^Tw subject to d(wx+b)>1$. This can be rewritten using lagrange multipliers as $min: J(w, b, \alpha)= 1/2 w^Tw - \sum(\alpha_i d_i (w^Tx_i+b) + \sum(\alpha_i))$ where $\alpha_i$ are lagrange multipliers for data samples $x_i$. Imagine a horse seat; This objective has to be minimized wrt (w, b) to get to minimum of the seat and maximized wrt $\alpha$ to get to the top of the seat (saddle point). 
