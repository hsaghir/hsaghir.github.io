---
layout: article
title: Numerical Computation Deep Learning book Ch4
comments: true
categories: data_science
image:
  teaser: jupyter-main-logo.svg
---

- Statistics focuses on finding causal relationships or estimating confidence intervals of functions (usually simple / linear models). Machine learning focuses more on complex function estimation and prediction. Bayesian ML focuses on complex function estimations along with confidence estimations.  

- Learning is defined as using data points to improve on a task as measured by a performance measure. Designing a performance measure (loss function) is an art and case specific. Supervised Learning [Classification, Regression, Structured Output prediction] involves conditional probability estimation of outputs from inputs P(Y/X). Unsipervised learning [e.g. generative modelling, denoising, imputation, clustering, etc] involves a Density estimation i.e. learning P(X). In a modelling effort we might introduce model variables Z and try to do a density estimation on P(X,Z) instead. This involves finding a function from (N dimensional) data space to a probability surface in (N+1 dimension). Semi-Supervised learning is a combination of supervised and unsupervised data while multi-instance learning is the case where individual samples don't have a label but we know a certain type of sample exists among a group.

- Using the chain rule of probabilities we can transform an unsupervised learning problem to many supervised learning problems P(X)=P(x_i/x_1, x_2, ..). In the same sense, we can solve a supervised learning problem using the Bayes theorem by estimating P(Y/X)=P(X,Y)/P(X). 

- A generalized linear model (GLM) can be defined as $Y_out=WX$ where X is input data and w are parameters. Setting the gradient of quadratic cost for this model to zero to find optimum parameters that minimize quadratic cost will return least square solution analytically as $W= ((X X^T)^-1)(X^T)(Y_target)$ called normal equation. We can make polynomial or arbitrary nonlinear models of X by applying arbitrary nonlinear functions to X and extending the input matrix and corresponding number of variables W. However, since the model is still linear wrt to parameters W, it's still a linear model (GLM) and the least square solution is given by normal equation although it can model nonlinearity ! We might want to add L1 or L2 regularization terms to the squared error in which case we find parameters by stochastic gradient descent instead of normal equation.

- In machine Learning we minimize training error but we actually care about minimizing test error that we can't directly access. The least amount of error possible incurred by a perfect model that knows the data generation distribution is called Bayes error. If the data generating distribution for the training and test sets are different, we will have high error rate for our test set and we can't do much but to get more data that is more similar in data generation distribution to our test set. 

However, If we assume that both train and test sets are iid samples (independantly sampled from same distribution) of one data generating distribution, then the expectation of error for a fixed model would be exactly the same for both train and test samples. However, in the learning process, we sample the data generating distribution (batch) and modify model parameters to reduce training error. Therefore, the expectation of the error for samples drawn for training would be smaller (or equal) than the expectation of error for samples drawn for test (no learning). 

- A well-performing machine learning algorithm will have 1) low training error (bias) and 2) small gap between train and test errors (variance). These two correspond to model capacity, a simple model might underfit and have large bias while a complex model might overfit and have large variance. The recipe is that we usually increase model complexity if the bias is high to better fit the data. If the variance is high, we try applying regularization techniques to reduce overfitting. 

- The no free lunch theorem says if we average over all possible data generation distributions, all machine learning algorithms will have the same test error rate. This means that there is no best universal algorithm for all data sets (data generation distributions). We should therefore focus on finding data generation distributions we encounter in the real world, and the machine learning algorithms that work well for them. 

- Hyperparameters are parameters that are set but not learned e.g. learning rate. We use therefore, further divide the training set into training and validation sets to be able to set hyperparameters for the learning algorithm for example adaptive gradient descent (adagrad). Dividing the data to fixed test and training doesn't give much certainty about generalization if test set is small so we do k-fold cross validation (divide data to k segments and repeat learning k times keeping a different segment for test each time) to get mean and variance of generalization error.

- Point estimate finds single set of values for model parameters. For example, sample mean/std are point estimates for population mean/std under a Gaussian model. Point estimate from samples (as opposed to population) have error which are quantified by bias and variance of the estimator. Bias is $bias(param)=(E[sample param] - population param)$. For example, if we assume a Gaussian distribution model, sample mean estimator is unbiased estimate of real mean but sample variance estimator underestimates population variance. Consistency is when the bias converges to zero with increasing amount of data. 

- Variance of a point estimator is simply the variance of the estimate over data. Its square root is called standard error which provides a measure of how much variation in our point estimate we expect as we calculate the estimate on a different sample. Standard error for sample mean point estimator is $SE(sample mean)= (population std) / sqrt(number of samples)$. We don't know population std so we use sample std which we know is still biased, but if the number of samples is large it's reasonable. According to central limit theorem, which says the mean will be approximately distributed with a normal distribution, we can calculate 95% confidence interval using mean and standard error (95%CI= mean +- 1.96*SE).

- There is a trade-off between bias and variance of an estimator, therefore, we might use a biased point estimator in cases where we are interested in low estimator variance. Cross-validation helps with deciding about the tradeoff. An example of where this concept might be useful is batch normalization in deep nets. The statistics of each batch is slightly different from another and therefore as it propagates through a deep net, it changes drastically. This makes learning difficult and batch normalization solves it by normalizing each batch at each layer using sample mean and variance point estimates (low estimator variance is desired in training). However, since sample variance is biased, in testing where we care about low bias in our prediction we use the unbiased sample variance point estimator instead.

- Maximum likelihood principle is a popular point estimator which states that model parameters should be chosen that maximize the probability of data. Assuming independant samples, that's $max(\prod p_model(x_i))$. We take log to convert the product to sum and help with numerical underflow i.e. max [sum(log p_model(x_i))]. By dividing by constant m number of data, we can write the sum as the expectation of model distribution $max(E_{p_em}[log p_model(x)])$ over emprical distribution of data p_em(x). This way we can interpret maximum likelihood as minimizing the KL divergence between the model distribution p_model(x) and the emprical distribution p_em(x) i.e. KL(p_model/p_em)=E_{p_em}[log p_em(x) - log p_model(x)]. Any loss function consisting of a negative log model probability (likelihood) is a cross-entropy between the model likelihood and emprical distribution defined by the training set. 

- In the supervised learning case, we write p_model(y/x) and follow the same procedure for maximum likelihood estimation. For example, in linear regression we assume a multivariate Gaussian model $p(y/x)=N(wx, \sigma^2)$ with a fixed variance a linear mean

