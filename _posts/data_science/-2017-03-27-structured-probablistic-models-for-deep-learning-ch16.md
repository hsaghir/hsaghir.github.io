---
layout: article
title: Structured Probabilistic Models for Deep Learning Ch16
comments: true
categories: data_science
image:
  teaser: jupyter-main-logo.svg
---

- A structured probabilistic model is a way of describing a probability distribution, using a graph to describe which random variables in the probability distribution interact with each other directly. It's useful in reducing the number of parameters needed to represent a distribution down from all possible combination of all variables to only the combination of variables that are directly connected in the graph. Here we use “graph” in the graph theory sense—a set of vertices connected to one another by a set of edges. Note that a graph only conveys information about dependence of variables not how they are dependent, thus it's very general. The exact relationship between connected variables in a graph is defined through the shape of the conditional distribution. A probabilistic graph defines a probability distribution through the product of a set of local conditional probability distributions. Therefore, one of the major difficulties in graphical modeling is understanding which variables need to be able to interact directly, i.e., which graph structures are most suitable for a given problem. 

- Directed graphical model, are also known as the belief network or Bayesian network. Vertices are random variables in the model, and the direction of the arrow indicates direction of dependence. Some restrictions on the graph structure, such as requiring it to be a tree, can guarantee that operations like computing marginal or conditional distributions over subsets of variables are efficient. 

- Another popular language is that of undirected models, otherwise known as Markov random fields (MRFs) or Markov networks. Unlike directed models there is little structure to the nodes and an undirected graph is usually represented in term of cliques. Cliques are subsets of nodes that are all connected to each other by an edge. For each clique in the graph, a factor $\phi$ (clique potential) measures the affinity of the variables in that clique.  These factors are non-negative and their product defines an unnormalized probability distribution on the Markov network. 

- One key difference between directed modeling and undirected modeling is that directed models are defined directly in terms of probability distributions from the start, while undirected models are defined more loosely by $\phi$ functions that are then converted into probability distributions through normalization. A probability distribution that is obtained by normalized product of factors is called Gibbs distribution. 

- To obtain a valid probability distribution for an undirected graph, we need to divide it by a normalizing  (partition) function. Normalizing function is an integral over all possible combinations of random variables, therefore, the clique factors need to be conducive to efficient calculation of the integral (i.e. integral should exist and domain of variables should be appropriate). Otherwise, the partition function is intractable, in which case we will need to use approximations (e.g. variational inference).

- One key idea to keep in mind while working with undirected models is that the domain of each of the variables has dramatic effect on the kind of probability distribution that a given set of $\phi$ functions corresponds to. Often, it is possible to leverage the effect of a carefully chosen domain of a variable in order to obtain complicated behavior from a relatively simple set of $\phi$ functions. For example, a binary variable RBM!

- Many interesting theoretical results about undirected models depend on the assumption that the unnormalized probability distribution is non-negative. A convenient way to enforce this condition is to use an energy-based model (EBM) where unnormalized probability distribution is the exponential of an energy function $p(x)=exp(-E(x))$. Any such distribution defined by an exponential is an example of a Boltzmann distribution, that's why many energy-based models are called Boltzmann machines. The exponentiation makes each term in the energy function correspond to a factor for a clique because $ exp(a) exp( b) = exp(a + b)$. Each term of the energy function can be thought of as an “expert” that governs a subset of random variables and the experts together enforce a complicated high-dimensional constraint. Many algorithms that operate on probabilistic models only need log of unnormalized probability distribution. Negative of this log probability is called free energy.

## Seperation
- separation and d-separation of a graph is the connectedness of the nodes in the graph. They tell us about conditional independences between nodes that we see in tha graph. There might be additional conditional independencies that we don't know about and thus have not dipicted in the graph. Graphs cannot represent Context-specific independences that dependent on the value of some variables in the network. Similar concepts apply to directed models, except that it is referred to as d-separation. The “d” stands for “dependence.” D-separation for directed graphs is defined the same as separation for undirected graphs. In directed nets, determining whether a path is active is somewhat more complicated.

- In an undirected graph, conditional independence is simply implied by the graph separation. If no path exists between two nodes, or all paths contain an observed variable (passive path), then they are separated (independent). When we draw a graph, we can indicate observed variables by shading them. 

- In a directed graph, due to direction of arrows three cases are possible for two variables("chain", "tree-root", "V").  In the "chain" and "tree-root" structures if the middle node is observed (or conditioned on) the two variables become independent. The "V" structure where middle node is a common child is an exception. This is called the explaining away effect where observing or conditioning on the connecting child will not make the two variables independent. It happens even if any descendant of the child is observed. The only way to block a path of two variables through a "V" structure is to observe none of the descendants of the shared child.

- No probabilistic model is inherently directed or undirected. Instead, some models are most easily described using a directed graph, and some using undirected graph. Directed models and undirected models both have their advantages and disadvantages and we should choose the one that better express a specific problem in terms of less dependencies. Sometimes a different language becomes more appropriate if we observe a certain subset of variables, or if we wish to perform a different computational task. For example, the directed model description often provides a straightforward approach to efficiently draw samples from the model while the undirected model formulation is often useful for deriving approximate inference procedures. 

- In other words, directed models can encode some independences that undirected models cannot encode, and vice versa. Directed models are able to use one specific kind of substructure that undirected models cannot represent perfectly. This substructure is called an immorality. The structure occurs when two random variables a and b are both parents of a third random variable c, and there is no edge directly connecting a and b in either direction.