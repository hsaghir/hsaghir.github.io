---
layout: article
title: Dialog systems
comments: true
categories: data_science
image:
  teaser: jupyter-main-logo.svg
---

# Dialog systems:

## chatbots:
They are general-purpose dialog systems usually designed for fun or to pass the Turing test. Two major chatbot architectures include: 
    + rule-based systems, 
    + and corpus-based chatbots that mine a conversation corpus. Corpus-based chatbots have two types:
        * systems based on information retrieval:
            - The principle behind information retrieval based chatbots is to respond to a user’s query by retrieving some sentence from a corpus. A common choice of corpus is to collect databases of human conversations (social media, movie dialog, other human convos).
                + Return the most similar sentence in corpus
                + Return the response to the most similar sentence in corpus.
        * systems based on supervised machine learning based on sequence transduction:
            - generating an answer to a question by a seq2seq model using a conversation corpus. 
                + response generation is more difficult since words in source and target sentences are not shared/aligned.
                + basic seq2seq produce predictable responses like “I’m OK”. can be fixed by using a mutual information objective, or by modifying a beam decoder to keep more diverse responses in the beam.
                +  seq2seq can't model the longer prior context of the conversation. but a hierarchical model can help. 
                +  seq2seq generate single responses, and don't do a good job of continuously generating coherent responses across convo. This can be addressed by using reinforcement learning, as well as techniques like adversarial networks, to learn to choose responses that make the overall conversation more natural 
    + Both rule-based and corpus-based chatbots do little modeling of the conversational context (unlike frame-based dialog systems). 
        * Instead, they focus on generating a single response turn that is appropriate given the user’s immediately previous utterance. 
        * For this reason they are often called **response generation systems**. 
        * Corpus-based chatbots thus have some similarity to question answering systems, which focus on single responses while ignoring context or larger conversational goals.
    + evaluation by humans! Slot-filling evaluation and word-overlap metrics (BLUE) are not good. 
    + Another eval metric is adversarial eval. The idea is to train a “Turing-like” evaluator classifier to distinguish between human-generated responses and machine-generated responses. The more successful a response generation system is at fooling this evaluator, the better the system.


### FAIR ParlAI platform for conversation agent research
a unified framework for training, testing, and sharing dialog models, especially multitask training or evaluation over many tasks at once; It consists of:

1. datasets, evaluation, and benchmarking tools for over 20 tasks including popular datasets such as SQuAD, bAbI tasks, MCTest, WikiQA, QACNN, QADailyMail, CBT, bAbI Dialog, Ubuntu, OpenSubtitles and VQA.

2. a repository of ML models for comparing with others’ models, and improving upon existing architectures (such as memory networks, seq2seq and attentive LSTMs).

3. integration of Amazon Mechanical Turk for data collection, human evaluation,
and online/reinforcement learning;

All the tasks in ParlAI have a single API which makes applying any agent to any task, or multiple tasks at once, simple. All agents (including teachers) speak to each other in a single common format – the observation/action object (a python dict with a set of fields depending on task). This object is returned from agent.act() and passed in to agent.observe().

The main concepts (classes) in ParlAI are:

- worlds: This can vary from being very simple, e.g. just two agents conversing, to much more complex scenarios.
- agents: an agent that can act (speak) in the world. An agent is either a
    + learner (i.e. a machine learned system), 
    + a hard-coded bot designed to interact with learners, 
    + or a human (e.g. a Turker).
- teachers:  a type of agent that talks to the learner in order to teach it, for example it can implement one of the tasks of: 
    + question answering (e.g. SQuAD dataset)
    + sentence completion (e.g. QADailyMail dataset)
    + dialog chit-chat (e.g. movies subreddit dataset)
    + Goal-oriented dialog (e.g. bAbi dialog task dataset)

The ParlAI codebase has five main directories:
- core: the primary code for the platform.
    + agents.py: defines the Agent base class for all agents, which implements the observe() and act() methods, the Teacher class which also reports metrics, and MultiTaskTeacher for multitask training.
    + dialog_teacher.py: the base teacher class for doing dialog with fixed chat logs.
    +  worlds.py: defines the base World class, DialogPartnerWorld for two speakers, MultiAgentDialogWorld for more than two, and two containers that can wrap a chosen environment: BatchWorld for batch training, and HogwildWorld for training across multiple threads.
    +  dict.py: code for building language dictionaries.
    +  metrics.py: computes exact match, F1 and ranking metrics for evaluation.
    +  params.py: uses argparse to interpret command line arguments for ParlAI
- agents: contains agents which can interact with the worlds/tasks (e.g. learning models).
    + different sample models sit here. 
- tasks: contains code for the different tasks available from within ParlAI.
    + To add a new task, one must implement build.py to download any required data, and agents.py for the teacher. 
- mturk: contains code for setting up Mechanical Turk and sample MTurk tasks.

#### Using ParlAI in practice
ParlAI is conceptually setup like an RL problem. Agents interact with each other and pass messages around using python dictionaries containing different types of information. An agent is either a student that learns or a teacher that provides data. Agents need to have an "observe()" and "act()" method implemented to be able to communicate. The world is setup by implementing a "parley()" method, which indicates how one set of interactions between agents is performed with each call. To be able to use ParlAI in practice, one needs to understand the following:
    + argument parsing 
    + data flow 
    + mini-batch processing

To use ParlAI, we call a main function from command line that sets up the environment for different settings like training, eval, etc . We usually use command line arguments to indicate our agents' class, the teachers' class, tasks and hyper-parameters. "argparse()" makes sense of these arguments. 
    + Command line arguments are parsed into a python dictionary called **opt** that is passed to world and agents for initialization. a few common of arguments are:
        -m: The agent model needs to be either placed in the "agents/" folder or the full path to the class be passed e.g. parlai.agents.<directory_name>.<file_name>:<class_name>
        -t: Task name of ParlAI. eg babi:Task1k:1 or babi,cbt
        -mf: File name for saving or loading the model.

for example, to train the drqa model on squad dataset with batch size 32:
```bash
python ~/ParlAI/examples/train_model.py -m drqa -t squad -b 32
```

We can define the main setup either using one of the pre-defined environment setups or define a new environment ourself.
    + Pre-defined environment setups:
        * display_data.py: display data from a particular task provided on the command-line.
        * display_model.py: show the predictions of a provided model.
        * eval_model.py: compute evaluation metrics for a given model on a given task.
        * train_model.py: execute a standard training procedure with a given task and model, including logging and possibly alternating between training and validation.

    + To define our own custom environment:
        1. First we need to define the world and agents.
        ```python 
        # opt contains hyper-parameters and an id
        teacher = SquadTeacher(opt) # the SQuAD dataset is teacher
        agent = MyAgent(opt) # a NN model is agent 
        world = World(opt, [teacher, agent]) # world defines the environment
        ```

            - There are a couple of pre-defined worlds that let agents interact in a set of common pre-defined ways. 
                + **World(object)** provides a generic parent class.
                + DialogPartnerWorld(World) provides a two-agent turn-based dialog setting. switches back and forth between two agents.
                + **MultiAgentDialogWorld(World)** provides a multi-agent setting. each agent gets a turn to speak in a round-robin fashion.
                + **MultiWorld(World)** creates a set of environments (worlds) for the same agent to multitask over.
                + **HogwildWorld(World)** is a container that creates another world within itself for every thread, in order to have separate simulated environments for each one. Each world gets its own agents initialized using the share() parameters from the original agents.
                + **BatchWorld(World)** is a container for doing minibatch training over a world by collecting batches of N copies of the environment (each with different state).

            - If we are not using one of the pre-defined worlds, we need to define how we want the agents to interact. 
                + We need to implement "world.parley()" to define one step of message exchange between the agents.
                ```python
                def parley(self):
                    for agent in self.agents: 
                        act = agent.act() # get a response from an agent 
                        for other_agent in self.agents: # feed it to other agents
                            if other_agent != agent:
                                other_agent.observe(act) # send the current state to agents.
                ```

        2. then we need to define a main loop for exchanges to happen inside. Each to call "world.parley()" in the main loop can execute one step of training / evaluation.
        ```python
        for i in range(num_exs): # main loop (repeated as for number of examples)
            world.parley() # run the world for one exchange (e.g. one example data). 
            print(world.display())
        ```

##### Batch training 
Let's say batch_size=4. ParlAI actually creates four shared versions of the teacher agent, and four shared versions of the student agent. The way the batchworld is initialized is followed:
    + First, Using the control "if not shared", we create a normal world (e.g. DialogPartnerWorld) and initialize the first instance of the agent and teacher similar to "batchsize=1" case.
    + Then, "batchsize" number of shared instances are created from both the agent and the teacher. Shared instances only call "__init__()" and "observe()" functions, if the class implements the "batch_act()" function. we can let shared instances have their own states like the number of the example.
        * for the seq2seq model, we only want one instance of all its parameters--we don't want to push 32 models into RAM / the GPU. so for the shared instances, we don't initialize nearly anything. 
    + This way, each of those shared instances can keep track of their batch row's local information--e.g., the previous utterances in the episode. Those observations will be collected back through the batchworld, and delivered to the original instance's batch_act method.

Every time we call "parley()" on this BatchWorld (shared data world), the following happens:
    1. each of the shared teachers perform "act()" individually.
    2. each of the shared agents perform "observe()" individually and the batchworld appends their observations into a list of observations. 
    3. The original agent executes "batch_act()" on a list of dict objects in the observations parameter (after BatchWorld has collected them).
    4. batch_act return a list of replies and each shared teacher performs "observe()" individually on its own sample and store results.


##### Building an agent
An agent can be a neural network that learns from a dataset or as simple as repeating and displaying data. A learn-able student agent (e.g. a seq2seq model) receives a set of messages in python dictionary format via observe() function, combines them into a minibatch tensor, passes them through the network, calculates loss, does a gradient step and produces output tensor, converts the output tensor back to a set of python dictionaries (messages), and sends them back to other agents via act(). The neural network model can be implemented in any DL framework. 

A teacher agent usually samples a batch from a dataset, converts them into a set of python dictionary messages, sends them to the student agent via its "act()" function (also tracks what samples have been sent out). It also receives the output of the student agent via its "observe()" method and uses it for evaluating the student model. 


Defining an Agent commonly involves implementing the following methods:
    1. __init__():
        * first we inherit the parent class (Agent)
        * initialization parameters is a dict of command-line parameters **opt** and an optional dict of shared parameters, **shared**.
        * we check if **shared** parameters exist in which case we only copy them to current instance. Otherwise we initialize the model from scratch using input **opt**. 
        ```python
        class ExampleAgent(Agent):

            def __init__(self, opt, shared=None):
                # initialize defaults first
                super().__init__(opt, shared)

                # ... some setup for both shared and original instances

                if not shared:
                    # set up model from scratch
                else:
                    # ... copy initialized data from shared table
        ```

    2. **observe()**: Observe training data, modify if necessary, and then return it to be queued for batching.
        * When the dataset teacher "acts()", it produces an observation for the student. Shared student agents aggregate observations until an "episode_done" flag is seen when all the minibatch size is observed. Then the minibatch is used for producing an output and learning. 
        ```python
        def observe(self, observation):
            observation = copy.deepcopy(observation)
            self.observation = observation
            return observation
        ```

    3. **batch_act()**: receives list of "batch_size" number of observations and returns a list of the same length with agent’s replies. This is how we need to set it up: 
        - Set up our list of dicts to send back as replies, with the agent’s ID set.
        - Convert the list of observations into a **tensor** that can be processed by the learning model (e.g. batchify()).
        - Calculate model output using input tensor, If labels were available, also calculate loss and update the model as well.
        - Unpack the predictions into a list of reply dicts and return them.
        ```python
        def batch_act(self, observations):
            batchsize = len(observations)
            # initialize a table of replies with this agent's id
            batch_reply = [{'id': self.getID()} for _ in range(batchsize)]

            # convert the observations into batches of inputs and targets
            # `labels` stores the true labels returned in the `ys` vector
            # `valid_inds` tells us the indices of all valid examples
            # e.g. for input [{}, {'text': 'hello'}, {}, {}], valid_inds is [1]
            # since the other three elements had no 'text' field
            xs, ys, labels, valid_inds, is_training = self.batchify(observations)

            if xs is None:
                # no valid examples, just return empty responses
                return batch_reply

            # produce predictions either way, but use the targets if is_training
            predictions = self.predict(xs, ys, is_training)

            for i in range(len(predictions)):
                # map the predictions back to non-empty examples in the batch
                # we join with spaces since we produce tokens one at a time
                batch_reply[valid_inds[i]]['text'] = ' '.join(
                    c for c in predictions[i] if c != self.EOS)

            return batch_reply
        ```
    
    4. **share()** Optionally, agents can use this method to share any information with other instances during batching or hogwild training. For example, model parameters can be shared in hogwild mode between instances.
    ```python
    def share(self):
        """In addition to default Agent shared parameters, share metrics."""
        shared = super().share()
        shared['metrics'] = self.metrics
        return shared
    ```


### Conversation AI challenge 

The RLLab solution:
- generate responses.
- score responses.
- select best response. 


Convai chatbot:
news article (squad) (about 10 lines)
- humans evaluate (1 to 5), they also upvote /downvote each response

- mix of rule base and generative system + a candidate selector (scoring model) that selects between open domain conversation and pre-defined question 


1- given a conv on reddit/twitter, produce an utterance using a hierarchical seq2seq

2- neural question generation using squad. 

3- squad model:  retrieval model (drqa model).

4- topic of article: a classifier trained on yahoo news corpus. (10 topics). ngrams feature to classify each sentence 

5- fact retriever: when have no idea to say, retrieve a fact (from a set defined before). minimum cosine distance between average word embeddings of conv history and average word embeddings of fact. 

6- rulebase system: extract entities (spacy)+ regex for asking a q based on entities. extract (persons, orgs, etc) return a predefined template sentence based on entities. 

7- predefined answers to predefined qs using regex rules. 
    - Alice chatbot (a linguistic internet comp entity)


8- greedy scorer
    - predict up/down vote of the user. loss: softmax cross entropy as a score.
    - data: 6k convai + 2k inter-lab conv
    - article, conv history , candidate response (hand engineered features)
        - feats: average word embeddings, cosine silimarty between candidate response and history/article, topic for candidate and prev user message, bigrams/trigrams, entities, notn-stop word overlap, confusion-intensifier-negation words, wh-word percentage, length of article and conv hist. 
    - an MLP classifier 
9- Q-scorer.
    - Q-learning. 

## Goal-oriented dialog systems:
- Frame-based architecture (the GUS architecture):
    + Modern task-based dialog systems are based on a domain ontology, a knowledge structure representing the kinds of intentions the system can extract from user sentences. 
        * The ontology defines one or more frames, each a collection of slots, and defines the values that each slot can take. The GUS architecture is a production rule system that dynamically switches control. Different types of inputs cause different productions to fire, each of which can flexibly fill in different frames. Commercial dialog systems provide convenient interfaces or libraries to make it easy to build systems with these kinds of finite-state or production rule systems, for example providing graphical interfaces to allow dialog modules to be chained together.
        * The set of slots in a GUS-style frame specifies what the system needs to know, and the filler of each slot is constrained to values of a particular semantic type.
        * The control architecture of frame-based dialog systems is designed around the frame. Most frame-based dialog systems are based on finite-state automata that are hand-designed for the task by a dialog designer. The goal is to fill the slots in the frame with the fillers the user intents, and then perform the relevant action for the user.
        * This system asks the user a series of questions, ignoring (or misinterpreting) anything that is not a direct answer to the question and then going on to the next question.
        * Most finite-state universal systems also allow universal commands that can be said anywhere in the dialog, like help, or start over. 
        * The standard GUS architecture for frame-based dialog systems, used in various forms in modern systems like Apple’s Siri, Amazon’s Alexa, and the Google Assistant, follows the frame in a more flexible way.
            - The system asks questions of the user, filling any slot that the user specifies, even if a user’s response fills multiple slots or doesn’t answer the question asked. The system simply skips questions associated with slots that are already filled. Slots may thus be filled out of sequence. The GUS architecture is thus a kind of **mixed initiative**, since the user can take at least a bit of conversational initiative in choosing what to talk about.
        * Once the system has enough information it performs the necessary action (like querying a database of flights) and returns the result to the user.
        * The system might need to deal with multiple frames.
- Dialog-state architecture: 
    + these agents fill slots, but they are also capable of understanding and generating such dialog acts, actions like asking a question, making a proposal, rejecting a suggestion, or acknowledging an utterance and they can incorporate this knowledge into a richer model of the state of the dialog at any point.
    + The system needs a dialog policy to decide what to say (when to answer the user’s questions, when to instead ask the user a clarification question, make a suggestion, and so on)
    + The system has a dialog state tracker which maintains the current state of the dialog.
        * As of the time of this writing, no commercial system uses a full dialog-state architecture, but some aspects of this architecture are beginning to appear in industrial systems.
    + the policy can be learned using RL and a POMDP.

- [Learning End-to-End Goal-Oriented Dialog](https://arxiv.org/abs/1605.07683):



# Encoding dialog conversation:

However several dialogue corpora, most notably those extracted from subtitles, do not include any explicit turn segmentation or speaker identification.

- [Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models.](http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/11957/12160)
    + Uses tokens for speech acts (end of turn, pause, etc) very much like words to indicate change of turn between persons. Other than that, it treats dialog modeling as a language modeling task
    + Uses two hierarchical levels of RNNs to encode the dialog.  
        * Level one is mapping the sequence of words in an utterance into an utterance embedding.
        * Level two encodes the sequence of utterance embeddings into a dialog state (aka context) embedding.
    + An RNN decoder decodes the input utterance into the response utterance conditioned on the dialog state (context) embedding.
- [A Persona-Based Neural Conversation Model](https://arxiv.org/pdf/1603.06155.pdf)
    + Learns an embedding for various speaker personas like other words in the dictionary and conditions the decoder on speaker vector. Speakers producing similar responses tend to have similar embeddings.
- [Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory](https://arxiv.org/abs/1704.01074)``
    + classifies the reponses with emotion categories. Then embeds emotion categories, and conditions the generation on the emotion category.
    + represents each emotion category as a learned low-dimensional vector.



- Modeling of users and speakers has been extensively studied within the standard dialog modeling framework (e.g., (Wahlster and Kobsa, 1989; Kobsa, 1990; Schatztnann et al., 2005; Lin and Walker, 2011)).
- improving the content quality of the generated responses, including diversity promotion (Li et al. 2016a), considering additional information (Xing et al. 2017; Mou et al. 2016; Li et al. 2016b), and handing unknown words (Gu et al. 2016)
- generate text from controllable variables. (Hu et al. 2017) proposed a generative model which can generate sentences conditioned on certain attributes of the language such as sentiment and tenses. Affect Language Model was proposed in (Ghosh et al. 2017) to generate text conditioned on context words and affect categories. (Cagan, Frank, and Tsarfaty 2017) incorporated the grammar information to generate comments for a document using sentiment and topics.



