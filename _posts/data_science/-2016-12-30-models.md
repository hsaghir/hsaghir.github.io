---
layout: article
title: Building generative model algorithms
comments: true
categories: data_science
image:
  teaser: jupyter-main-logo.svg
---

There is a clear distinction between the choice of model, choice of inference, and the resulting algorithm. A variety of models can be used and a variety of learning principles and inference algorithms (e.g. ML, MAP, EM, MCMC, Variational, etc) are available for performing inference in these models. Combining models with different inference schemes leads to different algorithms e.g. GAN, VAE, Regularization, Optimization methods like SGD, etc. 

The main ideas that repeatedly come up as one explores deep generative models are  Bayesian deep learning, variational approximations, memoryless and amortised inference, and stochastic gradient estimation. So the hope is that by having an intuition of these ideas before the mathematics, one can understand deep generative models. 

## treat deep networks as functions, Convolutional, recurrent, LSTM, GRU, fully-connected, etc

## Probabilistic modelling Examples
Three class of probabilistic generetive models exist. 

### 1. Fully-observed models: 

-  MADE: Masked Autoencoder for Distribution Estimation: Autoregressive models are used a lot in time series modelling and language modelling: hidden Markov models or recurrent neural networks are examples. There, autoregressive models are a very natural way to model data because the data comes ordered (in time).

What's weird about using autoregressive models in this context is that it is sensitive to ordering of dimensions, even though that ordering might not mean anything. If xx encodes an image, you can think about multiple orders in which pixel values can be serialised: sweeping left-to-right, top-to-bottom, inside-out etc. For images, neither of these orderings is particularly natural, yet all of these different ordering specifies a different model above.

But it turns out, you don't have to choose one ordering, you can choose all of them at the same time. The neat trick in the masking autoencoder paper is to train multiple autoregressive models all at the same time, all of them sharing (a subset of) parameters θθ, but defined over different ordering of coordinates. This can be achieved by thinking of deep autoregressive models as a special cases of an autoencoder, only with a few edges missing.

###2. Latent variable models (Explicit Probabilistic graphical models): 
- VAE

- Another example is LDA (latent Drichlet allocation), which is basically a mixture of multinomial distributions (topics) in a document. Its difference from a typical mixture is that each document has its own mixture proportions of the topics but the topics (multinomials) are shared across the whole collection (i.e. a mixed membership model). 


### 3. Transformation models (Implicit generative models): 

- Real-valued non-volume preserving transformation (Real NVP): is an invertable transformation model. The generative procedure from the model is very similar to the one used in Variational Auto-Encoders and Generative Adversarial Networks: sample a vector z from a simple distribution (here a Gaussian) and pass it through the generator network g to obtain a sample x=g(z). 
	- The idea is to split the parameters of a NN layer to two segments, then transform one group of params by a sequence of an element-wise scaling and an offset. These two element-wise transformations are done by two matrices that are a function of the other group of parameters. this function can be a NN. We interleave the transformation of the two groups of parameters. 
	- This constructs a reversible function that has a diagonal jacobian. Therefore calculating the determinant of the jacobian is as easy of the product of the diagonal of the jacobian. 
	- The generator network g has been built in the paper according to a convolutional architecture, making it relatively easy to reuse the model to generate bigger images. As the model is convolutional, the model is trying to generate a “texture” of the dataset rather than an upsampled version of the images it was trained on. This explains why the model is most successful when trained on background datasets like LSUN different subcategories. This sort of behaviour can also be observed in other models like Deep Convolutional Generative Adversarial Networks.
	- From the generated samples, it seems the model was able to capture the statistics from the original data distribution. For example, the samples are in general relatively sharp and coherent and therefore suggest that the models understands something more than mere correlation between neighboring pixels. This is due to not relying on fixed form reconstruction cost like squared loss on the data level. The models seems also to understand to some degree the notion of foreground/background, and volume, lighting and shadows. 

- Glow: it's similar to realNVP with the difference that the structure of the reversible function is different and uses 1x1 convolutions as the scaling transformation. 


- Another example, Deep Unsupervised Learning using Nonequilibrium Thermodynamics. What we typically try to do in representation learning is to map data to a latent representation. While the Data can have arbitrarily complex distribution along some complicated nonlinear manifold, we want the computed latent representations to have a nice distribution, like a multivariate Gaussian. This paper takes this idea very explicitly using a stochastic mapping to turn data into a representation: a random diffusion process. If you take any data, and apply Brownian motion-like stochastic process to this, you will end up with a standard Gaussian distributed variable due to the stationarity of the Brownian motion. Now the trick the authors used is to train a dynamical system (a Markov chain) to inverts this random walk, to be able to reconstruct the original data distribution from the random Gaussian noise. Amazingly, this works, and the traninig objective becomes very similar to variational autoencoders. 

- GANs are also of this type of (transformation) generative models where a random Gaussian noise is transformed into data (e.g.an image) using a deep neural network. These models also assume a noise model on the latent cause. The good thing about such models is that it's easy to sample and compute expectation from these models without knowing the final distribution. Since classifiers are well-develped, we can use our knowledge there for density ratio estimation in these models. However, these models lack noise model and likelyhood. It's also difficult to optimize them.  Any implicit model can be easily turned into a prescribed model by adding a simple likelihood function (noise model) on the generated outputs but models with likelihood functions also regularly face the problem of intractable marginal likelihoods.But the specification of a likelihood function provides knowledge of data marginal p(x) that leads to different algorithms by exploiting this knowledge, e.g., NCE resulting from class-probability based testing in un-normalised models, or variational lower bounds for directed graphical models.

- Think of GANs as a density estimation problem. There are two high dimensional surfaces. One of them is P(x) which is unknown, we want to estimate, and we only have some examples of. The other is a maleable goo from a family of surfaces that we want to match onto the unknown density to estimate it. 

- Variational inference tries to solve this by assuming a goo Q(z|x) defined by a parametric model from the exponential family, a fixed distance metric (i.e. KL divergence) to make a force field, and letting the physics takes its course (running SGD on parameters). MCMC methods don't use any model and instead try to just throw random points onto the surface and see where it ends up and use samples to estimate the shape of the surface P(x). Variational inference can't match all the intricacies of the high dimensional surface while MCMC is very costly since using samples to estimate a high dimensional surface is a fools errand!

- GANs have a more elegant approach, they define the goo to be a transformation model Q(x) that doesn't have a tractable likelihood function (can get very complex) but is very easy to sample from instead. Instead of using a rigid distance metric, GANs actively define an adaptive distance metric that tries to capture the intricacies of the unknown surface in every iteration. They do this by using the insight that classifiers can actually find a surface between two data sources easily and we have two data sources from the unknown surface and the goo. Therefore, the surface that the classifier finds to distinguish the data sources, captures the intricacies of the unknown surface P(x). So, if after each iteration that the classifier finds the optimum surface between the unknown and the goo, we reshape the goo to beat the classifier surface, the goo will very well take the shape of the surface at the end. This is a very clever way of matching the goo to the unknown surface P(x) that basically uses an adaptive distance metric using the distance to the classifer surface instead of the unknown surface. the distance to the classifier surface sort of hand holds the goo optimization step by step until it gets as close as possible to the unkown surface. Another point is that we only have a limited number of examples from the unknown surface, so we actually do the above process stocastically in batches of examples from unknown and the goo surfaces.

- Another type of implicit models, are simulators that transform a noise source into an output. Sampling from such simulators is easy but explicitly calculating a likelihood distribution function is usually no possible if the simulator is not invertable and mostly intractable. An example is a physical simulator based on differential euqations derived from eqautions of motions.

###4. Deep Implicit Models (DIM): 
These are stacked transformation models in a graph. For example, if a GAN is creating the noise code for the next GAN, it's a deep implicit model since likelihoods are not explicitly defined. This enables building complex densities in a hierarchical way similar to probabilistic graphical models. 

- Inference in these models encounters two problems. First, like other inference problems marginal probability is intractable. Second, in transformation models likelihood is also intractable. We thus turn to variational inference with density ratio estimation instead of density estimation.

- A DIM is simply a deep neural network with random noise injected at certain layers. An additional reason for deep latent structure appears from this perspective: training may be easier if we inject randomness at various layers of a neural net, rather than simply at the input. This relates to noise robustness.

## Inference problems:

1. Evidence estimation
- marginal likelihood of evidence: Write the log density as the marginalization of the joint. We introduce a variational approximate q, into our marginal integral of joint p(x,z), to get p(x). By taking the log from both sides, and using Jensen's inequality we get the ELBO. Maximizing the ELBO is equivalent to minimizing the KL divergence of the real and variational posterior. 

2. Density ratio estimation (Density estimation by comparison)
The main idea is to estimate a ratio of real data distribution and model data distribution p(x)/q(x) instead of computing two densities that are hard. The ELBO in variational inference can be written in terms of the ratio. Introducing the variational posterior into the marginal integral of the joint results in the ELBO being $E[log p(x,z)- log q(z/x)]$. By subtracting emprical distribution on the observations, q(x) which is a constant and doesn't change optimization we have the ELBO using ratio as $E[log p(x,z)/q(x,z)]$. 

- Probabilistic classification: We can frame the ratio estimation as as the problem of classifying the real data (p(x)) from the data produced from model (q(x)). This is what happens in GANs.

- moment matching (log density difference): if all the infinite statistical moments of two distributions are the same the distributions are the same. So the idea is to set the moments of the numenator distribution (p(x)) equal to the moments of a transformed version of the denumerator (r(x)q(x)). This makes it possible to calculate the ratio r(x).

- Ratio matching: basic idea is to directly match a density ratio model r(x) to the true density ratio under some divergence. A kernel is usually used for this density estimation problem plus a distance measure (e.g. KL divergence) to measure how close the estimation of r(x) is to the true estimation. So it's variational in some sense. Loosely speaking, this is what happens in variational Autoencoders!

- Divergence minimization: Another approach to two sample testing and density ratio estimation is to use the divergence (f-divergence, Bergman divergence) between the true density p and the model q, and use this as an objective to drive learning of the generative model. f-GANs use the KL divergence as a special case and are equipped with an exploitable variational formulation (i.e. the variational lower bound). There is no discriminator in this formulation, and this role is taken by the ratio function. We minimise the ratio loss, since we wish to minimise the negative of the variational lower bound; we minimise the generative loss since we wish to drive the ratio to one.

- Maximum mean discrepancy(MMD): is a nonparametric way to measure dissimilarity between two probability distributions. Just like any metric of dissimilarity between distributions, MMD can be used as an objective function for generative modelling.  The MMD criterion also uses the concept of an 'adversarial' function f that discriminates between samples from Q and P. However, instead of it being a binary classifier constrained to predict 0 or 1, here f can be any function chosen from some function class. The idea is: if P and Q are exactly the same, there should be no function whose expectations differ under Q and P. In GAN, the maximisation over f is carried out via stochastic gradient descent, here it can be done analytically. One could design a kernel which has a deep neural network in it, and use the MMD objective!?

- Instead of estimating ratios, we estimate gradients of log densities. For this, we can use[ denoising as a surrogate task](http://www.inference.vc/variational-inference-using-implicit-models-part-iv-denoisers-instead-of-discriminators/).denoisers estimate gradients directly, and therefore we might get better estimates than first estimating likelihood ratios and then taking the derivative of those


3. Moment computation

$E[f(z)|x] =\int f(z)p(z|x)dz$

4. Prediction

$p(xt+1) =\int p(xt+1|xt)p(xt)dxt$

5. Hypothesis Testing

$B = log p(x|H1) - log p(x|H2)$


## Other Generative modeling ideas (inspired by GANs)

[Unsupervised learning of visual representations by solving jigsaw puzzles](http://arxiv.org/abs/1603.09246) is a clever trick. The author break the image into a puzzle and train a deep neural network to solve the puzzle. The resulting network has one of the highest performance of pre-trained networks.

[Unsupervised learning of visual representations from image patches and locality](https://arxiv.org/abs/1511.06811) is also a clever trick. Here they take two patches of the same image closely located. These patches are statistically of the same object. A third patch is taken from a random picture and location, statistically not of the same object as the other 2 patches. Then a deep neural network is trained to discriminate between 2 patches of same object or different objects. The resulting network has one of the highest performance of pre-trained networks.

[Unsupervised learning of visual representations from stereo image reconstructions](http://arxiv.org/abs/1604.03650) takes a stereo image, say the left frame, and reconstruct the right frame. Albeit this work was not aimed at unsupervised learning, it can be! This method also generates interesting [3D movies form stills](https://github.com/piiswrong/deep3d).

[Unsupervised Learning of Visual Representations using surrogate categories](http://arxiv.org/abs/1406.6909) uses patches of images to create a very large number of surrogate classes. These image patches are then augmented, and then used to train a supervised network based on the augmented surrogate classes. This gives one of the best results in unsupervised feature learning.

[Unsupervised Learning of Visual Representations using Videos](http://arxiv.org/abs/1505.00687) uses an encoder-decoder LSTM pair. The encoder LSTM runs through a sequence of video frames to generate an internal representation. This representation is then decoded through another LSTM to produce a target sequence. To make this unsupervised, one way is to predict the same sequence as the input. Another way is to predict future frames.

Another paper (MIT: Vondrick, Torralba) using videos with very compelling results is [here](http://arxiv.org/abs/1504.08023). This work is from April 2015! The great idea behind this work is to predict the representation of future frames from a video input. This is an elegant approach.

[PredNet is a network designed to predict future frames in video](https://coxlab.github.io/prednet/)
PredNet is a very clever neural network model that in our opinion will have a major role in the future of neural networks. PredNet learns a neural representation that extends beyond the single frames of supervised CNN.It uses [predictive coding](http://www.nature.com/neuro/journal/v2/n1/full/nn0199_79.html) and using [feedback connections in neural models](http://arxiv.org/abs/1608.03425)

## To read on implicit models
[1] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., … Bengio, Y. (2014). Generative Adversarial Nets. In Neural Information Processing Systems.

[2] Dziugaite, G. K., Roy, D. M., & Ghahramani, Z. (2015). Training generative neural networks via Maximum Mean Discrepancy optimization. In Uncertainty in Artificial Intelligence.

[3] Li, Y., Swersky, K., & Zemel, R. (2015). Generative Moment Matching Networks. In International Conference on Machine Learning.

[4] Radford, A., Metz, L., & Chintala, S. (2016). Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. In International Conference on Learning Representations.

[5] Mohamed, S., & Lakshminarayanan, B. (2016). Learning in Implicit Generative Models. ArXiv.org.

[6] Arjovsky, M., Chintala, S., & Bottou, L. (2017). Wasserstein GAN. In International Conference on Machine Learning.

[7] Tran, D., Ranganath, R., & Blei, D. M. (2017). Deep and Hierarchical Implicit Models. ArXiv.org.

# Future
Unsupervised training is very much an open topic, where you can make a large contribution by:
- creating a new unsupervised task to train networks, e.g.: solve a puzzle, compare image patches, generate images, …)
- thinking of tasks that create great unsupervised features, e.g.: what is object and what is background, same on stereo images, same on video frames ~= similar to how our human visual system develops


## Side notes on Generative models
The objective function we use for training a probabilistic model should match the way we ultimately want to use the model. Very often people don't make this distinction clear when talking about generative models which is one of the reasons why there is still no clarity about what different objective functions do. In "How (not) to Train your Generative Model: Scheduled Sampling, Likelihood, Adversary?", Ferenc argues that when the goal is to train a model that can generate natural-looking samples, maximum likelihood is not a desirable training objective. Maximum likelihood is consistent so it can learn any distribution if it is given infinite data and a perfect model class. However, under model misspecification and finite data (almost always), it has a tendency to produce models that overgeneralise.

Generative modelling is about finding a probabilistic model $$Q$$ that in some sense approximates the natural distribution of data $$P$$. So if we want to train models that produce nice samples, my recommendation is to try to use $$KL[Q∥P]$$ as an objective function or something that behaves like it since perceived quality of each sample is related to the $$ −\log P(x)$$ under the human observers' subjective prior of stimuli. On the other hand, Maximum likelihood is roughly the same as minimising $$KL[P∥Q]$$. Both divergences ensure consistency, However, they differ fundamentally in the way they deal with finite data and model misspecification. $$KL[P∥Q]$$ tends to favour approximations $$Q$$ that overgeneralise $$P$$. Practically this means that the model will occasionally sample unplausible samples that don't look anything like samples from $$P$$. $$KL[Q‖P]$$ tends to favour under-generalisation. Practically this means that $$KL[Q‖P]$$ will try to avoid introducing unplausible samples, sometimes at the cost of missing the majority of plausible samples under $$P$$. In other words: In yet other words: $$KL[P‖Q]$$ is an optimist, $$KL[Q‖P]$$ is a pessimist.

In unsupervised learning we are given indepenent data samples from some underlying data distribution P which we don't know, and our goal is to come up with an approximate distribution Q that is as close to P as possible, only using the dataset. Often, Q is chosen from a parametric family of distributions, and our goal is to find the optimal parameters so that the distribution P is best approximated.

The central issue of unsupervised learning is choosing an appropriate objective function, that appropriately measures the quality of our approximation, and which is tractable to compute and optimise when we are working with complicated, deep models. Typically, a probability model is specified as q(x;θ)=f(x;θ)/Zθ, where f(x;θ) is some positive function parametrised by θ. The denominator Zθ is called the normalisation constant which makes sure that q is a valid probability model: it has to sum up to one over all possible configurations of x. The central problem in unsupervised learning is that for the most interesting models in high dimensional spaces, calculating ZθZθ is intractable. So The most straightforward choice of objective functions, i.e. marginal likelihood (log likelihood/evidence), is intractable in most cases.Therefore people come up with:

1-Alternative objective functions and fancy methods to deal with intractable models such as:
- density ratio estimation (like  adversarial training, maximum mean discrepancy, discussed later), 
- pseudolikelihood: The twist is this: instead of thinking about fitting a probabilistic model p(x;θ) to data x, you learn a joint probability distribution p(x,z;θ) of the data x and it's noise-corrupted version z. The noise corruption is artificially introduced by us, following a corruption distribution. The point is, if you learn the joint model p(x,z;θ), that also implies a generative model p(x;θ)=∫p(x,z;θ). To fit the joint model to observations (xi,zi), we use score matching with the pseudolikelihood scoring rule as objective function.

- MSE or maximum spacing estimation: if you squash a random variable X through its cumulative distribution function F, the transformed variable F(X) follows a standard uniform distribution between 0 and 1. Furthermore, we can say that for any random variable X, its CDF (F) is the only monotonic, left-continuous function with this property. Using this observation one can come up with objective functions for univariate density estimation. Let's assume we have a parametric family of distributions described by their CDFs Fθ, and we want to find the θ that best fits our observations Xi. If Fθ is close to the true CDF of the data distribution, we would expect Fθ(xi) to follow a uniform distribution. Therefore, we only need a way to measure the uniformity of Fθ(xi) across samples. Maximum Spacing Estimation (MSE) uses the geometric mean of spacing between ordered samples as an objective function. 

- Alternative optimisation methods or approximate inference methods such as contrastive divergence or variational Bayes.

2- Interesting models that have tractable normalisation constants:
-  MADE: Masked Autoencoder for Distribution Estimation: This paper sidesteps the high dimensional normalisation problem by restricting the class of probability distributions to autoregressive models! In a model like this, we only need to compute the normalisation of each q(xd|x1:d−1;θ) term, and we can be sure that the resulting model is a valid model over the whole vector x. But as normalising these one-dimensional probability distributions is a lot easier, we have a whole range of interesting tractable distributions at our disposal.

- Another idea in representation learning is to map data to a latent representation. While the Data can have arbitrarily complex distribution along some complicated nonlinear manifold, we want the computed latent representations to have a nice distribution, like a multivariate Gaussian. "Deep Unsupervised Learning using Nonequilibrium Thermodynamics" takes this idea very explicitly. If you take any data point, and apply Brownian motion-like stochastic process to this, you will end up with a standard Gaussian distributed variable, due to the stationarity of the Brownian motion. This is a diffusion process. Now the trick is to train a dynamical system to inverts this random walk, to be able to reconstruct the original data distribution from the random Gaussian noise. Amazingly, this works, and the traninig objective becomes very similar to variational autoencoders. Therefore, The information about the data distribution is encoded in the approximate inverse dynamical system.

## Non-Parametric Models
- Parametric models assume a finite set of parameters can explain data. Therfore, given the model parameters, future predictions x, are independant of the observed data.

- Non-parametric models assume that the data distribution cannot be defined in terms of such a finite set of parameters. But they can often be defined by assuming an infinite dimensional parameter vector. Usually we think of infinte parameters as a function. Therfore, the amount of information stored in a nonparametric model grows with more data. 

Parametric | Non-parametric | -> Application
polynomial regression | Gaussian processes | -> function approx.
logistic regression | Gaussian process classifiers | -> classification
mixture models, k-means \ Dirichlet process mixtures | -> clustering
hidden Markov models | infinite HMMs | -> time series
factor analysis, probabilistic PCA, PMF | infinite latent factor models | -> feature discovery

### Gaussian Process
- Consider the problem of nonlinear regression: You want to learn a function f with error bars from data D = {X, y}. A Gaussian process defines a distribution over functions p(f) which can be used for Bayesian regression: p(f|D) = p(f)p(D|f)/p(D)

- A multilayer perceptron (neural network) with infinitely many hidden units and Gaussian priors on the weights (bayesian neural net) is a GP (Neal, 1996)


-In a Gaussian process, each data point is considered to be a random variable. We form a similarity matrix between all data points which is used as the covariance matrix for a multivariate Gaussian (uses negative exponensial of Euclidean distance). Since the joint distribution of data is a multivariate Gaussian, assuming that the joint distribution of test data and training data are also a multivariate Gaussian, prediction will be conditional probability of test data given training data from a multivariate Gaussian joint. In a multivariate Gaussian, joint and marginal probabilities can be analytically calculated using linear algebra on the covariance matrix. Therefore, prediction consists of simply performing linear algebra on covariance matrix (similarity matrix) of training data. Note that the choice of the distance measure (i.e. negative exponensial of Euclidean distance) is the modelling prior in a regression problem (e.g. if a linear distance is chosen, then it's linear regression!).

- Other variants of non-parametric models are 1) nearest neighbor regression, where the model would simply store all (x,y) training pairs and then just return an extrapolation of the nearest neighbor y values for each x in the test set. AS another variant, 2) we can simply wrap a parametric model inside a bigger model that scales with training data size; for example in a generalized linear model, we can simply increase the degree of polynomial by extending the data matrix X and number of parameters. 

- Gaussian processes are a very flexible non-parametric model for unknown functions, and are widely used for regression, classification, and many other applications that require inference on functions. Dirichlet processes are a non-parametric model with a long history in statistics and are used for density estimation, clustering, time-series analysis and modelling the topics of documents. To illustrate Dirichlet processes, consider an application to modelling friendships in a social network, where each person can belong to one of many communities. A Dirichlet process makes it possible to have a model whereby the number of inferred communities (that is, clusters) grows with the number of people. The Indian buffet process (IBP)40 is a non-parametric model that can be used for latent feature modelling, learning overlapping clusters, sparse matrix factorization, or to non-parametrically learn the structure of a deep network41. Elaborating the social network modelling example, an IBP-based model allows each person to belong to some subset of a large number of potential communities (for example, as defined by different families, workplaces, schools, hobbies, and so on) rather than a single community, and the probability of friendship between two people depends on the number of overlapping communities they have42. In this case, the latent features of each person correspond to the communities, which are not assumed to be observed directly. The IBP can be thought of as a way of endowing Bayesian non-parametric models with ‘distributed representations’, as popularized in the neural network literature. An interesting link between Bayesian non-parametrics and neural networks is that, under fairly general conditions, a neural network with infinitely many hidden units is equivalent to a Gaussian process.

- Standard variance calculations involve multiplying the covariance matrix by an inverse of the covariance matrix. However, calculating the inverse of a covariance matrix requires a substantial expansion of the matrix dimensions that can rapidly exhaust all available memory. 

### Conditional Random Fields
CRFs are a type of discriminative undirected probabilistic graphical model (a type of Markov network). It is used to encode known relationships between observations. a CRF can take context into account; e.g. predict sequences of labels for sequences of input samples. Probability of a sequence of labels, y, given a sequence of inputs, X is written as the normalized product of factors(factors look at a subset of y and X that are dependant and tell us how much they like their association effectively making othe parts of y and X independant of those included in that factor). 

The factor is often parameterized using exponentials (to convert the product to a sum) of parameters times features. Features are also called sufficient statistics since the whole log P(y/X) of the dataset can be written as a linear combination of features with parameters as coefficients. 

### Linear Chain CRF
The assumption in a linear chaing CRF is that each sequence observation is independant from other sequence observations. Therefore, the joint probability of all dataset will be the product of conditional probabilities of observations representing each sequence observation by a factor. 

An example suppose we have a feedforward net that does optical character recognition. If we use softmax in the output, with the feedforward assumption that all observations are independant, the total P(y/x) will be a product of p_k(y/X) for k observations P(y/X)=product(p_k(y/x)). We know that the character in the current time step is dependant on the one in previous step. Therefore, a simple CRF to model this interdependancy is to write the total P(y/X) using linear chain CRF as a product of p_k(y/X) at observation times the p_k+1(y/x) of the next observation P(y/X)=product(p_k(y/x)p_k+1(y/x)). Using the factor parameterization mentioned above, the product of exponentials will be converted to exponential of sum of factors. A sum for the net output and a sum for dependancy on the previous output. The network weights are shared among k and k+1. Therefore, we'll end up with a type of recursive NN.

In linear chain CRFs, four types of factors can be considered. 1- Factors of current labels and previous inputs, 2- Factors of current labels and current inputs, 3-Factors of current labels and future inputs, and 4- Factors of current labels and future labels.


### self-organizing maps (SOM)
- Self organizing maps (SOM) unsupervisedly map a vector from some input space (usually of high dimensionality) onto a vector in some output space (usually 2d) while maintaining topographic relationships (vectors that are similar in the input space correspond to vectors that are similar in the output space). Do transformation models like GANs perform the opposite!? [video](https://www.youtube.com/watch?v=b3nG4c2NECI)


- SOM is a space-filling grid (for example a 2D lattice of neurons) that provides a dimensionality reduction of the data. For intuition think of a bunch of people in a stadium, every one compares themselves to their neighbors (using a distance metric like euclidean) and moves closer to the ones that are more similar to them until convergence. Looking from above, we see a self organizing map. SOM is biologically inspired by the way that sensory information is processed in neocortex by highly ordered neural nets. Representations of the sensory modalities are organized into well ordered maps in the brain tangent to the cortical surfaces.


- You start with a high-dimensional space of data points, and an arbitrary grid that sits in that space. The grid can be of any dimension, but is usually smaller than the dimension of your dataset, and is commonly 2D, because that's easy to visualise. For each datum in your data set, you find the nearest grid point, and "pull" that grid point toward the data set. You also pull each of the neighbouring grid points toward the new position of the first grid point. At the start of the process, you pull lots of the neighbours toward the data point. Later in the process, when your grid is starting to fill the space, you move less neighbours, and this acts as a kind of fine tuning. This process results in a set of points in the data space that fit the shape of the space reasonably well, but can also be treated as a lower-dimension grid.


- SOM learning algorithm is simple. 1- initialize all weights randomly 2- feed a data point to the net and find its distance to weight vectors of all nodes, the minimum distance is best matching unit (BMU) 3- Find the set of points on the lattice within a given radius of from BMU (neighborhood radius gets smaller with time). 4- Adjust the weight vectors of nodes in neibourhood to be more like the data point (add a portion of the distance to datapoint to the weights based on their proximity to BMU) 5- Repeat unit convergence! 6- the density of nodes on the lattice (number of data points connected to each node) as a heat map is called SOM and we can run clustering algos on them.



### Co-Training

co-training paradigm proposed by Blum and Mitchell, trains two classifiers separately on two different views, i.e. two independent sets of attributes, and uses the predictions of each classifier on unlabeled examples to augment the training set of the other, utilizing the natural redundancy in the attributes.

The standard co-training algorithm requires two sufficient and redundant views, that is, the attributes be naturally partitioned into two sets, each of which is sufficient for learning and conditionally independent to the other given the class label.

Goldman and Zhou proposed an algorithm which does not exploit attribute partition. However, it requires using two different supervised learning algorithms that partition the instance space into a set of equivalence classes, and employing time-consuming cross validation technique to determine how to label the unlabeled examples and how to produce the final hypothesis.


Let L denote the labeled example set with size |L| and U denote the unlabeled example set with size |U|. In co-training style algorithms, two classifiers are initially trained from L, each of which is then re-trained with the help of unlabeled examples that are labeled by the latest version of the other classifier. In order to determine which example in U should be labeled and which classifier should be biased in prediction, the confidence of the labeling of each classifier must be explicitly measured. Sometimes such a measuring process is quite time-consuming.

### Tri-Training [ZH Zhou, M Li]

Assume that besides these two classifiers, i.e. h1 and h2, a classifier h3 is initially trained from L. Then, for any classifier, an unlabeled example can be labeled for it as long as the other two classifiers agree on the labeling of this example, while the confidence of the labeling of the classifiers are not needed to be explicitly measured. 


For instance, if h2 and h3 agree on the labeling of an example x in U, then x can be labeled for h1. It is obvious that in such a scheme if the prediction of h2 and h3 on x is correct, then h1 will receive a valid new example for further training; otherwise h1 will get an example with noisy label. However, even in the worse case, the increase in the classification noise rate can be compensated if the amount of newly labeled examples is sufficient, under certain conditions.


Tri-training does not require sufficient and redundant views, nor does it require the use of different supervised learning algorithms whose hypothesis partitions the instance space into a set of equivalence classes.

In contrast to previous algorithms that utilize two classifiers, tritraining uses three classifiers. This setting tackles the problem of determining how to label the unlabeled examples and how to produce the final hypothesis, which contributes much to the efficiency of the algorithm.


### Cooperative Training as a solution to discrete GANs.

in order to estimate the target distribution well in both quality and diversity senses, an ideal algorithm for generative models should be able to optimize a symmetric divergence or distance like Jenson-Shannon Divergence instead of KL.

Each iteration of Cooperative Training mainly consists of two parts. The first part is to train a mediator Mφ, which is a predictive module that measures a mixture distribution of the learned generative distribution Gθ and target latent distribution P as
Mφ =(P + Gθ)/2. 


- Lets say you want to train a language model to generate sentences for you and don't want to train it with maximum likelihood or teacher forcing. In teacher forcing, at every time step you give the right output as the previous word instead of what the RNN has actually generated, which ignores where the network made mistakes in generating words. You ideally want to provide your network a previous word that is a combination of what it generated in previous step and what it should have generated so that it can assign credits and get gradients to fix its mistakes. 

- Training with teacher forcing and maximum likelihood is equivalent to minimizing the KL divergence between the true distribution (P) and generator distribution (G). The alternative we are looking for based on above discussion is JS divergence between the two distributions instead. Since the actual JS divergence requires the true distribition (P) which we don't know, we use a second network called the mediator to approximate the JS divergence between the two distributions. It is achived by minimizing the KL divergence between the real JS divergence (unknown) and a variational approximation to the JS divergence through the parameterized mediator distribution M. This way, the mediator can provide an approximation for the JS divergence between the true distribution (P) and generator distribution (G) that we can use to train the generator. 

- Then they introduce a clever way for calculating the gradient of the approximated JS divergence as well. the gradient for training generator via Cooperative Training can be formulated as the sum of the differences of log logits that G and M produce for a sequence (sentence), times the logits that the G produces. You start from the first word in the sequence, calculate above, and repeat for all subsequent words and then sum all up. That's the gradient for your G without the need for sampling from the softmax of logits, therefore, you can train your generator that is producing categorical variables without the need to pass gradients through the undifferentiable categorical variables. 

Algorithm (example for language models):
- you make two RNN language models as the generator (G) and the mediator (M).
- you take a bunch of real sentence from your dataset, pass to generator and get also sentences that the generator produces. 
- you put the real sentences and the generated sentences in the same minibatch and send to the mediator. You then train the mediator as a language model with this mixed minibatch data using maximum likelihood. 
- Then you take another real data batch, send through the generator to get the logits_G for each of the words in the sentence.
- Then you pass this tensor of logits_G to your mediator to get another same-size tensor of logits_M.
- Then you starting from the first word, apply $$\log$$ to the logits vectors for each word and then calculate the difference and multiply by logits_G i.e. $$logits_G * (\log(logits_G) - \log(logits_M))$$.
- then sum the above values for all the words in the sentences in the minibatch and that's the gradient of your G.
- backpropagate it to your G and train your generator to generate sequences for you!


### Curriculum learning
- the idea is that a student learns best not when complex and easy examples are randomly presented, but when a certain curriculum is followed in presentation of examples that lets model achieve best performance. The result from the paper is that when training machine learning models, start with easier subtasks and gradually increase the difficulty level of the tasks.

- Can we learn this curriculum?
	- a GAN sort of learns a curriculum, as the discriminator and generator gradually evolve together. The generator doesn't suddenly provide the hardest possible example, and the discriminator doesn't suddenly discriminate the hardest possible negative example from the positive ones. Both evolve together according to a learned curriculum of gradually harder negative examples. 




